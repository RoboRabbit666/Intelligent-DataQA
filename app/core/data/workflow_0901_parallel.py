# coding: utf-8
import copy
import json
import re
import traceback
from concurrent.futures import ThreadPoolExecutor  # æ–°å¢ï¼šç”¨äºå¹¶è¡Œæ‰§è¡Œ
from dataclasses import dataclass
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple

from app.config.config import settings
from app.core.components import (
    business_info_kb,
    document_kb,
    qa_pair_kb,
    sql_kb,
    tokenizer,
)
from app.core.components.query_optimizer import (
    DataQAOptimizedQuery,
    QueryOptimizationType,
    QueryOptimizer,
)
from app.models import (
    ChatCompletionChoice,
    ChatStep,
    ChatUsage,
    DataQaChatCompletionResponse,
    DataQACompletionRequest,
    RerankerInfo,
)
from czce_ai.document import Chunk
from czce_ai.knowledge import SearchType
from czce_ai.llm.chat import LLMChat as LLMModel
from czce_ai.llm.message import Message as ChatMessage
from czce_ai.utils.log import logger

from .entities import WorkflowStepType
from .prompt import dataqa_prompt


@dataclass
class WorkflowConfig:
    """å·¥ä½œæµé…ç½®ç±»-ç»Ÿä¸€ç®¡ç†å‚æ•°ã€é­”æ³•æ•°å­—ç­‰"""
    # ä¿æŒä¸å˜çš„é…ç½®
    history_round: int = 3
    follow_up_round: int = 1
    reranking_threshold: float = 0.2
    sql_schema_collection: Optional[str] = "hybrid_sql"
    domain_collection: Optional[str] = "domain_k1"
    api_collection: Optional[str] = "api_k1"
    qa_collection: Optional[str] = "sql_qa"
    max_table_results: int = 3
    enable_entity_recognition: bool = True
    enable_reranker: bool = True
    
    # FAQé˜ˆå€¼ç­–ç•¥
    enable_fqa_answer: bool = True
    fqa_answer_threshold: float = 0.9
    fqa_reference_threshold: float = 0.7
    fqa_sample_num: int = 3
    
    # APIç›¸å…³é…ç½®
    enable_api_search: bool = True
    api_threshold: float = 0.8

    def __post_init__(self):
        if self.history_round < self.follow_up_round:
            raise ValueError(
                f"history_round({self.history_round}) must be >= follow_up_round({self.follow_up_round})"
            )


class DataQaWorkflow:
    """æ•°æ®é—®ç­”å·¥ä½œæµä¸»ç±»"""
    
    def __init__(
        self,
        ans_llm: LLMModel,
        ans_thinking_llm: LLMModel,
        query_llm: LLMModel,
        config: Optional[WorkflowConfig] = None,
        collection: Optional[str] = None,
        reranking_threshold: Optional[float] = None,
    ):
        # ä¿æŒä¸å˜ï¼šåˆå§‹åŒ–
        self.ans_client = ans_llm
        self.ans_thinking_client = ans_thinking_llm
        self.query_optimizer = QueryOptimizer(query_llm)
        self.config = config or WorkflowConfig()
        
        if collection:
            self.config.sql_schema_collection = collection
        if reranking_threshold is not None:
            self.config.reranking_threshold = reranking_threshold

    def _create_step(
        self, step_type: WorkflowStepType, number: int, prompt: Any
    ) -> ChatStep:
        """åˆ›å»ºå·¥ä½œæµæ­¥éª¤ - ä¿æŒä¸å˜"""
        step_names = {
            WorkflowStepType.FOLLOW_UP: "é—®é¢˜è¿½é—®",
            WorkflowStepType.MODIFY_QUERY: "é—®é¢˜æ”¹å†™",
            WorkflowStepType.ENTITY_RECOGNITION: "é—®é¢˜å®ä½“è¯†åˆ«",
            WorkflowStepType.LOCATE_API: "APIå®šä½",
            WorkflowStepType.SEARCH_FAQ: "è¯­ä¹‰æœç´¢FAQ",
            WorkflowStepType.LOCATE_TABLE: "è¡¨æ ¼å®šä½",
            WorkflowStepType.GENERATE_SQL: "SQLç”Ÿæˆ",
            WorkflowStepType.BUSINESS_INFO: "ä¸šåŠ¡çŸ¥è¯†æœç´¢",
        }
        
        if not isinstance(prompt, str):
            if isinstance(prompt, list):
                prompt = json.dumps(prompt, ensure_ascii=False, indent=2)
            elif isinstance(prompt, dict):
                prompt = json.dumps(prompt, ensure_ascii=False, indent=2)
            else:
                prompt = str(prompt)
        
        return ChatStep(
            key=step_type.value,
            name=step_names.get(step_type, step_type.value),
            number=number,
            prompt=prompt,
            finished=True,
        )

    def _extract_input_messages(
        self, request: DataQACompletionRequest
    ) -> List[ChatMessage]:
        """æå–è¾“å…¥ä¿¡æ¯åˆ—è¡¨ - ä¿æŒä¸å˜"""
        return request.messages[-self.config.history_round * 2 :]

    def modify_query(
        self,
        input_messages: List[ChatMessage],
        enable_follow_up: bool,
    ) -> DataQAOptimizedQuery:
        """é—®é¢˜æ”¹å†™ - ä¿æŒä¸å˜"""
        try:
            input_messages_copy = copy.deepcopy(input_messages)
            optimization_type = (
                QueryOptimizationType.FOLLOWUP
                if enable_follow_up
                else QueryOptimizationType.DATAQA
            )
            
            last_user_msg = None
            last_index = None
            for index, message in enumerate(input_messages_copy):
                if message.role == "user":
                    last_user_msg = message
                    last_index = index
            
            if not last_user_msg:
                return DataQAOptimizedQuery(
                    original_query="",
                    rewritten_query="",
                    is_sufficient=False
                )
            
            optimized_query = self.query_optimizer.generate_optimized_query(
                query=last_user_msg.content,
                chat_history=input_messages_copy[:last_index] if last_index else [],
                optimization_type=optimization_type,
            )
            return optimized_query
        except Exception as e:
            logger.error(f"Modify query Error: {e}")
            traceback.print_exc()
            raise e

    def entity_recognition(self, query: str) -> str:
        """å®ä½“è¯†åˆ« - ä¿æŒä¸å˜"""
        if not self.config.enable_entity_recognition:
            return query
        
        try:
            enhanced_query = query
            entity_list = tokenizer.recognize(query)
            
            for entity in entity_list:
                if entity["id"] and entity["text"] in enhanced_query:
                    if entity["label"] == "åˆçº¦":
                        normalized_code = re.sub(
                            r"[-_\.\s/]+", "", entity["text"].upper()
                        )
                        substring = f"({normalized_code})(åˆçº¦)"
                    else:
                        substring = f"({entity['text']})({entity['label']})"
                    enhanced_query = enhanced_query.replace(
                        entity["text"], substring, 1
                    )
            
            logger.info(f"å®ä½“è¯†åˆ«å®Œæˆ: {query} -> {enhanced_query}")
            return enhanced_query
        except Exception as e:
            logger.error(f"å®ä½“è¯†åˆ«å¤±è´¥: {e}")
            return query

    def business_info_search(
        self, query: str, top_k: int = 2, knowledge_base_ids: Optional[List[str]] = None
    ) -> List[Chunk]:
        """æœç´¢ä¸šåŠ¡çŸ¥è¯† - ä¿æŒä¸å˜"""
        try:
            ranked_qas = business_info_kb.search(
                collection=self.config.domain_collection,
                query=query,
                search_type=SearchType.dense,
                top_k=top_k,
                use_reranker=False,
                score_fusion=False,
                knowledge_ids=knowledge_base_ids,
            )
            return ranked_qas
        except Exception as e:
            logger.error(f"Business info search Error: {e}")
            return []

    def search_qa(
        self, query: str, knowledge_base_ids: Optional[List[str]] = None
    ) -> List[Chunk]:
        """æœç´¢FAQçŸ¥è¯†åº“ - ä¿æŒä¸å˜"""
        try:
            ranked_qas = qa_pair_kb.search(
                collection=self.config.qa_collection,
                query=query,
                top_k=self.config.fqa_sample_num,
                use_reranker=self.config.enable_reranker,
                score_fusion=False,
                knowledge_ids=knowledge_base_ids,
            )
            return ranked_qas
        except Exception as e:
            logger.error(f"QA Search Error: {e}")
            return []

    def locate_api(
        self, query: str, knowledge_base_ids: Optional[List[str]] = None
    ) -> List[Chunk]:
        """å®šä½æ•°æ®API - ä¿æŒä¸å˜"""
        if not self.config.enable_api_search:
            return []
        
        try:
            ranked_apis = document_kb.search(
                collection=self.config.api_collection,
                query=query,
                knowledge_ids=knowledge_base_ids,
                top_k=self.config.max_table_results,
                use_reranker=self.config.enable_reranker,
            )
            return ranked_apis
        except Exception as e:
            logger.error(f"Locate API Error: {e}")
            return []

    def locate_table(
        self,
        query: str,
        request: DataQACompletionRequest,
        knowledge_base_ids: Optional[List[str]] = None,
    ) -> List[Dict[str, Any]]:
        """å®šä½è¡¨æ ¼ - ä¿æŒä¸å˜"""
        input_messages = self._extract_input_messages(request)
        search_content = (
            "\n".join([f"{msg.role}: {msg.content}" for msg in input_messages])
            + "\nUser: "
            + query
        )
        logger.info(f"Search content: {search_content}")
        
        try:
            ranked_tables = sql_kb.search(
                collection=self.config.sql_schema_collection,
                query=search_content,
                knowledge_ids=knowledge_base_ids,
                top_k=self.config.max_table_results,
                use_reranker=self.config.enable_reranker,
            )
            
            tables = [
                {
                    "chunk_uuid": table.chunk_id,
                    "table_name": table.data.table_name,
                    "table_info": table.data.table_info,
                    "table_schema": table.data.table_schema,
                    "score": table.reranking_score,
                }
                for table in ranked_tables
            ]
            return tables
        except Exception as e:
            logger.error(f"Locate table Error: {e}")
            return []

    def generate_sql(
        self,
        table_schema: str,
        input_messages: List[ChatMessage],
        faq_results: Optional[List[Chunk]] = None,
        business_context: Optional[List[Chunk]] = None,
        thinking: bool = False,
    ):
        """ç”ŸæˆSQL - ä¿æŒä¸å˜"""
        query = input_messages[-1].content
        
        # å¤„ç†FAQç»“æœ
        faq_text = ""
        faq_score = 0.0
        if faq_results:
            faq_examples = []
            for faq in faq_results[:self.config.fqa_sample_num]:
                faq_examples.append(
                    f"é—®é¢˜: {faq.data.question}\nSQL: {faq.data.answer}"
                )
            faq_text = "\n---\n".join(faq_examples)
            faq_score = faq_results[0].reranking_score if faq_results else 0.0
        
        # æ„å»ºä¸šåŠ¡ä¸Šä¸‹æ–‡
        business_text = ""
        if business_context:
            business_info = []
            for info in business_context[:2]:
                if hasattr(info.data, 'business_desc'):
                    business_info.append(info.data.business_desc)
                elif hasattr(info.data, 'content'):
                    business_info.append(info.data.content)
            if business_info:
                business_text = "\nä¸šåŠ¡èƒŒæ™¯çŸ¥è¯†ï¼š\n" + "\n".join(business_info)
        
        content = dataqa_prompt.format(
            table_schema=table_schema + business_text,
            question=query,
            faq_results=faq_text,
            faq_score=faq_score,
        )
        
        system_msg = ChatMessage(
            role="system",
            content=content,
        )
        
        llm = self.ans_thinking_client if thinking else self.ans_client
        response = llm.invoke(messages=[system_msg] + input_messages[:])
        return response

    def _handle_follow_up(
        self,
        optimized_query: DataQAOptimizedQuery,
        request: DataQACompletionRequest,
    ) -> Tuple[Optional[DataQaChatCompletionResponse], DataQAOptimizedQuery]:
        """å¤„ç†è¿½é—®é€»è¾‘ - ä¿æŒä¸å˜"""
        follow_up_num = (
            request.follow_up_num + 1 if not optimized_query.is_sufficient else 0
        )
        
        if (
            not optimized_query.is_sufficient
            and follow_up_num <= self.config.follow_up_round
        ):
            step = self._create_step(
                step_type=WorkflowStepType.FOLLOW_UP,
                number=0,
                prompt=optimized_query.rewritten_query,
            )
            
            choices = [
                ChatCompletionChoice(
                    finish_reason="stop",
                    index=0,
                    message=ChatMessage(
                        role="assistant",
                        content=optimized_query.rewritten_query,
                        reasoning_content=None,
                        is_follow_up=True,
                    ),
                )
            ]
            
            return (
                DataQaChatCompletionResponse(
                    model=request.model or "dataqa",
                    created=int(datetime.now().timestamp()),
                    choices=choices,
                    usage=ChatUsage(
                        prompt_tokens=0,
                        completion_tokens=0,
                        total_tokens=0
                    ),
                    steps=[step],
                    follow_up_num=follow_up_num,
                ),
                optimized_query,
            )
        
        elif follow_up_num > self.config.follow_up_round:
            input_messages = self._extract_input_messages(request)
            recent_messages = input_messages[-self.config.follow_up_round * 2 :]
            manual_query = " ".join(
                [msg.content for msg in recent_messages if msg.role == "user"]
            )
            
            return None, DataQAOptimizedQuery(
                original_query=optimized_query.original_query,
                rewritten_query=manual_query or optimized_query.rewritten_query,
                is_sufficient=True,
            )
        
        return None, optimized_query

    # æ–°å¢ï¼šå¹¶è¡Œæœç´¢FAQå’ŒAPIçš„è¾…åŠ©å‡½æ•°
    def _parallel_search_faq_api(
        self, 
        query: str, 
        knowledge_base_ids: Optional[List[str]] = None
    ) -> Tuple[List[Chunk], List[Chunk]]:
        """å¹¶è¡Œæ‰§è¡ŒFAQå’ŒAPIæœç´¢"""
        with ThreadPoolExecutor(max_workers=2) as executor:
            # æäº¤ä¸¤ä¸ªä»»åŠ¡
            faq_future = executor.submit(self.search_qa, query, knowledge_base_ids)
            api_future = executor.submit(self.locate_api, query, knowledge_base_ids)
            
            # ç­‰å¾…ä¸¤ä¸ªä»»åŠ¡å®Œæˆå¹¶è·å–ç»“æœ
            faq_results = faq_future.result()
            api_results = api_future.result()
            
        return faq_results, api_results

    def do_generate(
        self,
        input_messages: List[ChatMessage],
        use_reranker: bool = True,
        reranker_info: Optional[RerankerInfo] = None,
        knowledge_base_ids: Optional[List[str]] = None,
        thinking: bool = False,
    ) -> DataQaChatCompletionResponse:
        """ç”Ÿæˆæ•°æ®é—®ç­”å“åº” - ä¿®æ”¹ï¼šå®ç°çœŸæ­£çš„å¹¶è¡Œæ‰§è¡Œ"""
        
        # æ„é€ è¯·æ±‚å¯¹è±¡ - ä¿æŒä¸å˜
        request = DataQACompletionRequest(
            messages=input_messages,
            knowledge_base_ids=knowledge_base_ids or [],
            use_reranker=use_reranker,
            reranker_info=reranker_info,
            follow_up_num=0,
            thinking=thinking,
        )
        
        if reranker_info:
            self.config.enable_reranker = use_reranker
            self.config.reranking_threshold = reranker_info.threshold
        
        extracted_messages = self._extract_input_messages(request)
        logger.info(f"è¾“å…¥æ¶ˆæ¯: {extracted_messages}")
        
        # Step 1: é—®é¢˜æ”¹å†™ - ä¿æŒä¸å˜
        optimized_query = self.modify_query(
            input_messages=extracted_messages,
            enable_follow_up=True,
        )
        
        follow_up_response, optimized_query = self._handle_follow_up(
            optimized_query=optimized_query,
            request=request
        )
        if follow_up_response:
            return follow_up_response
        
        step1 = self._create_step(
            WorkflowStepType.MODIFY_QUERY, 1, optimized_query.rewritten_query
        )
        
        # Step 2: å®ä½“è¯†åˆ« - ä¿æŒä¸å˜
        query = optimized_query.rewritten_query
        entity_enriched_query = self.entity_recognition(query)
        step2 = self._create_step(
            WorkflowStepType.ENTITY_RECOGNITION, 2, entity_enriched_query
        )
        
        # Step 3: ä¸šåŠ¡çŸ¥è¯†æœç´¢ - ä¿æŒä¸å˜
        business_context = self.business_info_search(
            entity_enriched_query, 
            top_k=2, 
            knowledge_base_ids=knowledge_base_ids
        )
        step3 = self._create_step(
            WorkflowStepType.BUSINESS_INFO, 
            3, 
            f"æ‰¾åˆ°{len(business_context)}æ¡ä¸šåŠ¡çŸ¥è¯†"
        )
        
        # Step 4: çœŸæ­£çš„å¹¶è¡Œæ‰§è¡ŒFAQå’ŒAPIæœç´¢ - é‡å¤§ä¿®æ”¹
        faq_results, api_results = self._parallel_search_faq_api(
            entity_enriched_query, 
            knowledge_base_ids
        )
        
        # è·å–åˆ†æ•°
        faq_score = faq_results[0].reranking_score if faq_results else 0.0
        api_score = api_results[0].reranking_score if api_results else 0.0
        
        # åˆ¤æ–­å¿«é€Ÿè·¯å¾„ - APIä¼˜å…ˆ
        if api_results and api_score >= self.config.api_threshold:
            # APIå¿«é€Ÿè·¯å¾„
            step4 = self._create_step(
                WorkflowStepType.LOCATE_API,
                4,
                f"å‘½ä¸­APIï¼ˆç›¸ä¼¼åº¦: {api_score:.2f}ï¼‰"
            )
            
            # æ„é€ APIå“åº”
            api_data = api_results[0].data
            api_info = {
                "api_id": api_data.api_id if hasattr(api_data, 'api_id') else "",
                "api_name": api_data.api_name if hasattr(api_data, 'api_name') else "",
                "api_description": api_data.api_description if hasattr(api_data, 'api_description') else "",
                "api_request": api_data.api_request if hasattr(api_data, 'api_request') else "",
                "api_response": api_data.api_response if hasattr(api_data, 'api_response') else "",
            }
            
            response_content = f"""æ‰¾åˆ°åŒ¹é…çš„APIæ¥å£ï¼š

APIåç§°: {api_info['api_name']}
APIæè¿°: {api_info['api_description']}
è¯·æ±‚å‚æ•°: {api_info['api_request']}
å“åº”å‚æ•°: {api_info['api_response']}

ä½¿ç”¨æ­¤APIå¯ä»¥è·å–æ‰€éœ€æ•°æ®ã€‚"""
            
            choices = [
                ChatCompletionChoice(
                    finish_reason="stop",
                    index=0,
                    message=ChatMessage(
                        role="assistant",
                        content=response_content,
                        reasoning_content=None,
                        is_follow_up=False,
                    ),
                )
            ]
            
            return DataQaChatCompletionResponse(
                model="api_call",
                created=int(datetime.now().timestamp()),
                choices=choices,
                usage=ChatUsage(
                    prompt_tokens=0,
                    completion_tokens=0,
                    total_tokens=0
                ),
                steps=[step1, step2, step3, step4],
            )
        
        elif faq_results and faq_score >= self.config.fqa_answer_threshold:
            # FAQå¿«é€Ÿè·¯å¾„
            step4 = self._create_step(
                WorkflowStepType.SEARCH_FAQ,
                4,
                f"å‘½ä¸­å¸¸ç”¨æŸ¥è¯¢ï¼ˆç›¸ä¼¼åº¦: {faq_score:.2f}ï¼‰"
            )
            
            response_content = faq_results[0].data.answer
            
            choices = [
                ChatCompletionChoice(
                    finish_reason="stop",
                    index=0,
                    message=ChatMessage(
                        role="assistant",
                        content=response_content,
                        reasoning_content=None,
                        is_follow_up=False,
                    ),
                )
            ]
            
            return DataQaChatCompletionResponse(
                model="faq_sql",
                created=int(datetime.now().timestamp()),
                choices=choices,
                usage=ChatUsage(
                    prompt_tokens=0,
                    completion_tokens=0,
                    total_tokens=0
                ),
                steps=[step1, step2, step3, step4],
            )
        
        # æ²¡æœ‰å¿«é€Ÿè·¯å¾„ï¼Œç»§ç»­å®Œæ•´æµç¨‹
        if faq_results and faq_score >= self.config.fqa_reference_threshold:
            step4 = self._create_step(
                WorkflowStepType.SEARCH_FAQ,
                4,
                f"æ‰¾åˆ°ç›¸å…³FAQï¼ˆç›¸ä¼¼åº¦: {faq_score:.2f}ï¼‰ï¼Œä½œä¸ºå‚è€ƒ"
            )
        else:
            step4 = self._create_step(
                WorkflowStepType.SEARCH_FAQ, 4, "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³FAQ"
            )
            faq_results = None  # åˆ†æ•°å¤ªä½ï¼Œä¸ä½¿ç”¨
        
        # Step 5: è¡¨æ ¼å®šä½ - ä¿æŒä¸å˜
        located_tables = self.locate_table(
            entity_enriched_query, request, knowledge_base_ids
        )
        step5 = self._create_step(
            WorkflowStepType.LOCATE_TABLE,
            5,
            [{"table_name": t["table_name"], "score": t["score"]} for t in located_tables]
        )
        
        # Step 6: SQLç”Ÿæˆ - ä¿æŒä¸å˜
        enhanced_input_messages = copy.deepcopy(extracted_messages)
        enhanced_input_messages[-1].content = entity_enriched_query
        
        table_schema = ""
        if located_tables:
            schemas = []
            for table in located_tables[:2]:
                schemas.append(f"è¡¨å: {table['table_name']}\n{table['table_schema']}")
            table_schema = "\n\n".join(schemas)
        
        response = self.generate_sql(
            table_schema=table_schema,
            input_messages=enhanced_input_messages,
            faq_results=faq_results,
            business_context=business_context,
            thinking=thinking,
        )
        
        step6 = self._create_step(
            WorkflowStepType.GENERATE_SQL, 6, "SQLç”Ÿæˆå®Œæˆ"
        )
        
        # æ„é€ æœ€ç»ˆå“åº” - ä¿æŒä¸å˜
        usage = ChatUsage(
            prompt_tokens=response.usage.prompt_tokens,
            completion_tokens=response.usage.completion_tokens,
            total_tokens=response.usage.total_tokens,
        )
        
        choices = [
            ChatCompletionChoice(
                finish_reason=choice.finish_reason,
                index=choice.index,
                message=ChatMessage(
                    role=choice.message.role,
                    content=choice.message.content,
                    reasoning_content=choice.message.reasoning_content,
                    is_follow_up=False,
                ),
            )
            for choice in response.choices
        ]
        
        return DataQaChatCompletionResponse(
            id=response.id,
            model=response.model,
            created=response.created,
            choices=choices,
            usage=usage,
            steps=[step1, step2, step3, step4, step5, step6],
        )

    async def do_stream(
        self,
        input_messages: List[ChatMessage],
        use_reranker: bool = True,
        reranker_info: Optional[RerankerInfo] = None,
        knowledge_base_ids: Optional[List[str]] = None,
        thinking: bool = False,
    ):
        """æµå¼ç”Ÿæˆ - ä¿æŒä¸å˜"""
        try:
            # æ‰§è¡Œå®Œæ•´å·¥ä½œæµ
            response = self.do_generate(
                input_messages=input_messages,
                use_reranker=use_reranker,
                reranker_info=reranker_info,
                knowledge_base_ids=knowledge_base_ids,
                thinking=thinking,
            )
            
            # å¦‚æœæ˜¯è¿½é—®ï¼Œç›´æ¥è¿”å›
            if response.follow_up_num > 0:
                yield f"data: {response.model_dump_json()}\n\n"
                yield "data: [DONE]\n\n"
                return
            
            # æµå¼è¾“å‡ºç»“æœ
            if response.choices and response.choices[0].message.content:
                content = response.choices[0].message.content
                # åˆ†å—è¾“å‡º
                chunk_size = 50
                for i in range(0, len(content), chunk_size):
                    chunk_content = content[i:i+chunk_size]
                    chunk_response = {
                        "id": response.id,
                        "object": "chat.completion.chunk",
                        "created": response.created,
                        "model": response.model,
                        "choices": [{
                            "index": 0,
                            "delta": {"content": chunk_content},
                            "finish_reason": None
                        }]
                    }
                    yield f"data: {json.dumps(chunk_response, ensure_ascii=False)}\n\n"
                
                # å‘é€å®Œæˆä¿¡å·
                final_chunk = {
                    "id": response.id,
                    "object": "chat.completion.chunk",
                    "created": response.created,
                    "model": response.model,
                    "choices": [{
                        "index": 0,
                        "delta": {},
                        "finish_reason": "stop"
                    }],
                    "usage": response.usage.model_dump() if response.usage else None,
                    "steps": [step.model_dump() for step in response.steps] if response.steps else None
                }
                yield f"data: {json.dumps(final_chunk, ensure_ascii=False)}\n\n"
            
            yield "data: [DONE]\n\n"
            
        except Exception as e:
            logger.error(f"æµå¼ç”Ÿæˆé”™è¯¯: {e}")
            error_response = {
                "error": {
                    "message": str(e),
                    "type": "stream_error"
                }
            }
            yield f"data: {json.dumps(error_response, ensure_ascii=False)}\n\n"
            yield "data: [DONE]\n\n"



"""

# -*- coding: utf-8 -*-
"""
ç«¯åˆ°ç«¯æµ‹è¯•è„šæœ¬
æµ‹è¯•å·¥ä½œæµçš„æ‰€æœ‰è·¯å¾„ï¼šAPIå¿«é€Ÿè·¯å¾„ã€FAQå¿«é€Ÿè·¯å¾„ã€å®Œæ•´SQLç”Ÿæˆ
"""

import json
import time
import re
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from datetime import datetime

# å¯¼å…¥æ–°çš„å·¥ä½œæµ
from app.core.data.workflow_0901_parallel import DataQaWorkflow, WorkflowConfig
from app.core.components import qwen3_llm, qwen3_thinking_llm, embedder
from czce_ai.llm.message import Message as ChatMessage
import numpy as np


class WorkflowTester:
    """å·¥ä½œæµæµ‹è¯•å™¨"""
    
    def __init__(self, thinking: bool = False):
        """åˆå§‹åŒ–æµ‹è¯•å™¨"""
        # ä½¿ç”¨æ–°çš„å¹¶è¡Œå·¥ä½œæµ
        self.workflow = DataQaWorkflow(
            ans_llm=qwen3_llm,
            ans_thinking_llm=qwen3_thinking_llm,
            query_llm=qwen3_llm,
        )
        self.thinking = thinking
        self.results = []
        
    def load_test_cases(self) -> List[Dict]:
        """åŠ è½½æµ‹è¯•ç”¨ä¾‹ï¼ˆFAQé—®é¢˜ + APIæµ‹è¯•ï¼‰"""
        test_cases = []
        
        # 1. åŠ è½½FAQæµ‹è¯•é—®é¢˜
        print("åŠ è½½FAQæµ‹è¯•é—®é¢˜...")
        tables_dir = Path("test_data/tables")
        if not tables_dir.exists():
            tables_dir = Path("../test_data/tables")
        if not tables_dir.exists():
            tables_dir = Path("../../test_data/tables")
        
        if tables_dir.exists():
            for table_dir in tables_dir.iterdir():
                if not table_dir.is_dir():
                    continue
                
                for sql_file in table_dir.glob("*sql*çŸ¥è¯†åº“.txt"):
                    with open(sql_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    pattern = r'é—®é¢˜[:ï¼š](.*?)\n(?:--.*?\n)*((?:WITH|SELECT|INSERT|UPDATE|DELETE|CREATE|DROP|ALTER).*?)(?=\n\né—®é¢˜[:ï¼š]|\n\n$|$)'
                    matches = re.findall(pattern, content, re.DOTALL | re.IGNORECASE)
                    
                    for question, expected_sql in matches:
                        test_cases.append({
                            'type': 'faq',
                            'question': question.strip(),
                            'expected': expected_sql.strip(),
                            'table_name': table_dir.name
                        })
        
        # 2. æ·»åŠ APIæµ‹è¯•ç”¨ä¾‹
        print("æ·»åŠ APIæµ‹è¯•ç”¨ä¾‹...")
        api_test_cases = [
            {
                'type': 'api',
                'question': 'æŸ¥è¯¢å“ç§æ€»ä»“å•å˜åŠ¨',
                'expected': 'å“ç§æ€»ä»“å•å˜åŠ¨',  # APIåç§°
            },
            {
                'type': 'api', 
                'question': 'è·å–å•è¾¹å¸‚æƒ…å†µç»Ÿè®¡',
                'expected': 'å•è¾¹å¸‚æƒ…å†µç»Ÿè®¡',
            },
            {
                'type': 'api',
                'question': 'æŸ¥è¯¢å¥—ä¿æŠ¥è¡¨æ˜ç»†',
                'expected': 'æŸ¥è¯¢å¥—ä¿æŠ¥è¡¨æ˜ç»†',
            }
        ]
        test_cases.extend(api_test_cases)
        
        print(f"åŠ è½½äº† {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
        return test_cases
    
    def test_single_case(self, test_case: Dict) -> Dict:
        """æµ‹è¯•å•ä¸ªç”¨ä¾‹"""
        start_time = time.time()
        
        # æ„é€ æ¶ˆæ¯
        messages = [ChatMessage(role="user", content=test_case['question'])]
        
        # è°ƒç”¨å·¥ä½œæµ
        try:
            response = self.workflow.do_generate(
                input_messages=messages,
                use_reranker=True,
                knowledge_base_ids=["test_kb_id"],  # æµ‹è¯•ç”¨çŸ¥è¯†åº“ID
                thinking=self.thinking
            )
            
            # æå–ç»“æœ
            result = {
                'question': test_case['question'],
                'type': test_case['type'],
                'expected': test_case['expected'],
                'actual': '',
                'model': response.model,
                'path': self._get_path(response),
                'steps': len(response.steps) if response.steps else 0,
                'time': time.time() - start_time,
                'is_match': False,
                'match_score': 0.0
            }
            
            # æå–å®é™…ç»“æœ
            if response.choices and response.choices[0].message.content:
                result['actual'] = response.choices[0].message.content
            
            # åˆ¤æ–­åŒ¹é…
            if test_case['type'] == 'api':
                # APIæµ‹è¯•ï¼šæ£€æŸ¥æ˜¯å¦è¿”å›æ­£ç¡®çš„API
                result['is_match'] = (
                    response.model == 'api_call' and 
                    test_case['expected'] in result['actual']
                )
                result['match_score'] = 1.0 if result['is_match'] else 0.0
            else:
                # FAQ/SQLæµ‹è¯•ï¼šæ¯”è¾ƒSQL
                result['is_match'], result['match_score'] = self._compare_sql(
                    test_case['expected'], 
                    self._extract_sql(result['actual'])
                )
            
            return result
            
        except Exception as e:
            return {
                'question': test_case['question'],
                'type': test_case['type'],
                'error': str(e),
                'time': time.time() - start_time,
                'is_match': False
            }
    
    def _get_path(self, response) -> str:
        """è¯†åˆ«æ‰§è¡Œè·¯å¾„"""
        if response.model == 'api_call':
            return 'api_fast'
        elif response.model == 'faq_sql':
            return 'faq_fast'
        else:
            return 'full_sql'
    
    def _extract_sql(self, content: str) -> str:
        """æå–SQLè¯­å¥"""
        if not content:
            return ""
        
        # æå–SQLä»£ç å—
        blocks = re.findall(r"```\s*sql?\s*([\s\S]*?)```", content, re.IGNORECASE)
        if blocks:
            return blocks[0].strip()
        
        # ç›´æ¥æå–SQL
        sql_match = re.search(r'((?:WITH|SELECT)[\s\S]*?)(?:\n\n|$)', content, re.DOTALL | re.IGNORECASE)
        if sql_match:
            return sql_match.group(1).strip()
        
        return content.strip()
    
    def _compare_sql(self, expected: str, actual: str) -> Tuple[bool, float]:
        """æ¯”è¾ƒSQLï¼ˆä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦ï¼‰"""
        if not expected or not actual:
            return False, 0.0
        
        # æ ‡å‡†åŒ–
        def normalize(sql):
            sql = re.sub(r'--.*?\n', '\n', sql)
            sql = re.sub(r'\s+', ' ', sql.strip().upper())
            return sql.rstrip(';')
        
        norm_expected = normalize(expected)
        norm_actual = normalize(actual)
        
        if norm_expected == norm_actual:
            return True, 1.0
        
        # å‘é‡ç›¸ä¼¼åº¦æ¯”è¾ƒ
        try:
            exp_embedding = np.array(embedder.get_embedding(norm_expected))
            act_embedding = np.array(embedder.get_embedding(norm_actual))
            
            # ä½™å¼¦ç›¸ä¼¼åº¦
            similarity = np.dot(exp_embedding, act_embedding) / (
                np.linalg.norm(exp_embedding) * np.linalg.norm(act_embedding)
            )
            
            return similarity >= 0.85, float(similarity)
        except:
            return False, 0.0
    
    def run_batch_test(self, test_cases: Optional[List[Dict]] = None, limit: Optional[int] = None):
        """æ‰¹é‡æµ‹è¯•"""
        if test_cases is None:
            test_cases = self.load_test_cases()
        
        if limit:
            test_cases = test_cases[:limit]
        
        print(f"\nå¼€å§‹æ‰¹é‡æµ‹è¯• {len(test_cases)} ä¸ªç”¨ä¾‹...")
        print("-" * 80)
        
        for i, test_case in enumerate(test_cases, 1):
            print(f"[{i}/{len(test_cases)}] æµ‹è¯•: {test_case['question'][:50]}...")
            result = self.test_single_case(test_case)
            self.results.append(result)
            
            # å®æ—¶åé¦ˆ
            if result.get('is_match'):
                print(f"  âœ“ åŒ¹é… (è·¯å¾„: {result.get('path', 'unknown')}, è€—æ—¶: {result.get('time', 0):.2f}s)")
            else:
                print(f"  âœ— ä¸åŒ¹é… (è·¯å¾„: {result.get('path', 'unknown')}, åˆ†æ•°: {result.get('match_score', 0):.2f})")
        
        print("-" * 80)
        self._print_statistics()
    
    def test_parallel_performance(self):
        """æµ‹è¯•å¹¶è¡Œæ€§èƒ½"""
        print("\næµ‹è¯•å¹¶è¡Œæ‰§è¡Œæ€§èƒ½...")
        print("-" * 80)
        
        test_query = "ç™½ç³–æœŸè´§çš„æˆäº¤é‡æ˜¯å¤šå°‘ï¼Ÿ"
        messages = [ChatMessage(role="user", content=test_query)]
        
        # æµ‹è¯•10æ¬¡å–å¹³å‡
        times = []
        for i in range(10):
            start = time.time()
            response = self.workflow.do_generate(
                input_messages=messages,
                use_reranker=True,
                knowledge_base_ids=["test_kb_id"],
                thinking=False
            )
            elapsed = time.time() - start
            times.append(elapsed)
            print(f"  è¿è¡Œ {i+1}: {elapsed:.3f}s (è·¯å¾„: {response.model})")
        
        avg_time = sum(times) / len(times)
        print(f"\nå¹³å‡æ‰§è¡Œæ—¶é—´: {avg_time:.3f}s")
        print(f"æœ€å¿«: {min(times):.3f}s, æœ€æ…¢: {max(times):.3f}s")
    
    def _print_statistics(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        if not self.results:
            print("æ— æµ‹è¯•ç»“æœ")
            return
        
        total = len(self.results)
        matched = sum(1 for r in self.results if r.get('is_match'))
        
        # è·¯å¾„ç»Ÿè®¡
        paths = {}
        for r in self.results:
            path = r.get('path', 'error')
            paths[path] = paths.get(path, 0) + 1
        
        # ç±»å‹ç»Ÿè®¡
        types = {}
        for r in self.results:
            t = r.get('type', 'unknown')
            types[t] = types.get(t, 0) + 1
        
        print("\nğŸ“Š æµ‹è¯•ç»Ÿè®¡")
        print(f"  æ€»ç”¨ä¾‹æ•°: {total}")
        print(f"  åŒ¹é…æ•°: {matched}")
        print(f"  å‡†ç¡®ç‡: {matched/total*100:.2f}%")
        
        print("\n  æ‰§è¡Œè·¯å¾„åˆ†å¸ƒ:")
        for path, count in paths.items():
            print(f"    {path}: {count} ({count/total*100:.1f}%)")
        
        print("\n  æµ‹è¯•ç±»å‹åˆ†å¸ƒ:")
        for t, count in types.items():
            type_matched = sum(1 for r in self.results if r.get('type') == t and r.get('is_match'))
            print(f"    {t}: {count} (å‡†ç¡®ç‡: {type_matched/count*100:.1f}%)")
        
        # æ€§èƒ½ç»Ÿè®¡
        times = [r.get('time', 0) for r in self.results if 'time' in r]
        if times:
            print(f"\n  å¹³å‡æ‰§è¡Œæ—¶é—´: {sum(times)/len(times):.2f}s")
            print(f"  æœ€å¿«: {min(times):.2f}s, æœ€æ…¢: {max(times):.2f}s")
    
    def save_results(self, filename: str = None):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        if not filename:
            filename = f"test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        print(f"\nç»“æœå·²ä¿å­˜åˆ°: {filename}")


# ========== å¿«é€Ÿæµ‹è¯•å‡½æ•° ==========

def quick_test():
    """å¿«é€Ÿæµ‹è¯•ä¸»è¦åŠŸèƒ½"""
    tester = WorkflowTester(thinking=False)
    
    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        {'type': 'api', 'question': 'æŸ¥è¯¢å“ç§æ€»ä»“å•å˜åŠ¨', 'expected': 'å“ç§æ€»ä»“å•å˜åŠ¨'},
        {'type': 'faq', 'question': 'ç™½ç³–çš„æˆäº¤é‡æ˜¯å¤šå°‘ï¼Ÿ', 'expected': 'SELECT ...'},
        {'type': 'sql', 'question': 'åˆ†ææœ€è¿‘çš„æœŸè´§ä»·æ ¼è¶‹åŠ¿', 'expected': ''},
    ]
    
    print("å¿«é€Ÿæµ‹è¯•...")
    for case in test_cases:
        result = tester.test_single_case(case)
        print(f"  {case['type']}: {result.get('path', 'error')} - {result.get('is_match', False)}")


def full_test(limit: Optional[int] = None):
    """å®Œæ•´æµ‹è¯•"""
    tester = WorkflowTester(thinking=False)
    
    # æ‰¹é‡æµ‹è¯•
    tester.run_batch_test(limit=limit)
    
    # æ€§èƒ½æµ‹è¯•
    tester.test_parallel_performance()
    
    # ä¿å­˜ç»“æœ
    tester.save_results()


if __name__ == "__main__":
    # å¿«é€Ÿæµ‹è¯•
    quick_test()
    
    # å®Œæ•´æµ‹è¯•ï¼ˆé™åˆ¶100ä¸ªç”¨ä¾‹ï¼‰
    # full_test(limit=100)
    
    # å®Œæ•´æµ‹è¯•ï¼ˆæ‰€æœ‰ç”¨ä¾‹ï¼‰
    # full_test()

"""