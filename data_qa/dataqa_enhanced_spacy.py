from pathlib import Path
import sys
sys.path.append(str(Path.cwd().parent))

from typing import List, Optional
import re
import copy
import traceback
# ====== æ–°å¢ï¼šspaCyå¢å¼ºåŠŸèƒ½çš„å¯¼å…¥ ======
import json
import spacy
from spacy.pipeline import EntityRuler
# =======================================

from czce_ai.knowledge import SearchType, SQLSchemaKnowledge
from czce_ai.nlp.nlp.nlp import NLPToolkit
from czce_ai.llm.message import Message as ChatMessage
from czce_ai.llm.chat import LLMChat as LLMModel
from app.core.components import (
    mxbai_reranker,
    embedder,
    tokenizer,
)
from app.core.components.query_optimizer import (
    OptimizedQuery,
    QueryOptimizationType,
    QueryOptimizer,
)
from app.core.components import qwen3_llm, qwen3_thinking_llm
from resources import (
    USER_DICT_PATH,
    SYNONYM_DICT_PATH,
    STOP_WORDS_PATH,
    NER_PATTERNs_PATH,
)
from app.models import (
    ChatCompletionChoice,
    ChatCompletionResponse,
    ChatReference,
    ChatStep,
    ChatUsage,
)
from data_qa.prompt import dataqa_prompt
from czce_ai.utils.log import logger


class DataQaWorkflow:
    def __init__(
        self,
        ans_llm: LLMModel,
        ans_thinking_llm: LLMModel,
        query_llm: LLMModel,
        history_round: int = 1,
        reranking_threshold: float = 0.2,
        knowledge_id: Optional[str] = "3cc33ed2-21fb-4452-9e10-528867bd5f99",
        bucket_name: Optional[str] = "czce-ai-dev",
        collection: Optional[str] = "hybrid_sql"
    ):
        self.knowledge_id = knowledge_id
        self.bucket_name = bucket_name
        self.url = 'http://10.251.146.131:19530'
        self.reranking_threshold = reranking_threshold
        self.history_round = history_round
        self.ans_client = ans_llm
        self.ans_thinking_client = ans_thinking_llm
        self.collection = collection
        self.query_optimizer = QueryOptimizer(query_llm)
        
        # ====== æ–°å¢ï¼šåˆå§‹åŒ–spaCyç®¡é“ ======
        # ä¸ºå¢å¼ºå®ä½“è¯†åˆ«åŠŸèƒ½åˆå§‹åŒ–spaCyç®¡é“
        self._setup_spacy_pipeline()
        # ==================================

    # ====== æ–°å¢æ–¹æ³•ï¼šè®¾ç½®spaCyç®¡é“ ======
    def _setup_spacy_pipeline(self):
        """è®¾ç½®spaCyç®¡é“ï¼Œä½¿ç”¨zh_core_web_mdæ¨¡å‹å’Œåˆçº¦ä»£ç æ¨¡å¼
        
        æ–°å¢æ–¹æ³•ï¼šæ­¤æ–¹æ³•ç”¨äºå°†spaCyé›†æˆåˆ°å·¥ä½œæµä¸­
        """
        try:
            # åŠ è½½ä¸­æ–‡ä¸­ç­‰æ¨¡å‹ï¼ˆä¼˜å…ˆé€‰æ‹©zh_core_web_mdï¼‰
            self.nlp = spacy.load("zh_core_web_md")
            logger.info("âœ… æˆåŠŸåŠ è½½zh_core_web_mdæ¨¡å‹")
        except OSError:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°zh_core_web_mdï¼Œå°è¯•ä½¿ç”¨zh_core_web_sm")
            try:
                self.nlp = spacy.load("zh_core_web_sm")
                logger.info("âœ… æˆåŠŸåŠ è½½zh_core_web_smä½œä¸ºå¤‡é€‰")
            except OSError:
                logger.error("âŒ æœªæ‰¾åˆ°ä¸­æ–‡spaCyæ¨¡å‹")
                self.nlp = None
                return

        # åœ¨NERç»„ä»¶ä¹‹å‰æ·»åŠ EntityRulerç”¨äºåˆçº¦ä»£ç è¯†åˆ«
        ruler = self.nlp.add_pipe("entity_ruler", before="ner")
        
        # ä»ner_patterns.jsonl + æ‰‹åŠ¨è¡¥å……åˆ›å»ºåˆçº¦ä»£ç æ¨¡å¼
        patterns = self._create_contract_patterns()
        ruler.add_patterns(patterns)
        
        logger.info(f"âœ… å·²å‘spaCy EntityRuleræ·»åŠ {len(patterns)}ä¸ªæ¨¡å¼")
    # ===================================

    # ====== æ–°å¢æ–¹æ³•ï¼šä»JSONLæ–‡ä»¶è¯»å–æ¨¡å¼ ======
    def _extract_product_codes_from_jsonl(self):
        """ä»ner_patterns.jsonlæ–‡ä»¶ä¸­æå–äº§å“ä»£ç 
        
        æ–°å¢æ–¹æ³•ï¼šæ­¤æ–¹æ³•è¯»å–ç°æœ‰æ¨¡å¼å¹¶æå–äº§å“ä»£ç 
        """
        product_codes = set()
        
        try:
            if Path(NER_PATTERNs_PATH).exists():
                with open(NER_PATTERNs_PATH, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        if not line:
                            continue
                        
                        try:
                            pattern_data = json.loads(line)
                            
                            # ä»"å“ç§"æ ‡ç­¾æ¨¡å¼ä¸­æå–äº§å“ä»£ç 
                            if pattern_data.get('label') == 'å“ç§':
                                pattern = pattern_data.get('pattern')
                                
                                # å¤„ç†å­—ç¬¦ä¸²æ¨¡å¼ï¼ˆç›´æ¥äº§å“ä»£ç ï¼‰
                                if isinstance(pattern, str):
                                    # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„äº§å“ä»£ç ï¼ˆ1-3ä¸ªå¤§å†™å­—æ¯ï¼‰
                                    if re.match(r'^[A-Z]{1,3}$', pattern.upper()):
                                        product_codes.add(pattern.upper())
                                
                                # å¤„ç†åˆ—è¡¨æ¨¡å¼ï¼ˆåŸºäºtokençš„æ¨¡å¼ï¼‰
                                elif isinstance(pattern, list) and len(pattern) == 1:
                                    token_pattern = pattern[0]
                                    if isinstance(token_pattern, dict):
                                        # ä»LOWERæ¨¡å¼ä¸­æå–
                                        if 'LOWER' in token_pattern:
                                            text = token_pattern['LOWER']
                                            if re.match(r'^[a-z]{1,3}$', text):
                                                product_codes.add(text.upper())
                                        
                                        # ä»TEXTæ¨¡å¼ä¸­æå–
                                        elif 'TEXT' in token_pattern:
                                            text = token_pattern['TEXT']
                                            if re.match(r'^[A-Za-z]{1,3}$', text):
                                                product_codes.add(text.upper())
                        
                        except json.JSONDecodeError:
                            continue
            
            logger.info(f"ğŸ“ ä»ner_patterns.jsonlä¸­æå–äº†{len(product_codes)}ä¸ªäº§å“ä»£ç ")
            
        except Exception as e:
            logger.warning(f"è¯»å–ner_patterns.jsonlå¤±è´¥: {e}")
        
        return list(product_codes)
    # =========================================

    # ====== æ–°å¢æ–¹æ³•ï¼šæ‰‹åŠ¨è¡¥å…… ======
    def _get_manual_supplement_codes(self):
        """è·å–ç¼ºå¤±æˆ–æ–°äº§å“çš„æ‰‹åŠ¨è¡¥å……äº§å“ä»£ç 
        
        æ–°å¢æ–¹æ³•ï¼šæä¾›jsonlæ–‡ä»¶ä¸­å¯èƒ½æ²¡æœ‰çš„é¢å¤–äº§å“ä»£ç 
        """
        # å¯¹å¸¸è§ç¼ºå¤±æˆ–æ–°äº§å“ä»£ç çš„æ‰‹åŠ¨è¡¥å……
        # åªæ·»åŠ å¯èƒ½åœ¨ner_patterns.jsonlä¸­ç¼ºå¤±çš„ä»£ç 
        manual_codes = [
            # å¯èƒ½ç¼ºå¤±çš„å¸¸è§ä»£ç 
            'EC',    # æ¬§çº¿é›†è¿ï¼ˆè¾ƒæ–°äº§å“ï¼‰
            'BC',    # å›½é™…é“œï¼ˆè¾ƒæ–°äº§å“ï¼‰ 
            'LU',    # ä½ç¡«ç‡ƒæ–™æ²¹ï¼ˆè¾ƒæ–°äº§å“ï¼‰
            'NR',    # 20å·èƒ¶ï¼ˆè¾ƒæ–°äº§å“ï¼‰
            'SS',    # ä¸é”ˆé’¢ï¼ˆè¾ƒæ–°äº§å“ï¼‰
            'IM',    # ä¸­è¯1000ï¼ˆè¾ƒæ–°äº§å“ï¼‰
            'XS',    # å¤šæ™¶ç¡…ï¼ˆè¾ƒæ–°äº§å“ï¼‰
            'LC',    # ç¢³é…¸é”‚ï¼ˆè¾ƒæ–°äº§å“ï¼‰
            'SI',    # å·¥ä¸šç¡…ï¼ˆè¾ƒæ–°äº§å“ï¼‰
            
            # å¯èƒ½è¢«é—æ¼çš„å•å­—æ¯ä»£ç 
            'T',     # 10å¹´æœŸå›½å€º
            'L',     # èšä¹™çƒ¯
            'V',     # èšæ°¯ä¹™çƒ¯
            'A',     # è±†ä¸€
            'B',     # è±†äºŒ
            'C',     # ç‰ç±³
            'I',     # é“çŸ¿çŸ³
            'J',     # ç„¦ç‚­
            'M',     # è±†ç²•
            'P',     # æ£•æ¦ˆæ²¹
            'Y',     # è±†æ²¹
        ]
        
        logger.info(f"â• æ‰‹åŠ¨è¡¥å……ï¼š{len(manual_codes)}ä¸ªé¢å¤–çš„äº§å“ä»£ç ")
        return manual_codes
    # ===============================

    # ====== æ–°å¢æ–¹æ³•ï¼šåˆ›å»ºåˆçº¦æ¨¡å¼ ======
    def _create_contract_patterns(self):
        """é€šè¿‡ç»„åˆjsonl + æ‰‹åŠ¨è¡¥å……åˆ›å»ºåˆçº¦ä»£ç æ¨¡å¼
        
        æ–°å¢æ–¹æ³•ï¼šç»“åˆæ–‡ä»¶å’Œæ‰‹åŠ¨è¡¥å……çš„æ¨¡å¼
        """
        # æ­¥éª¤1ï¼šä»ner_patterns.jsonlæå–äº§å“ä»£ç 
        jsonl_codes = self._extract_product_codes_from_jsonl()
        
        # æ­¥éª¤2ï¼šè·å–æ‰‹åŠ¨è¡¥å……ä»£ç 
        manual_codes = self._get_manual_supplement_codes()
        
        # æ­¥éª¤3ï¼šåˆå¹¶å¹¶å»é‡
        all_codes = list(set(jsonl_codes + manual_codes))
        
        logger.info(f"ğŸ”„ åˆå¹¶äº§å“ä»£ç ï¼š{len(jsonl_codes)}ä¸ªæ¥è‡ªjsonl + {len(manual_codes)}ä¸ªæ‰‹åŠ¨ = æ€»è®¡{len(all_codes)}ä¸ª")
        
        # æ­¥éª¤4ï¼šç”Ÿæˆåˆçº¦ä»£ç æ¨¡å¼
        patterns = []
        
        for code in all_codes:
            # æ¨¡å¼1ï¼šæ ‡å‡†æ ¼å¼ï¼ˆAP2405, CU2312ï¼‰- å¤§å†™å’Œå°å†™
            patterns.extend([
                {"label": "åˆçº¦ä»£ç ", "pattern": [{"TEXT": {"REGEX": f"^{code.upper()}\\d{{4}}$"}}]},
                {"label": "åˆçº¦ä»£ç ", "pattern": [{"TEXT": {"REGEX": f"^{code.lower()}\\d{{4}}$"}}]}
            ])
            
            # æ¨¡å¼2ï¼šå¸¦åˆ†éš”ç¬¦ï¼ˆAP-2405, AP_2405, AP.2405, AP/2405ï¼‰
            for sep in ["-", "_", "\\.", "/"]:
                patterns.extend([
                    {"label": "åˆçº¦ä»£ç ", "pattern": [{"TEXT": {"REGEX": f"^{code.upper()}{sep}\\d{{4}}$"}}]},
                    {"label": "åˆçº¦ä»£ç ", "pattern": [{"TEXT": {"REGEX": f"^{code.lower()}{sep}\\d{{4}}$"}}]}
                ])
            
            # æ¨¡å¼3ï¼šç©ºæ ¼åˆ†éš”ï¼ˆAP 2405ï¼‰- ä¸¤ä¸ªtoken
            patterns.extend([
                {"label": "åˆçº¦ä»£ç ", "pattern": [
                    {"TEXT": {"REGEX": f"^{code.upper()}$"}}, 
                    {"TEXT": {"REGEX": "^\\d{4}$"}}
                ]},
                {"label": "åˆçº¦ä»£ç ", "pattern": [
                    {"TEXT": {"REGEX": f"^{code.lower()}$"}}, 
                    {"TEXT": {"REGEX": "^\\d{4}$"}}
                ]}
            ])
        
        logger.info(f"ğŸ¯ ç”Ÿæˆäº†{len(patterns)}ä¸ªåˆçº¦ä»£ç æ¨¡å¼")
        return patterns
    # ===================================

    def sql_knowledge(self):
        # åŸå§‹æ–¹æ³• - æœªä¿®æ”¹
        sql_kl = SQLSchemaKnowledge(tokenizer, embedder, self.url, mxbai_reranker)
        return sql_kl

    # ====== å¢å¼ºæ–¹æ³•ï¼šå®ä½“è¯†åˆ« ======
    def entity_recognition(self, query: str):
        """ä½¿ç”¨spaCy + åŸå§‹æ–¹æ³•çš„å¢å¼ºå®ä½“è¯†åˆ«
        
        å¢å¼ºæ–¹æ³•ï¼šæ­¤æ–¹æ³•ç°åœ¨ç»“åˆäº†spaCyå’ŒåŸå§‹NLPå·¥å…·åŒ…
        
        Args:
            query: æœ¬è½®é—®é¢˜
        Returns:
            query: å¢åŠ å®ä½“è¯†åˆ«åçš„query
        """
        
        try:
            enhanced_query = query
            
            # ====== æ–°å¢ï¼šspaCyå®ä½“è¯†åˆ« ======
            if self.nlp is not None:
                # æ­¥éª¤1ï¼šä½¿ç”¨spaCyè¿›è¡Œå®ä½“è¯†åˆ«ï¼ˆåŒ…æ‹¬åˆçº¦ä»£ç ï¼‰
                doc = self.nlp(query)
                
                # æå–å¹¶æ ‡æ³¨spaCyå®ä½“
                spacy_entities = []
                for ent in doc.ents:
                    spacy_entities.append({
                        'text': ent.text,
                        'label': ent.label_,
                        'start': ent.start_char,
                        'end': ent.end_char
                    })
                
                # åº”ç”¨spaCyå®ä½“ï¼ˆä»å³åˆ°å·¦å¤„ç†é¿å…ä½ç½®åç§»ï¼‰
                spacy_entities.sort(key=lambda x: x['start'], reverse=True)
                for entity in spacy_entities:
                    start, end = entity['start'], entity['end']
                    entity_text = entity['text']
                    label = entity['label']
                    
                    # åˆçº¦ä»£ç çš„ç‰¹æ®Šå¤„ç†ï¼šæ ‡å‡†åŒ–æ ¼å¼
                    if label == "åˆçº¦ä»£ç ":
                        # ç§»é™¤åˆ†éš”ç¬¦å¹¶è½¬æ¢ä¸ºå¤§å†™ï¼šAP-2405 â†’ AP2405
                        normalized = re.sub(r'[-_\.\s/]+', '', entity_text.upper())
                        replacement = f"{normalized}({label})"
                    else:
                        replacement = f"{entity_text}({label})"
                    
                    enhanced_query = enhanced_query[:start] + replacement + enhanced_query[end:]
                
                logger.info(f"ğŸ” spaCyæ‰¾åˆ°{len(spacy_entities)}ä¸ªå®ä½“")
            # ====================================
            
            # ====== åŸå§‹ï¼šä¼ ç»Ÿå®ä½“è¯†åˆ« ======
            # æ­¥éª¤2ï¼šåº”ç”¨åŸå§‹å®ä½“è¯†åˆ«ä»¥ä¿æŒå…¼å®¹æ€§
            # ä¿®å¤ï¼šæ·»åŠ äº†ç¼ºå¤±çš„pattetns_pathå‚æ•°
            try:
                tokenizer_nlp = NLPToolkit(
                    user_dict_path=USER_DICT_PATH, 
                    syn_dict_path=SYNONYM_DICT_PATH, 
                    stop_words_path=STOP_WORDS_PATH,
                    pattetns_path=NER_PATTERNs_PATH  # ä¿®å¤ï¼šæ·»åŠ äº†ç¼ºå¤±çš„å‚æ•°
                )
                entity_list = tokenizer_nlp.recognize(enhanced_query)
                
                # åº”ç”¨è¿˜æœªè¢«spaCyæ ‡æ³¨çš„åŸå§‹å®ä½“
                for entity in entity_list:
                    if entity['id'] != '' and entity['text'] in enhanced_query:
                        # åªåœ¨æœªè¢«æ ‡æ³¨æ—¶æ·»åŠ ï¼ˆé¿å…é‡å¤æ ‡æ³¨ï¼‰
                        if f"({entity['label']})" not in enhanced_query.replace(entity['text'], ''):
                            substring = entity['id'] + '(' + entity['label'] + ')'
                            enhanced_query = enhanced_query.replace(entity['text'], substring, 1)
                
                logger.info(f"ğŸ” åŸå§‹æ–¹æ³•æ‰¾åˆ°{len(entity_list)}ä¸ªå®ä½“")
            
            except Exception as original_error:
                logger.warning(f"åŸå§‹å®ä½“è¯†åˆ«å¤±è´¥: {original_error}")
            # ===============================
            
            logger.info(f"âœ… å®ä½“è¯†åˆ«: {query} -> {enhanced_query}")
            return enhanced_query
        
        except Exception as e:
            logger.error(f"å¢å¼ºå®ä½“è¯†åˆ«é”™è¯¯: {e}")
            traceback.print_exc()
            
            # ====== å›é€€åˆ°åŸå§‹æ–¹æ³• ======
            # ä¿®å¤ï¼šæ·»åŠ äº†ç¼ºå¤±çš„pattetns_pathå‚æ•°
            try:
                tokenizer_nlp = NLPToolkit(
                    user_dict_path=USER_DICT_PATH, 
                    syn_dict_path=SYNONYM_DICT_PATH, 
                    stop_words_path=STOP_WORDS_PATH,
                    pattetns_path=NER_PATTERNs_PATH  # ä¿®å¤ï¼šæ·»åŠ äº†ç¼ºå¤±çš„å‚æ•°
                )
                entity_list = tokenizer_nlp.recognize(query)
                for entity in entity_list:
                    if entity['id'] != '':
                        # ä»…é’ˆå¯¹å®šçš„å®ä½“è¿›è¡Œè¯†åˆ«
                        substring = entity['id'] + '(' + entity['label'] + ')'
                        query = query.replace(entity['text'], substring)
                return query
            except Exception as fallback_error:
                logger.error(f"å›é€€å®ä½“è¯†åˆ«å¤±è´¥: {fallback_error}")
                return query
    # ===============================

    def modify_query(
        self,
        input_messages: List[ChatMessage],
    ) -> OptimizedQuery:
        # åŸå§‹æ–¹æ³• - æœªä¿®æ”¹
        """é—®é¢˜æ”¹å†™
        
        Args:
            input_messages: è¾“å…¥çš„æ¶ˆæ¯åˆ—è¡¨
        Returns:
            out_messages: æœ€åä¸€ä¸ªç”¨æˆ·queryè¢«ä¿®æ”¹çš„æ¶ˆæ¯åˆ—è¡¨
        """
        
        try:
            input_messages_mq = copy.deepcopy(input_messages)
            
            optimized_query = self.query_optimizer.generate_optimized_query(
                query=input_messages[-1].content,
                chat_history=input_messages_mq[:-1],
                optimization_type=QueryOptimizationType.DATAQA,
            )
            
            return optimized_query
        
        except Exception as e:
            logger.error(f"ä¿®æ”¹æŸ¥è¯¢é”™è¯¯:{e}")
            traceback.print_exc()
            raise e

    # å®šä½è¡¨
    def locate_table(
        self,
        query: str,
    ) -> List[ChatReference]:
        # åŸå§‹æ–¹æ³• - æœªä¿®æ”¹
        """
        æ ¹æ®æŸ¥è¯¢å†…å®¹å®šä½åˆ°ç›¸å…³çš„è¡¨æ ¼
        :return: è¡¨æ ¼æ ‡é¢˜åŠåˆ†æ•°
        """
        
        sql_kl = self.sql_knowledge()  # è·å–SQLSchemaKnowledgeå®ä¾‹
        ranked_tables = sql_kl.search(
            self.collection,
            query,
            search_type=SearchType.hybrid,
            knowledge_ids=self.knowledge_id,
            top_k=3,
            use_reranker=True
        )
        
        # è·å–å‰3ä¸ªè¡¨æ ¼åŠå…¶åˆ†æ•°
        #table_names = [table.data.table_name for table in ranked_tables]
        #table_scores = [table.reranking_score for table in ranked_tables]
        #chunk_ids = [table.chunk_id for table in ranked_tables]
        #return chunk_ids[:3],table_names[:3], table_scores[:3]
        
        tables = list(
            map(
                lambda x: {'chunk_uuid':x.chunk_id,'table_name':x.data.table_name, 'score':x.reranking_score},
                ranked_tables,
            )
        )
        return tables
    

    #ç”Ÿæˆå•è¡¨æŸ¥è¯¢çš„prompt
    def generate_single_table_prompt(
        self,
        chunk_id:str
    ):
        # åŸå§‹æ–¹æ³• - æœªä¿®æ”¹
        # ä»æ•°æ®åº“ä¸­è·å–è¯¥è¡¨çš„å­—æ®µä¿¡æ¯ï¼Œè¯¥å­—æ®µä¿¡æ¯éœ€è¦åŒ…å«å®Œæ•´çš„å­—æ®µä¿¡æ¯ï¼ˆåŒ…æ‹¬è‹±æ–‡åç§°ï¼Œä¸­æ–‡åç§°ï¼Œè§£é‡Šç­‰ï¼‰
        # ç”¨fsqlç”Ÿæˆçš„promptçš„ä¸€éƒ¨åˆ†
        sql_kl = self.sql_knowledge()
        table_content = sql_kl.get_by_ids(self.collection,chunk_id)
        table_info = table_content[0].data.table_info
        # ç”Ÿæˆprompt
        table_prompt = f"å·²çŸ¥å¦‚ä¸‹æ•°æ®è¡¨ä¿¡æ¯: \n{table_info}\n"
        return table_prompt
    

    def extract_info(
        self,
        text:str,
        pattern:str
    ):
        # åŸå§‹æ–¹æ³• - æœªä¿®æ”¹
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–ä¿¡æ¯
        extract_pattern = re.compile(pattern,re.DOTALL)
        match = extract_pattern.search(text)
        if match:
            return match.group(1)
        else:
            return None


    def generate_sql_code(
        self,
        table_schema:str,
        input_messages: List[ChatMessage],
        thinking: Optional[bool] = False,
    ):
        # åŸå§‹æ–¹æ³• - æœªä¿®æ”¹
        query = input_messages[-1].content
        content=dataqa_prompt.format(table_schema=table_schema,question=query)
        #print(content)
        system_msg = ChatMessage(
            role="system",
            content=content,
        )
        if thinking is True:
            response = self.ans_thinking_client.invoke(
                messages=[system_msg] + input_messages[:]
            )
        else:
            response = self.ans_client.invoke(messages=[system_msg] + input_messages[:])
        return response
        
        '''
        result = response.choices[0].message.content
        sql_pattern = self.extract_info(result,r'```sql(.*?)```')
        analysis_pattern = self.extract_info(result,r"ä»¥ä¸‹æ˜¯é—®é¢˜åˆ†æå’Œä»£ç è§£é‡Š: (.*?)---")
        if analysis_pattern != None:
            analysis_info = analysis_pattern
        else:
            analysis_info = "æœªæ‰¾åˆ°é—®é¢˜åˆ†æä»£ç è§£é‡Š"
        if sql_pattern != None:
            sql_code = sql_pattern
        else:
            sql_code = "æœªæ‰¾åˆ°SQLä»£ç "
        return analysis_info,sql_code
        '''

    def do_generate(
        self,
        input_messages: List[ChatMessage],
        knowledge_base_ids: Optional[List[str]] = None,
        thinking: Optional[bool] = False,
    ):
        # åŸå§‹æ–¹æ³• - æœªä¿®æ”¹ï¼ˆé™¤äº†step2æ³¨é‡Šæ›´æ–°ï¼‰
        """ç”Ÿæˆå›ç­”"""
        # ä¿ç•™æœ€åä¸­é—´çš„å¯¹è¯ï¼Œä¸­é—´çš„å¯¹è¯æœ€å¤šä¿ç•™ self.history_round * 2 è½®
        if len(input_messages[1:-1]) > self.history_round * 2:
            del input_messages[1 : -1 - self.history_round * 2]

        # step1 ä¿®æ”¹æŸ¥è¯¢
        optimized_input_messages = self.modify_query(input_messages)
        step1 = ChatStep(
            key="modify_query",
            name="æ”¹å†™é—®é¢˜",
            number=1,
            prompt=optimized_input_messages.rewritten_query,
            finished=True,
        )

        # step2 æŸ¥è¯¢å®ä½“è¯†åˆ«ï¼ˆå¢å¼ºï¼šç°åœ¨ä½¿ç”¨spaCy + åŸå§‹æ–¹æ³•ï¼‰
        query = optimized_input_messages[-1].content
        entitled_query = self.entity_recognition(query)
        optimized_input_messages[-1].content = entitled_query
        step2 = ChatStep(
            key="query_entity_recognition",
            name="é—®é¢˜å®ä½“è¯†åˆ«",
            number=2,
            prompt=entitled_query,
            finished=True,
        )

        # step3 å®šä½è¡¨æ ¼
        located_table = self.locate_table(optimized_input_messages.rewritten_query)
        step3 = ChatStep(
            key="locate_table",
            name="å®šä½è¡¨æ ¼",
            number=3,
            prompt=located_table,
            finished=True,
        )

        # step4 ç”Ÿæˆå•è¡¨æç¤ºè¯
        single_table_prompt = self.generate_single_table_prompt(located_table[0]['chunk_uuid'])
        step4 = ChatStep(
            key="generate_single_table_prompt",
            name="ç”Ÿæˆå•è¡¨æç¤ºè¯",
            number=4,
            prompt=single_table_prompt,
            finished=True,
        )

        # step5 ç”ŸæˆSQL
        response = self.generate_sql_code(table_schema=single_table_prompt, input_messages=optimized_input_messages, thinking=thinking)
        step5 = ChatStep(
            key="generate_sql",
            name="ç”ŸæˆSQL",
            number=5,
            prompt=response,
            finished=True,
        )

        usage = ChatUsage(
            prompt_tokens=response.usage.prompt_tokens,
            completion_tokens=response.usage.completion_tokens,
            total_tokens=response.usage.total_tokens,
        )

        choices = list(
            map(
                lambda x: ChatCompletionChoice(
                    finish_reason=x.finish_reason,
                    index=x.index,
                    message=ChatMessage(
                        role=x.message.role,
                        content=x.message.content,
                        reasoning_content=x.message.reasoning_content,
                    ),
                ),
                response.choices,
            )
        )

        return ChatCompletionResponse(
            id=response.id,
            model=response.model,
            created=response.created,
            choices=choices,
            usage=usage,
            steps=[step1, step2, step3, step4, step5],
        )


# ====== æ–°å¢ï¼šæµ‹è¯•å‡½æ•° ======
def test_enhanced_entity_recognition():
    """æµ‹è¯•å¢å¼ºå®ä½“è¯†åˆ«ä¸spaCyé›†æˆ
    
    æ–°å¢å‡½æ•°ï¼šéªŒè¯å¢å¼ºåŠŸèƒ½çš„æµ‹è¯•å‡½æ•°
    """
    
    print("ğŸ§ª æµ‹è¯•å¢å¼ºå®ä½“è¯†åˆ«ï¼ˆspaCy + åŸå§‹æ–¹æ³•ï¼‰")
    print("=" * 80)
    
    # ç”¨äºæµ‹è¯•çš„æ¨¡æ‹ŸLLM
    class MockLLM:
        pass
    
    try:
        # åˆå§‹åŒ–å¢å¼ºå·¥ä½œæµ
        dataqa = DataQaWorkflow(MockLLM(), MockLLM(), MockLLM())
        
        test_cases = [
            # å„ç§æ ¼å¼çš„åˆçº¦ä»£ç 
            "è‹¹æœæœŸè´§AP2405çš„ä»·æ ¼èµ°åŠ¿å¦‚ä½•ï¼Ÿ",
            "è¯·æŸ¥è¯¢éƒ‘å•†æ‰€AP-2405åˆçº¦çš„æŒä»“æ•°æ®",
            "åæ³°æœŸè´§å¯¹CU_2312é“œæœŸè´§çš„åˆ†ææŠ¥å‘Š",
            "å¤§å•†æ‰€M.2405è±†ç²•æœŸè´§ä»Šæ—¥æ”¶ç›˜ä»·æ ¼",
            "ä¸ŠæœŸæ‰€RB/2405èºçº¹é’¢æœŸè´§èµ°åŠ¿åˆ†æ",
            "ä¸­é‡‘æ‰€IF 2406æ²ªæ·±300æœŸè´§åŸºå·®å˜åŒ–",
            
            # å¤šä¸ªåˆçº¦
            "æ¯”è¾ƒAP2405å’ŒCU2312ä¸¤ä¸ªåˆçº¦çš„è¡¨ç°",
            "åˆ†æap2405è‹¹æœæœŸè´§çš„å¤šç©ºæ ¼å±€",
            
            # åŒ…å«æ··åˆå®ä½“çš„å¤æ‚å¥å­
            "åæ³°æœŸè´§ç ”ç©¶æ‰€è®¤ä¸ºï¼Œéƒ‘å•†æ‰€AP-2405è‹¹æœæœŸè´§å’Œä¸ŠæœŸæ‰€CU_2312é“œæœŸè´§åœ¨å½“å‰å¸‚åœºç¯å¢ƒä¸‹å€¼å¾—å…³æ³¨ï¼Œå»ºè®®é‡ç‚¹å…³æ³¨IF 2406æ²ªæ·±300æœŸè´§çš„åŸºå·®å˜åŒ–æƒ…å†µã€‚",
            
            # è¾¹ç•Œæƒ…å†µ
            "è‹¹æœå…¬å¸è‚¡ç¥¨AAPLä»Šæ—¥èµ°åŠ¿",  # ä¸åº”åŒ¹é…åˆçº¦ä»£ç 
            "ä»Šå¤©APé”™è¿‡äº†å¥½æœºä¼š",         # ä¸å®Œæ•´ï¼Œä¸åº”åŒ¹é…
        ]
        
        for i, query in enumerate(test_cases, 1):
            print(f"\n{i:2d}. æµ‹è¯•æŸ¥è¯¢:")
            print(f"    åŸå§‹: {query}")
            
            try:
                enhanced = dataqa.entity_recognition(query)
                print(f"    å¢å¼º: {enhanced}")
                
                # åˆ†æç»“æœ
                contract_count = enhanced.count("(åˆçº¦ä»£ç )")
                other_entities = enhanced.count("(") - contract_count
                
                print(f"    ğŸ“Š è¯†åˆ«: {contract_count}ä¸ªåˆçº¦ä»£ç , {other_entities}ä¸ªå…¶ä»–å®ä½“")
                
                # æ£€æŸ¥åˆçº¦ä»£ç æ˜¯å¦å·²æ ‡å‡†åŒ–
                if contract_count > 0:
                    print(f"    âœ… åˆçº¦ä»£ç å·²è¯†åˆ«å¹¶æ ‡å‡†åŒ–")
                
            except Exception as e:
                print(f"    âŒ å¤„ç†å¤±è´¥: {e}")
        
        print(f"\nâœ… å¢å¼ºå®ä½“è¯†åˆ«æµ‹è¯•å®Œæˆï¼")
        print(f"\nğŸ’¡ å±•ç¤ºçš„å…³é”®åŠŸèƒ½:")
        print(f"   ğŸ”— spaCy zh_core_web_mdé›†æˆ")
        print(f"   ğŸ“ ä»ner_patterns.jsonlè‡ªåŠ¨åŠ è½½æ¨¡å¼")
        print(f"   â• ç¼ºå¤±ä»£ç çš„æ‰‹åŠ¨è¡¥å……")
        print(f"   ğŸ”„ æ— ç¼å›é€€åˆ°åŸå§‹æ–¹æ³•")
        print(f"   ğŸ¯ åˆçº¦ä»£ç æ ‡å‡†åŒ–ï¼ˆAP-2405 â†’ AP2405ï¼‰")
        print(f"   ğŸ›¡ï¸ ä¿æŒå‘åå…¼å®¹æ€§")
        print(f"   âœ… ä¿ç•™æ‰€æœ‰åŸå§‹NLPToolkitå‚æ•°")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        print(f"ğŸ’¡ è¯·ç¡®ä¿å®‰è£…: pip install spacy && python -m spacy download zh_core_web_md")


if __name__ == "__main__":
    test_enhanced_entity_recognition()
    
    '''
    # ç”Ÿäº§ç¯å¢ƒä½¿ç”¨ç¤ºä¾‹:
    dataqa = DataQaWorkflow(
        ans_llm=qwen3_llm,
        ans_thinking_llm=qwen3_thinking_llm,
        query_llm=qwen3_llm
    )
    
    # æµ‹è¯•å®é™…æŸ¥è¯¢
    query = "åæ³°æœŸè´§å¯¹éƒ‘å•†æ‰€AP-2405è‹¹æœæœŸè´§çš„åˆ†ææŠ¥å‘Š"
    enhanced_query = dataqa.entity_recognition(query)
    print(f"æŸ¥è¯¢: {query}")
    print(f"å¢å¼º: {enhanced_query}")
    '''