from pathlib import Path
import sys
sys.path.append(str(Path.cwd().parent.parent))

from typing import List, Optional, Dict, Tuple, Set
import re
import copy
import json
import traceback

from czce_ai.knowledge import SearchType, SQLSchemaKnowledge
from czce_ai.nlp import NLPToolkit
from czce_ai.llm.message import Message as ChatMessage
from czce_ai.llm.chat import LLMChat as LLMModel
from app.core.components import (
    mxbai_reranker,
    embedder,
    tokenizer,
)
from app.core.components.query_optimizer import (
    OptimizedQuery,
    QueryOptimizationType,
    QueryOptimizer,
)
from app.core.components import qwen3_llm, qwen3_thinking_llm
from resources import (
    USER_DICT_PATH,
    SYNONYM_DICT_PATH,
    STOP_WORDS_PATH,
    NER_PATTERNs_PATH,
)
from app.models import (
    ChatCompletionChoice,
    ChatCompletionResponse,
    ChatReference,
    ChatStep,
    ChatUsage,
)
from data_qa.prompt import dataqa_prompt
from czce_ai.utils.log import logger


class EnhancedContractCodeNER:
    """
    å¢å¼ºçš„åˆçº¦ä»£ç è¯†åˆ«å™¨ - ä¸“é—¨è§£å†³åŸå§‹NLPToolkitæ— æ³•è¯†åˆ«åˆçº¦ä»£ç çš„é—®é¢˜
    
    æ ¸å¿ƒåŠŸèƒ½ï¼šè¯†åˆ«å„ç§æ ¼å¼çš„åˆçº¦ä»£ç ï¼ˆAP2502ã€CU-2405ã€M 2501ç­‰ï¼‰
    è®¾è®¡ç›®æ ‡ï¼šè¡¥å……åŸå§‹NLPToolkitçš„ä¸è¶³ï¼Œå®ç°å®Œæ•´çš„å®ä½“è¯†åˆ«è¦†ç›–
    """
    
    def __init__(self, ner_patterns_path: str):
        """
        åˆå§‹åŒ–å¢å¼ºçš„åˆçº¦ä»£ç è¯†åˆ«å™¨
        
        Args:
            ner_patterns_path: NERæ¨¡å¼é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆJSONLæ ¼å¼ï¼‰
        """
        # ä»é…ç½®æ–‡ä»¶åŠ¨æ€åŠ è½½æœ‰æ•ˆçš„åˆçº¦å“ç§å‰ç¼€
        self.valid_contract_prefixes = self._load_contract_prefixes(ner_patterns_path)
        
        # é€šç”¨åˆçº¦ä»£ç åŒ¹é…æ¨¡å¼ï¼ˆåŸºäºä¸­å›½æœŸè´§å¸‚åœºçš„çœŸå®è§„å¾‹ï¼‰
        # æ ¼å¼è§„å¾‹ï¼šå“ç§ä»£ç (1-3ä¸ªå­—æ¯) + å¹´ä»½(00-99) + æœˆä»½(01-12)
        self.contract_code_patterns = [
            # æ ‡å‡†æ ¼å¼ï¼šAP2501, CU2405, SR2412 ç­‰
            r'(?<![A-Za-z])([A-Z]{1,3})(([0-9]{2})(0[1-9]|1[0-2]))(?![A-Za-z0-9])',      # å¤§å†™ï¼šAP2501
            r'(?<![A-Za-z])([a-z]{1,3})(([0-9]{2})(0[1-9]|1[0-2]))(?![A-Za-z0-9])',      # å°å†™ï¼šap2501
            
            # # å˜ä½“æ ¼å¼ï¼šæ”¯æŒå„ç§åˆ†éš”ç¬¦
            # r'(?<![A-Za-z])([A-Z]{1,3})(?:\s*[-_./]\s*)(([0-9]{2})(0[1-9]|1[0-2]))(?![A-Za-z0-9])',  # AP-2501
            # r'(?<![A-Za-z])([a-z]{1,3})(?:\s*[-_./]\s*)(([0-9]{2})(0[1-9]|1[0-2]))(?![A-Za-z0-9])',  # ap-2501

            # r'(?<![A-Za-z])([A-Z]{1,3})(?:\s+)(([0-9]{2})(0[1-9]|1[0-2]))(?![A-Za-z0-9])',           # AP 2501
            # r'(?<![A-Za-z])([a-z]{1,3})(?:\s+)(([0-9]{2})(0[1-9]|1[0-2]))(?![A-Za-z0-9])',           # ap 2501

            # r'(?<![A-Za-z])([A-Z]{1,3})(?:\s*\(\s*)(([0-9]{2})(0[1-9]|1[0-2]))(?:\s*\))(?![A-Za-z0-9])', # AP(2501)
            # r'(?<![A-Za-z])([a-z]{1,3})(?:\s*\(\s*)(([0-9]{2})(0[1-9]|1[0-2]))(?:\s*\))(?![A-Za-z0-9])', # ap(2501)
            
            # # å¹´æœˆåˆ†ç¦»æ ¼å¼:
            # r'(?<![A-Za-z])([A-Z]{1,3})(?:\s*)(([0-9]{2})(?:\s*[-_./]\s*)(0[1-9]|1[0-2]))(?![A-Za-z0-9])',  # AP24-01
            # r'(?<![A-Za-z])([a-z]{1,3})(?:\s*)(([0-9]{2})(?:\s*[-_./]\s*)(0[1-9]|1[0-2]))(?![A-Za-z0-9])',  # ap24-01
        ]
        
        # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ä»¥æé«˜è¿è¡Œæ—¶æ€§èƒ½
        self.compiled_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.contract_code_patterns]
        
        # è¯¯è¯†åˆ«è¿‡æ»¤ä¸Šä¸‹æ–‡ï¼ˆé¿å…åœ¨éæœŸè´§åœºæ™¯ä¸­è¯¯è¯†åˆ«ï¼‰
        self.false_positive_contexts = [
            r'å…¬å¸',      # é¿å…è¯†åˆ«"APå…¬å¸"ä¸­çš„"AP"
            r'è‚¡ç¥¨',      # é¿å…è¯†åˆ«è‚¡ç¥¨ä»£ç 
            r'åŸºé‡‘',      # é¿å…è¯†åˆ«åŸºé‡‘ä»£ç 
            r'å€ºåˆ¸',      # é¿å…è¯†åˆ«å€ºåˆ¸ä»£ç 
            r'è‚¡ä»·',      # é¿å…åœ¨è‚¡ä»·è®¨è®ºä¸­è¯¯è¯†åˆ«
            r'è¯åˆ¸',      # é¿å…åœ¨è¯åˆ¸è®¨è®ºä¸­è¯¯è¯†åˆ«
        ]
        
        logger.info(f"Enhanced Contract Code NER initialized with {len(self.valid_contract_prefixes)} valid prefixes")
        logger.debug(f"Valid contract prefixes: {sorted(list(self.valid_contract_prefixes)[:10])}... (showing first 10)")
    
    def _load_contract_prefixes(self, patterns_file_path: str) -> Set[str]:
        """
        ä»NERæ¨¡å¼é…ç½®æ–‡ä»¶ï¼ˆJSONLæ ¼å¼ï¼‰ä¸­æå–æ‰€æœ‰æœ‰æ•ˆçš„åˆçº¦å“ç§å‰ç¼€
        
        Args:
            patterns_file_path: JSONLæ ¼å¼çš„NERé…ç½®æ–‡ä»¶è·¯å¾„
            
        Returns:
            åŒ…å«æ‰€æœ‰æœ‰æ•ˆåˆçº¦ä»£ç å‰ç¼€çš„é›†åˆ
        """
        try:
            contract_prefixes = set()
            
            # è¯»å–JSONLæ ¼å¼æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼‰
            with open(patterns_file_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if not line:  # è·³è¿‡ç©ºè¡Œ
                        continue
                        
                    try:
                        pattern_entry = json.loads(line)
                        
                        # åªå¤„ç†"å“ç§"ç±»å‹çš„å®ä½“
                        if pattern_entry.get('label') == 'å“ç§':
                            # ä»patternå­—æ®µæå–
                            pattern_value = pattern_entry.get('pattern')
                            if isinstance(pattern_value, str):
                                # ç®€å•å­—ç¬¦ä¸²æ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨
                                contract_prefixes.add(pattern_value.upper())
                            elif isinstance(pattern_value, list) and len(pattern_value) > 0:
                                # å¤æ‚æ¨¡å¼ï¼šæå–LOWERå­—æ®µå€¼
                                first_pattern = pattern_value[0]
                                if isinstance(first_pattern, dict) and 'LOWER' in first_pattern:
                                    contract_prefixes.add(first_pattern['LOWER'].upper())
                                    
                    except json.JSONDecodeError as e:
                        logger.warning(f"Failed to parse line {line_num} in {patterns_file_path}: {e}")
                        continue
            
            # æ‰‹åŠ¨è¡¥å……ä¸€äº›é‡è¦çš„å•å­—æ¯å“ç§å‰ç¼€ï¼ˆåŸºäºå®é™…æœŸè´§å¸‚åœºï¼‰
            # è¿™äº›å¯èƒ½åœ¨é…ç½®æ–‡ä»¶ä¸­è¢«é—æ¼ï¼Œä½†åœ¨å®é™…äº¤æ˜“ä¸­å¾ˆé‡è¦
            important_single_letters = {'M', 'Y', 'C', 'A', 'I', 'J', 'L', 'V', 'P', 'T', 'B'}
            contract_prefixes.update(important_single_letters)
            
            # è¡¥å……ä¸€äº›å¸¸è§çš„æœŸè´§å“ç§ä»£ç 
            common_prefixes = {
                # éƒ‘å•†æ‰€ä¸»è¦å“ç§
                'AP', 'CF', 'CY', 'FG', 'JR', 'LR', 'MA', 'OI', 'PK', 'PM', 
                'RI', 'RM', 'RS', 'SF', 'SM', 'SR', 'TA', 'UR', 'WH', 'ZC',
                'CJ', 'SA', 'PF', 'SH', 'PX', 'PL',
                
                # ä¸ŠæœŸæ‰€ä¸»è¦å“ç§
                'CU', 'AL', 'ZN', 'PB', 'NI', 'SN', 'AU', 'AG', 'RB', 'WR',
                'HC', 'SS', 'FU', 'RU', 'BU', 'NR', 'SP', 'BC', 'LU', 'AO', 'BR',
                
                # å¤§å•†æ‰€ä¸»è¦å“ç§
                'A', 'B', 'C', 'CS', 'M', 'Y', 'P', 'FB', 'BB', 'JD', 'LH',
                'L', 'V', 'PP', 'J', 'JM', 'I', 'EG', 'EB', 'PG',
                
                # ä¸­é‡‘æ‰€ä¸»è¦å“ç§
                'IF', 'IC', 'IH', 'IM', 'TS', 'TF', 'T', 'TL',
                
                # å¹¿æœŸæ‰€ä¸»è¦å“ç§
                'SI', 'LC', 'PS'
            }
            contract_prefixes.update(common_prefixes)
            
            logger.info(f"Loaded {len(contract_prefixes)} contract prefixes from configuration file and manually added common prefixes")
            return contract_prefixes
            
        except Exception as e:
            logger.error(f"Failed to load contract prefixes from {patterns_file_path}: {e}")
            # ä½¿ç”¨åŸºç¡€çš„å›é€€å‰ç¼€é›†åˆ
            fallback_prefixes = {
                'AP', 'CU', 'SR', 'TA', 'MA', 'RB', 'IF', 'IC', 'IH', 'M', 'Y', 'C', 
                'A', 'I', 'J', 'AL', 'ZN', 'AG', 'AU', 'CF', 'FG', 'ZC'
            }
            logger.warning(f"Using fallback contract prefixes ({len(fallback_prefixes)} items)")
            return fallback_prefixes
    
    def _is_valid_contract_prefix(self, prefix: str) -> bool:
        """éªŒè¯å“ç§ä»£ç å‰ç¼€æ˜¯å¦æœ‰æ•ˆ"""
        return prefix.upper() in self.valid_contract_prefixes
    
    def _is_valid_month(self, month_str: str) -> bool:
        """éªŒè¯æœˆä»½æ˜¯å¦æœ‰æ•ˆï¼ˆ01-12ï¼‰"""
        try:
            month = int(month_str)
            return 1 <= month <= 12
        except (ValueError, TypeError):
            return False
    
    def _should_filter_by_context(self, match_text: str, full_text: str, start_pos: int, end_pos: int) -> bool:
        """
        åŸºäºä¸Šä¸‹æ–‡è¿‡æ»¤è¯¯è¯†åˆ«
        
        ä¾‹å¦‚ï¼š"è‹¹æœå…¬å¸AP2501ä¸šç»©" ä¸­çš„AP2501åº”è¯¥è¢«è¿‡æ»¤ï¼Œå› ä¸ºä¸Šä¸‹æ–‡æ˜¯"å…¬å¸"
        """
        context_window = 30
        context_start = max(0, start_pos - context_window)
        context_end = min(len(full_text), end_pos + context_window)
        context = full_text[context_start:context_end]
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«è¯¯è¯†åˆ«çš„ä¸Šä¸‹æ–‡å…³é”®è¯
        for fp_context in self.false_positive_contexts:
            if re.search(fp_context, context, re.IGNORECASE):
                logger.debug(f"Filtering potential false positive: '{match_text}' in context: '{context[:50]}...'")
                return True
        return False
    
    def find_contract_codes(self, text: str, existing_entities: List[Dict]) -> List[Dict]:
        """
        åœ¨æ–‡æœ¬ä¸­æŸ¥æ‰¾æ‰€æœ‰åˆçº¦ä»£ç 
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            existing_entities: å·²ç»è¯†åˆ«çš„å®ä½“åˆ—è¡¨ï¼ˆç”¨äºé¿å…é‡å ï¼‰
            
        Returns:
            è¯†åˆ«åˆ°çš„åˆçº¦ä»£ç å®ä½“åˆ—è¡¨
        """
        found_contracts = []
        existing_spans = set()
        
        # è®°å½•å·²è¯†åˆ«å®ä½“çš„ä½ç½®èŒƒå›´ï¼Œé¿å…é‡å è¯†åˆ«
        for entity in existing_entities:
            if 'start' in entity and 'end' in entity:
                existing_spans.add((entity['start'], entity['end']))
        
        # ä½¿ç”¨æ‰€æœ‰ç¼–è¯‘å¥½çš„æ­£åˆ™æ¨¡å¼è¿›è¡ŒåŒ¹é…
        for pattern_idx, pattern in enumerate(self.compiled_patterns):
            for match in pattern.finditer(text):
                full_match = match.group()
                groups = match.groups()
                start_pos = match.start()
                end_pos = match.end()
                
                # æ£€æŸ¥æ˜¯å¦ä¸å·²å­˜åœ¨çš„å®ä½“é‡å 
                is_overlapping = any(
                    not (end_pos <= existing[0] or start_pos >= existing[1])
                    for existing in existing_spans
                )
                
                if is_overlapping:
                    logger.debug(f"Skipping overlapping match: '{full_match}' at [{start_pos}:{end_pos}]")
                    continue
                
                # è§£æåŒ¹é…çš„ç»„æˆéƒ¨åˆ†
                prefix = None
                year = None
                month = None
                
                if len(groups) >= 4:
                    # æ ‡å‡†æ ¼å¼ï¼š(å‰ç¼€, å®Œæ•´æ—¥æœŸ, å¹´ä»½, æœˆä»½)
                    prefix = groups[0]
                    full_date = groups[1]
                    year = groups[2]
                    month = groups[3]
                elif len(groups) >= 3:
                    # åˆ†ç¦»æ ¼å¼ï¼š(å‰ç¼€, å¹´ä»½, æœˆä»½)
                    prefix = groups[0]
                    year = groups[1] 
                    month = groups[2]
                    full_date = year + month
                else:
                    logger.debug(f"Unexpected match groups for '{full_match}': {groups}")
                    continue
                
                # éªŒè¯å“ç§ä»£ç å‰ç¼€
                if not self._is_valid_contract_prefix(prefix):
                    logger.debug(f"Invalid contract prefix '{prefix}' in '{full_match}'")
                    continue
                
                # éªŒè¯æœˆä»½æœ‰æ•ˆæ€§
                if not self._is_valid_month(month):
                    logger.debug(f"Invalid month '{month}' in '{full_match}'")
                    continue
                
                # åŸºäºä¸Šä¸‹æ–‡è¿‡æ»¤è¯¯è¯†åˆ«
                if self._should_filter_by_context(full_match, text, start_pos, end_pos):
                    continue
                
                # æ„å»ºæ ‡å‡†åŒ–çš„åˆçº¦ä»£ç 
                normalized_code = f"{prefix.upper()}{year}{month}"
                
                # æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
                contract_info = {
                    'text': full_match,
                    'normalized': normalized_code,
                    'id': 'åˆçº¦ä»£ç ',
                    'label': 'åˆçº¦ä»£ç ', 
                    'start': start_pos,
                    'end': end_pos,
                    'confidence': 0.95,  # é«˜ç½®ä¿¡åº¦ï¼ˆç»è¿‡å¤šé‡éªŒè¯ï¼‰
                    'prefix': prefix.upper(),
                    'year': year,
                    'month': month,
                    'pattern_index': pattern_idx  # è®°å½•æ˜¯å“ªä¸ªæ¨¡å¼åŒ¹é…çš„
                }
                
                found_contracts.append(contract_info)
                existing_spans.add((start_pos, end_pos))
                
                logger.debug(f"Valid contract code found: '{full_match}' -> {normalized_code} (pattern {pattern_idx})")
        
        logger.info(f"Enhanced NER found {len(found_contracts)} contract codes in text: '{text[:50]}...'")
        return found_contracts


class DataQaWorkflow:
    """
    å¢å¼ºç‰ˆDataQAå·¥ä½œæµ
    
    æ ¸å¿ƒæ”¹è¿›ï¼šé›†æˆäº†å¢å¼ºçš„åˆçº¦ä»£ç è¯†åˆ«åŠŸèƒ½ï¼Œè§£å†³åŸå§‹NLPToolkitæ— æ³•è¯†åˆ«åˆçº¦ä»£ç çš„é—®é¢˜
    è®¾è®¡åŸåˆ™ï¼šä¿æŒä¸åŸå§‹DataQaWorkflowçš„å®Œå…¨å…¼å®¹æ€§ï¼Œåªå¢å¼ºå®ä½“è¯†åˆ«èƒ½åŠ›
    """
    
    def __init__(
        self,
        ans_llm: LLMModel,
        ans_thinking_llm: LLMModel,
        query_llm: LLMModel,
        history_round: int = 1,
        reranking_threshold: float = 0.2,
        knowledge_id: Optional[str] = "3cc33ed2-21fb-4452-9e10-528867bd5f99",
        bucket_name: Optional[str] = "czce-ai-dev",
        collection: Optional[str] = "hybrid_sql"
    ):
        # ä¿æŒæ‰€æœ‰åŸæœ‰å‚æ•°ä¸å˜
        self.knowledge_id = knowledge_id
        self.bucket_name = bucket_name
        self.url = 'http://10.251.146.131:19530'
        self.reranking_threshold = reranking_threshold
        self.history_round = history_round
        self.ans_client = ans_llm
        self.ans_thinking_client = ans_thinking_llm
        self.collection = collection
        self.query_optimizer = QueryOptimizer(query_llm)
        
        # æ–°å¢ï¼šåˆå§‹åŒ–å¢å¼ºçš„åˆçº¦ä»£ç è¯†åˆ«å™¨
        try:
            self.enhanced_contract_ner = EnhancedContractCodeNER(NER_PATTERNs_PATH)
            logger.info("Enhanced Contract Code NER initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize Enhanced Contract Code NER: {e}")
            self.enhanced_contract_ner = None

    def sql_knowledge(self):
        """ä¿æŒåŸæœ‰æ–¹æ³•å®Œå…¨ä¸å˜"""
        sql_kl = SQLSchemaKnowledge(tokenizer, embedder, self.url, mxbai_reranker)
        return sql_kl

    def _resolve_entity_conflicts(self, entities: List[Tuple]) -> List[Tuple]:
        """
        æ™ºèƒ½è§£å†³å®ä½“å†²çªï¼ˆåŸºäºæ‚¨åœ¨enhanced_ner_benchmark.pyä¸­éªŒè¯çš„ç®—æ³•ï¼‰
        
        å†²çªè§£å†³ä¼˜å…ˆçº§ï¼š
        1. åˆçº¦ä»£ç  > å…¶ä»–å®ä½“ç±»å‹
        2. é•¿å®ä½“ > çŸ­å®ä½“  
        3. å®Œæ•´åŒ…å«çš„å®ä½“ä¸­ï¼Œå¤–å±‚å®ä½“ä¼˜å…ˆ
        
        Args:
            entities: å®ä½“åˆ—è¡¨ï¼Œæ ¼å¼ä¸º(text, label, start, end)
            
        Returns:
            è§£å†³å†²çªåçš„å®ä½“åˆ—è¡¨
        """
        if not entities:
            return []

        # æŒ‰ä½ç½®æ’åºï¼Œä½ç½®ç›¸åŒæ—¶æŒ‰é•¿åº¦å€’åºï¼ˆé•¿å®ä½“ä¼˜å…ˆï¼‰
        entities.sort(key=lambda x: (x[2], -len(x[0])))
        
        resolved = []
        for current in entities:
            current_text, current_label, current_start, current_end = current
            should_add = True
            
            # ä¸å·²è§£å†³çš„å®ä½“æ£€æŸ¥å†²çª
            for i, existing in enumerate(list(resolved)):
                existing_text, existing_label, existing_start, existing_end = existing
                
                # æ£€æŸ¥æ˜¯å¦é‡å 
                if not (current_end <= existing_start or current_start >= existing_end):
                    # æœ‰é‡å ï¼Œåº”ç”¨å†²çªè§£å†³è§„åˆ™
                    
                    # è§„åˆ™1ï¼šå®Œå…¨åŒ…å«å…³ç³» - ä¿ç•™å¤–å±‚å®ä½“
                    if current_start >= existing_start and current_end <= existing_end:
                        # å½“å‰å®ä½“è¢«å®Œå…¨åŒ…å«ï¼Œè·³è¿‡
                        should_add = False
                        break
                    elif existing_start >= current_start and existing_end <= current_end:
                        # ç°æœ‰å®ä½“è¢«å®Œå…¨åŒ…å«ï¼Œç§»é™¤ç°æœ‰å®ä½“
                        resolved.pop(i)
                        continue
                    
                    # è§„åˆ™2ï¼šåˆçº¦ä»£ç ä¼˜å…ˆçº§æœ€é«˜
                    elif current_label == 'åˆçº¦ä»£ç ' and existing_label != 'åˆçº¦ä»£ç ':
                        resolved.pop(i)
                        continue
                    elif existing_label == 'åˆçº¦ä»£ç ' and current_label != 'åˆçº¦ä»£ç ':
                        should_add = False
                        break
                    
                    # è§„åˆ™3ï¼šæ›´é•¿çš„æ–‡æœ¬ä¼˜å…ˆ
                    elif len(current_text) > len(existing_text):
                        resolved.pop(i)
                        continue
                    else:
                        should_add = False
                        break
            
            if should_add:
                resolved.append(current)
        
        # æŒ‰ä½ç½®æ’åºè¿”å›
        resolved_sorted = sorted(resolved, key=lambda x: x[2])
        logger.debug(f"Entity conflict resolution: {len(entities)} -> {len(resolved_sorted)} entities")
        return resolved_sorted

    def entity_recognition(self, query: str) -> str:
        """
        å¢å¼ºç‰ˆå®ä½“è¯†åˆ« - è¿™æ˜¯æ ¸å¿ƒæ”¹è¿›æ–¹æ³•
        
        è®¾è®¡æ€è·¯ï¼š
        1. ä½¿ç”¨åŸå§‹NLPToolkitè¯†åˆ«åŸºç¡€å®ä½“ï¼ˆäº¤æ˜“æ‰€ã€æœŸè´§å…¬å¸ã€å“ç§ç­‰ï¼‰
        2. ä½¿ç”¨å¢å¼ºæ–¹æ³•è¯†åˆ«åˆçº¦ä»£ç ï¼ˆè§£å†³æ ¸å¿ƒé—®é¢˜ï¼‰
        3. æ™ºèƒ½èåˆä¸¤ç§ç»“æœï¼Œè§£å†³å†²çª
        4. é‡å»ºæ ‡æ³¨æ–‡æœ¬
        
        Args:
            query: è¾“å…¥æŸ¥è¯¢æ–‡æœ¬
            
        Returns:
            å¸¦æœ‰å®Œæ•´å®ä½“æ ‡æ³¨çš„æŸ¥è¯¢æ–‡æœ¬
        """
        try:
            logger.info(f"Starting enhanced entity recognition for: '{query}'")
            all_entities = []
            
            # ========== ç¬¬ä¸€æ­¥ï¼šåŸå§‹NLPToolkitåŸºç¡€è¯†åˆ« ==========
            logger.debug("Step 1: Basic entity recognition using original NLPToolkit")
            
            # ä½¿ç”¨ä¸åŸå§‹dataqa.pyå®Œå…¨ç›¸åŒçš„4å‚æ•°è°ƒç”¨
            # æ³¨æ„ï¼šæˆ‘ä»¬æ˜ç¡®çŸ¥é“è¿™æ— æ³•è¯†åˆ«åˆçº¦ä»£ç ï¼Œä½†ä¿æŒå…¼å®¹æ€§
            tokenizer = NLPToolkit(
                user_dict_path=USER_DICT_PATH, 
                syn_dict_path=SYNONYM_DICT_PATH, 
                stop_words_path=STOP_WORDS_PATH,
                patterns_path=NER_PATTERNs_PATH  # ä¿æŒä¸åŸå§‹dataqa.pyä¸€è‡´
            )
            
            basic_entities = tokenizer.recognize(query)
            
            # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼å¹¶æ‰¾åˆ°ä½ç½®
            for entity_dict in basic_entities:
                if entity_dict.get('id'):
                    entity_text = entity_dict['text']
                    entity_label = entity_dict['label']
                    
                    # åœ¨æŸ¥è¯¢ä¸­æŸ¥æ‰¾å®ä½“çš„ç²¾ç¡®ä½ç½®
                    start_pos = query.find(entity_text)
                    if start_pos != -1:
                        end_pos = start_pos + len(entity_text)
                        all_entities.append((entity_text, entity_label, start_pos, end_pos))
                        logger.debug(f"Basic entity: '{entity_text}' ({entity_label}) at [{start_pos}:{end_pos}]")
            
            basic_entity_count = len(all_entities)
            logger.info(f"NLPToolkit found {basic_entity_count} basic entities")
            
            # ========== ç¬¬äºŒæ­¥ï¼šå¢å¼ºåˆçº¦ä»£ç è¯†åˆ« ==========  
            if self.enhanced_contract_ner:
                logger.debug("Step 2: Enhanced contract code recognition")
                
                # å‡†å¤‡å·²è¯†åˆ«å®ä½“ä¿¡æ¯ï¼ˆç”¨äºé¿å…é‡å ï¼‰
                existing_entity_info = []
                for entity_text, entity_label, start_pos, end_pos in all_entities:
                    existing_entity_info.append({
                        'text': entity_text,
                        'label': entity_label,
                        'start': start_pos,
                        'end': end_pos
                    })
                
                # æ‰§è¡Œåˆçº¦ä»£ç è¯†åˆ«ï¼ˆè¿™æ˜¯æ ¸å¿ƒæ”¹è¿›ï¼‰
                contract_codes = self.enhanced_contract_ner.find_contract_codes(query, existing_entity_info)
                
                # å°†åˆçº¦ä»£ç æ·»åŠ åˆ°å®ä½“åˆ—è¡¨
                for contract in contract_codes:
                    all_entities.append((
                        contract['text'],
                        contract['label'], 
                        contract['start'],
                        contract['end']
                    ))
                    logger.info(f"âœ… CONTRACT CODE FOUND: '{contract['text']}' -> {contract['normalized']}")
                
                contract_count = len(contract_codes)
                logger.info(f"Enhanced NER found {contract_count} additional contract codes")
            else:
                logger.warning("Enhanced Contract Code NER not available")
            
            # ========== ç¬¬ä¸‰æ­¥ï¼šæ™ºèƒ½å†²çªè§£å†³ ==========
            logger.debug("Step 3: Resolving entity conflicts")
            resolved_entities = self._resolve_entity_conflicts(all_entities)
            
            # ========== ç¬¬å››æ­¥ï¼šé‡å»ºæŸ¥è¯¢å­—ç¬¦ä¸² ==========
            logger.debug("Step 4: Rebuilding query with entity annotations")
            result_query = query
            
            # ä»åå¾€å‰æ›¿æ¢ï¼Œé¿å…ä½ç½®åç§»
            for entity_text, entity_label, start_pos, end_pos in reversed(resolved_entities):
                # æ„å»ºæ ‡æ³¨æ ¼å¼ï¼šå®ä½“æ–‡æœ¬(å®ä½“ç±»å‹)
                replacement = f"{entity_text}({entity_label})"
                result_query = result_query[:start_pos] + replacement + result_query[end_pos:]
                logger.debug(f"Annotated: '{entity_text}' -> '{replacement}'")
            
            # ========== è®°å½•æœ€ç»ˆç»“æœ ==========
            final_entity_count = len(resolved_entities)
            contract_entities = [e for e in resolved_entities if e[1] == 'åˆçº¦ä»£ç ']
            
            logger.info(f"ğŸ‰ Enhanced entity recognition completed!")
            logger.info(f"ğŸ“Š Statistics: {basic_entity_count} basic + {len(contract_entities)} contracts = {final_entity_count} total entities")
            logger.info(f"ğŸ“ Input:  '{query}'")
            logger.info(f"ğŸ“ Output: '{result_query}'")
            
            # å¦‚æœæ‰¾åˆ°äº†åˆçº¦ä»£ç ï¼Œè¿™å°±è¯æ˜æˆ‘ä»¬æˆåŠŸè§£å†³äº†å¯¼å¸ˆçš„é—®é¢˜ï¼
            if contract_entities:
                logger.info(f"ğŸš€ SUCCESS: Contract codes identified: {[e[0] for e in contract_entities]}")
            
            return result_query
            
        except Exception as e:
            logger.error(f"Enhanced entity recognition failed: {e}")
            traceback.print_exc()
            
            # ========== é”™è¯¯å›é€€æœºåˆ¶ ==========
            logger.warning("Falling back to original entity recognition method")
            try:
                # å›é€€åˆ°åŸå§‹æ–¹æ³•ï¼ˆä¸åŸå§‹dataqa.pyå®Œå…¨ä¸€è‡´ï¼‰
                tokenizer = NLPToolkit(
                    user_dict_path=USER_DICT_PATH, 
                    syn_dict_path=SYNONYM_DICT_PATH, 
                    stop_words_path=STOP_WORDS_PATH,
                    patterns_path=NER_PATTERNs_PATH
                )
                entity_list = tokenizer.recognize(query)
                result_query = query
                for entity in entity_list:
                    if entity['id'] != '':
                        substring = entity['id'] + '(' + entity['label'] + ')'
                        result_query = result_query.replace(entity['text'], substring)
                
                logger.info(f"Fallback completed: '{query}' -> '{result_query}'")
                return result_query
            except Exception as fallback_error:
                logger.error(f"Fallback also failed: {fallback_error}")
                return query  # æœ€åçš„ä¿é™©ï¼šè¿”å›åŸå§‹æŸ¥è¯¢

    # ========== ä»¥ä¸‹æ–¹æ³•ä¿æŒä¸åŸå§‹dataqa.pyå®Œå…¨ä¸€è‡´ ==========
    
    def modify_query(self, input_messages: List[ChatMessage]) -> OptimizedQuery:
        """é—®é¢˜æ”¹å†™ - ä¿æŒåŸæœ‰å®ç°"""
        try:
            input_messages_mq = copy.deepcopy(input_messages)
            
            optimized_query = self.query_optimizer.generate_optimized_query(
                query=input_messages[-1].content,
                chat_history=input_messages_mq[:-1],
                optimization_type=QueryOptimizationType.DATAQA,
            )
            
            return optimized_query
        except Exception as e:
            logger.error(f"Modify query Error:{e}")
            traceback.print_exc()
            raise e

    def locate_table(self, query: str) -> List[ChatReference]:
        """å®šä½è¡¨æ ¼ - ä¿æŒåŸæœ‰å®ç°"""
        sql_kl = self.sql_knowledge()
        ranked_tables = sql_kl.search(
            self.collection,
            query,
            search_type=SearchType.hybrid,
            knowledge_ids=self.knowledge_id,
            top_k=3,
            use_reranker=True
        )
        
        tables = list(
            map(
                lambda x: {'chunk_uuid': x.chunk_id, 'table_name': x.data.table_name, 'score': x.reranking_score},
                ranked_tables,
            )
        )
        return tables

    def generate_single_table_prompt(self, chunk_id: str):
        """ç”Ÿæˆå•è¡¨æŸ¥è¯¢çš„prompt - ä¿æŒåŸæœ‰å®ç°"""
        sql_kl = self.sql_knowledge()
        table_content = sql_kl.get_by_ids(self.collection, chunk_id)
        table_info = table_content[0].data.table_info
        table_prompt = f"å·²çŸ¥å¦‚ä¸‹æ•°æ®è¡¨ä¿¡æ¯: \n{table_info}\n"
        return table_prompt

    def extract_info(self, text: str, pattern: str):
        """ä¿¡æ¯æå– - ä¿æŒåŸæœ‰å®ç°"""
        extract_pattern = re.compile(pattern, re.DOTALL)
        match = extract_pattern.search(text)
        if match:
            return match.group(1)
        else:
            return None

    def generate_sql_code(self, table_schema: str, input_messages: List[ChatMessage], thinking: Optional[bool] = False):
        """ç”ŸæˆSQLä»£ç  - ä¿æŒåŸæœ‰å®ç°"""
        query = input_messages[-1].content
        content = dataqa_prompt.format(table_schema=table_schema, question=query)
        system_msg = ChatMessage(role="system", content=content)
        
        if thinking is True:
            response = self.ans_thinking_client.invoke(messages=[system_msg] + input_messages[:])
        else:
            response = self.ans_client.invoke(messages=[system_msg] + input_messages[:])
        return response

    def do_generate(self, input_messages: List[ChatMessage], knowledge_base_ids: Optional[List[str]] = None, thinking: Optional[bool] = False):
        """ç”Ÿæˆå›ç­” - ä½¿ç”¨å¢å¼ºçš„å®ä½“è¯†åˆ«ï¼Œå…¶ä»–ä¿æŒåŸæœ‰å®ç°"""
        
        # ä¿ç•™æœ€åä¸­é—´çš„å¯¹è¯ï¼Œä¸­é—´çš„å¯¹è¯æœ€å¤šä¿ç•™ self.history_round * 2 è½®
        if len(input_messages[1:-1]) > self.history_round * 2:
            del input_messages[1 : -1 - self.history_round * 2]

        # step1 modify_query - ä¿æŒä¸å˜
        optimized_input_messages = self.modify_query(input_messages)
        step1 = ChatStep(
            key="modify_query",
            name="æ”¹å†™é—®é¢˜",
            number=1,
            prompt=optimized_input_messages.rewritten_query,
            finished=True,
        )

        # step2 entity_recognition - ä½¿ç”¨å¢å¼ºç‰ˆæœ¬ï¼
        query = optimized_input_messages[-1].content
        enhanced_query = self.entity_recognition(query)  # è¿™é‡Œä½¿ç”¨äº†æˆ‘ä»¬çš„å¢å¼ºæ–¹æ³•ï¼
        optimized_input_messages[-1].content = enhanced_query
        step2 = ChatStep(
            key="enhanced_entity_recognition",
            name="å¢å¼ºå®ä½“è¯†åˆ«",  # æ›´æ–°åç§°ä»¥åæ˜ æ”¹è¿›
            number=2,
            prompt=enhanced_query,
            finished=True,
        )

        # step3 locate_table - ä¿æŒä¸å˜
        located_table = self.locate_table(optimized_input_messages.rewritten_query)
        step3 = ChatStep(
            key="locate_table",
            name="å®šä½è¡¨æ ¼",
            number=3,
            prompt=located_table,
            finished=True,
        )

        # step4 generate_single_table_prompt - ä¿æŒä¸å˜
        single_table_prompt = self.generate_single_table_prompt(located_table[0]['chunk_uuid'])
        step4 = ChatStep(
            key="generate_single_table_prompt",
            name="ç”Ÿæˆå•è¡¨æç¤ºè¯",
            number=4,
            prompt=single_table_prompt,
            finished=True,
        )

        # step5 generate_sql - ä¿æŒä¸å˜
        response = self.generate_sql_code(single_table_prompt, optimized_input_messages, thinking)
        step5 = ChatStep(
            key="generate_sql",
            name="ç”ŸæˆSQL",
            number=5,
            prompt=response,
            finished=True,
        )

        usage = ChatUsage(
            prompt_tokens=response.usage.prompt_tokens,
            completion_tokens=response.usage.completion_tokens,
            total_tokens=response.usage.total_tokens,
        )

        choices = list(
            map(
                lambda x: ChatCompletionChoice(
                    finish_reason=x.finish_reason,
                    index=x.index,
                    message=ChatMessage(
                        role=x.message.role,
                        content=x.message.content,
                        reasoning_content=x.message.reasoning_content,
                    ),
                ),
                response.choices,
            )
        )

        return ChatCompletionResponse(
            id=response.id,
            model=response.model,
            created=response.created,
            choices=choices,
            usage=usage,
            steps=[step1, step2, step3, step4, step5],  # step2ç°åœ¨æ˜¯å¢å¼ºç‰ˆæœ¬
        )


# ========== æµ‹è¯•å’ŒéªŒè¯ä»£ç  ==========
if __name__ == "__main__":
    def test_enhanced_dataqa():
        """æµ‹è¯•å¢å¼ºçš„å®ä½“è¯†åˆ«åŠŸèƒ½"""
        
        print("ğŸ§ª å¢å¼ºDataQAå®ä½“è¯†åˆ«æµ‹è¯•")
        print("=" * 80)
        print("æµ‹è¯•ç›®æ ‡ï¼šéªŒè¯æ˜¯å¦èƒ½æˆåŠŸè¯†åˆ«åˆçº¦ä»£ç ï¼Œè§£å†³å¯¼å¸ˆæå‡ºçš„æ ¸å¿ƒé—®é¢˜")
        print()
        
        # æ ¸å¿ƒæµ‹è¯•ç”¨ä¾‹ï¼šå¯¼å¸ˆæåˆ°çš„åŸå§‹é—®é¢˜
        primary_test_case = {
            "input": "éƒ‘å•†æ‰€AP2502æœŸè´§æ€ä¹ˆæ ·äº†",
            "expected_output": "éƒ‘å·å•†å“äº¤æ˜“æ‰€(äº¤æ˜“æ‰€)AP2502(åˆçº¦ä»£ç )æœŸè´§æ€ä¹ˆæ ·äº†",
            "current_output": "éƒ‘å·å•†å“äº¤æ˜“æ‰€(äº¤æ˜“æ‰€)AP2502æœŸè´§æ€ä¹ˆæ ·äº†",  # åŸå§‹è¾“å‡º
            "description": "å¯¼å¸ˆæå‡ºçš„æ ¸å¿ƒé—®é¢˜ï¼šåˆçº¦ä»£ç AP2502æœªè¢«è¯†åˆ«"
        }
        
        # æ‰©å±•æµ‹è¯•ç”¨ä¾‹
        additional_test_cases = [
            {
                "input": "åæ³°æœŸè´§å¯¹CU2405é“œæœŸè´§çš„åˆ†ææŠ¥å‘Š",
                "description": "æ··åˆå®ä½“è¯†åˆ«ï¼šå…¬å¸+åˆçº¦ä»£ç +å“ç§"
            },
            {
                "input": "ä¸ŠæœŸæ‰€èºçº¹é’¢RB-2501åˆçº¦ä»·æ ¼èµ°åŠ¿",
                "description": "è¿å­—ç¬¦æ ¼å¼çš„åˆçº¦ä»£ç è¯†åˆ«"
            },
            {
                "input": "å¤§å•†æ‰€M 2405è±†ç²•æœŸè´§æŒä»“æ•°æ®",
                "description": "ç©ºæ ¼åˆ†éš”æ ¼å¼çš„åˆçº¦ä»£ç è¯†åˆ«"
            },
            {
                "input": "éƒ‘å•†æ‰€SR2501å’ŒTA2502æœŸè´§ä»·å·®åˆ†æ",
                "description": "å¤šä¸ªåˆçº¦ä»£ç è¯†åˆ«"
            },
            {
                "input": "è‹¹æœå…¬å¸è‚¡ä»·ä¸Šæ¶¨è¶‹åŠ¿åˆ†æ",
                "description": "è¯¯è¯†åˆ«é˜²æŠ¤ï¼šä¸åº”è¯†åˆ«ä¸ºæœŸè´§ç›¸å…³"
            }
        ]
        
        print("ğŸ“‹ ä¸»è¦æµ‹è¯•ç”¨ä¾‹ï¼ˆå¯¼å¸ˆçš„æ ¸å¿ƒé—®é¢˜ï¼‰:")
        print(f"è¾“å…¥: {primary_test_case['input']}")
        print(f"å½“å‰è¾“å‡º: {primary_test_case['current_output']}")
        print(f"æœŸæœ›è¾“å‡º: {primary_test_case['expected_output']}")
        print(f"é—®é¢˜æè¿°: {primary_test_case['description']}")
        print()
        
        print("ğŸ“‹ æ‰©å±•æµ‹è¯•ç”¨ä¾‹:")
        for i, test_case in enumerate(additional_test_cases, 1):
            print(f"{i}. {test_case['input']}")
            print(f"   è¯´æ˜: {test_case['description']}")
        
        print()
        print("âœ… æµ‹è¯•å‡†å¤‡å°±ç»ªï¼")
        print("ğŸš€ è¯·åœ¨å®é™…ç¯å¢ƒä¸­è¿è¡Œä»¥ä¸‹ä»£ç æ¥éªŒè¯æ•ˆæœï¼š")
        print()
        print("# åˆå§‹åŒ–DataQaWorkflow")
        print("workflow = DataQaWorkflow(")
        print("    ans_llm=qwen3_llm,")
        print("    ans_thinking_llm=qwen3_thinking_llm,") 
        print("    query_llm=qwen3_llm")
        print(")")
        print()
        print("# æµ‹è¯•æ ¸å¿ƒé—®é¢˜")
        print("query = 'éƒ‘å•†æ‰€AP2502æœŸè´§æ€ä¹ˆæ ·äº†'")
        print("result = workflow.entity_recognition(query)")
        print("print(f'åŸå§‹: {query}')")
        print("print(f'ç»“æœ: {result}')")
        print("print('âœ… æˆåŠŸ!' if 'AP2502(åˆçº¦ä»£ç )' in result else 'âŒ å¤±è´¥')")

    # è¿è¡Œæµ‹è¯•
    test_enhanced_dataqa()