# %% [markdown]
# ## NERæ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯• - å¢å¼ºç‰ˆï¼šä¸“ä¸šè¯­æ–™å’Œé«˜çº§æŒ‡æ ‡åˆ†æ
# 
# ## ğŸ“‹ æ–°å¢åŠŸèƒ½
# # 1. ä¸“ä¸šæŒ‘æˆ˜æ€§æµ‹è¯•è¯­æ–™ï¼ˆæœŸè´§åˆçº¦ä»£ç ç­‰ï¼‰
# # 2. ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°ç­‰è¯¦ç»†æŒ‡æ ‡
# # 3. å¤±è´¥æ¨¡å¼é‡åŒ–åˆ†æ
# # 4. å®ä½“çº§åˆ«å’Œç±»å‹çº§åˆ«çš„è¯¦ç»†æ€§èƒ½åˆ†æ

# %%
import spacy
import time
import json
from typing import Dict, List, Tuple, Set, Optional
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from IPython.display import display, HTML, Markdown
import warnings
import re
from collections import defaultdict, Counter
from sklearn.metrics import precision_recall_fscore_support, classification_report
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# è®¾ç½®å›¾è¡¨æ ·å¼
sns.set_style("whitegrid")
plt.style.use('seaborn-v0_8')

print("ğŸš€ å¢å¼ºç‰ˆ NERæ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•ç³»ç»Ÿ")
print("=" * 80)
print("âœ… ç¯å¢ƒè®¾ç½®å®Œæˆ")
print(f"ğŸ“¦ spaCyç‰ˆæœ¬: {spacy.__version__}")
print(f"ğŸ“ Pandasç‰ˆæœ¬: {pd.__version__}")
print(f"ğŸ“Š NumPyç‰ˆæœ¬: {np.__version__}")

# %% [markdown]
# ## 2. å¢å¼ºæµ‹è¯•è¯­æ–™åº“åˆ›å»º - æ·»åŠ ä¸“ä¸šæŒ‘æˆ˜æ€§æµ‹è¯•

# %%
def create_enhanced_professional_test_corpus():
    """åˆ›å»ºå¢å¼ºçš„ä¸“ä¸šæœŸè´§äº¤æ˜“é¢†åŸŸæµ‹è¯•è¯­æ–™åº“ï¼ŒåŒ…å«æ›´å…·æŒ‘æˆ˜æ€§çš„æ¡ˆä¾‹"""
    
    # åŸæœ‰åŸºç¡€æµ‹è¯•ä¿æŒä¸å˜
    basic_entity_tests = [
        "è‹¹æœæœŸè´§åœ¨éƒ‘å·å•†å“äº¤æ˜“æ‰€äº¤æ˜“ï¼Œåæ³°æœŸè´§å…¬å¸å‚ä¸å…¶ä¸­",
        "ä¸Šæµ·æœŸè´§äº¤æ˜“æ‰€çš„é“œæœŸè´§ä»·æ ¼ä¸Šæ¶¨ï¼Œä¸­ä¿¡æœŸè´§å‘å¸ƒç ”ç©¶æŠ¥å‘Š", 
        "å¤§è¿å•†å“äº¤æ˜“æ‰€æ¨å‡ºæ–°çš„è±†ç²•æœŸè´§åˆçº¦ï¼Œæ°¸å®‰æœŸè´§ç§¯æå‚ä¸äº¤æ˜“",
        "éƒ‘å•†æ‰€ç™½ç³–æœŸè´§ä¸»åŠ›åˆçº¦æ”¶ç›˜ä»·æ ¼ä¸º5200å…ƒ/å¨",
        "ä¸­å›½é‡‘èæœŸè´§äº¤æ˜“æ‰€è‚¡æŒ‡æœŸè´§IF2024åˆçº¦æ³¢åŠ¨åŠ å‰§",
        "æ°¸å®‰æœŸè´§ã€æµ·é€šæœŸè´§ã€ç”³é“¶ä¸‡å›½æœŸè´§ä¸‰å®¶å…¬å¸æŒä»“æ’åå‰ä¸‰",
        "å›½æ³°å›å®‰æœŸè´§ç ”ç©¶æ‰€å‘å¸ƒèºçº¹é’¢æœŸè´§æŠ•èµ„ç­–ç•¥æŠ¥å‘Š",
        "æ–¹æ­£ä¸­æœŸæœŸè´§åœ¨æ£‰èŠ±æœŸè´§äº¤æ˜“ä¸­è¡¨ç°æ´»è·ƒ",
        "å…‰å¤§æœŸè´§ä¸é“¶æ²³æœŸè´§åœ¨åŸæ²¹æœŸè´§å¸‚åœºä»½é¢é¢†å…ˆ",
        "æ‹›å•†æœŸè´§ã€å¹¿å‘æœŸè´§ã€ä¸œè¯æœŸè´§è”åˆå‘å¸ƒå¸‚åœºåˆ†æ"
    ]
    
    # â­ æ–°å¢ï¼šä¸“ä¸šæŒ‘æˆ˜æ€§æµ‹è¯•è¯­æ–™ï¼ˆåŒ…å«åˆçº¦ä»£ç å’Œå¤æ‚é‡‘èæœ¯è¯­ï¼‰
    professional_challenging_tests = [
        # åˆçº¦ä»£ç è¯†åˆ«æŒ‘æˆ˜
        "éƒ‘å•†æ‰€AP2502æœŸè´§æ€ä¹ˆæ ·äº†",
        "ä¸ŠæœŸæ‰€CU2503åˆçº¦ä»Šæ—¥æ¶¨åœï¼Œåæ³°æœŸè´§å»ºè®®å…³æ³¨",
        "å¤§å•†æ‰€M2505è±†ç²•æœŸè´§ä¸»åŠ›åˆçº¦æˆäº¤æ´»è·ƒ",
        "ä¸­é‡‘æ‰€IF2503æ²ªæ·±300è‚¡æŒ‡æœŸè´§åŸºå·®æ”¶çª„",
        "INEåŸæ²¹SC2504åˆçº¦ä¸å¸ƒä¼¦ç‰¹åŸæ²¹ä»·å·®æ‰©å¤§",
        
        # å¤æ‚æœºæ„åç§°å’Œäº§å“ç»„åˆ
        "æ–¹æ­£ä¸­æœŸæœŸè´§é£é™©ç®¡ç†å­å…¬å¸åœ¨PTA2504åˆçº¦ä¸Šå»ºç«‹ç©ºå¤´å¥—ä¿å¤´å¯¸",
        "å›½æŠ•å®‰ä¿¡æœŸè´§èµ„äº§ç®¡ç†éƒ¨é—¨çœ‹å¥½RB2503èºçº¹é’¢æœŸè´§åå¸‚è¡¨ç°",
        "åæ³°æœŸè´§ç ”ç©¶æ‰€åˆ†æå¸ˆè®¤ä¸ºZC2505åŠ¨åŠ›ç…¤æœŸè´§ä»·æ ¼å°†éœ‡è¡ä¸Šè¡Œ",
        "æ°¸å®‰èµ„æœ¬ç®¡ç†æœ‰é™å…¬å¸åœ¨CF2504æ£‰èŠ±æœŸè´§ä¸Šå¢æŒå¤šå¤´å¤´å¯¸",
        "ä¸­ä¿¡æœŸè´§è¡ç”Ÿå“äº‹ä¸šéƒ¨æ¨å‡ºåŸºäºSR2505ç™½ç³–æœŸè´§çš„ç»“æ„åŒ–äº§å“",
        
        # æŠ€æœ¯åˆ†ææœ¯è¯­ + åˆçº¦ä»£ç 
        "TA2504PTAæœŸè´§çªç ´å‰æœŸé«˜ç‚¹ï¼ŒMACDæŒ‡æ ‡æ˜¾ç¤ºå¤šå¤´ä¿¡å·",
        "AG2503ç™½é“¶æœŸè´§å½¢æˆhead and shouldersé¡¶éƒ¨å½¢æ€ï¼Œå»ºè®®å‡ä»“",
        "NI2504é•æœŸè´§price actionå‘ˆç°ä¸‰è§’å½¢æ•´ç†ï¼Œç­‰å¾…æ–¹å‘é€‰æ‹©",
        "FG2505ç»ç’ƒæœŸè´§implied volatilityé£™å‡ï¼ŒæœŸæƒskewåŠ å‰§",
        "I2504é“çŸ¿çŸ³æœŸè´§basis point valueè®¡ç®—ä¸­éœ€è€ƒè™‘duration risk",
        
        # æ—¶é—´æ•æ„Ÿå’Œæ•°å€¼ç²¾ç¡®æ€§æµ‹è¯•
        "2024å¹´12æœˆ17æ—¥ï¼ŒRB2505èºçº¹é’¢æœŸè´§æ”¶ç›˜ä»·4120å…ƒ/å¨ï¼Œè¾ƒå‰æ—¥ä¸Šæ¶¨2.3%",
        "æˆªè‡³2024å¹´12æœˆæ”¶ç›˜ï¼Œåæ³°æœŸè´§åœ¨AP2502è‹¹æœæœŸè´§ä¸ŠæŒä»“é‡è¾¾åˆ°15000æ‰‹",
        "éƒ‘å•†æ‰€CF2505æ£‰èŠ±æœŸè´§æœ€åäº¤æ˜“æ—¥ä¸º2025å¹´5æœˆ15æ—¥ï¼Œäº¤å‰²æœˆä¸´è¿‘",
        "ä¸ŠæœŸæ‰€AU2506é»„é‡‘æœŸè´§å¤œç›˜21:00-02:30æˆäº¤é‡æ”¾å¤§è‡³8.7ä¸‡æ‰‹",
        "ä¸­é‡‘æ‰€IC2503ä¸­è¯500æœŸè´§ä¿è¯é‡‘æ¯”ä¾‹è°ƒæ•´ä¸º12%ï¼Œæ¶¨è·Œåœæ¿å¹…åº¦6%",
        
        # æ··åˆè¯­è¨€å’Œç¼©å†™æŒ‘æˆ˜
        "shfe RB2503èºçº¹é’¢æœŸè´§volume weighted average priceçªç ´å…³é”®é˜»åŠ›ä½",
        "czce TA2504 PTAæœŸè´§open intereståˆ›å†å²æ–°é«˜ï¼Œmarket depthæ˜¾è‘—æ”¹å–„",
        "dce M2505è±†ç²•æœŸè´§ä¸CBOTå¤§è±†æœŸè´§ä»·å·®ï¼ˆbasisï¼‰æŒç»­æ‰©å¤§",
        "CFFEX IF2503æŒ‡æ•°æœŸè´§roll yieldä¸ºè´Ÿï¼Œcontangoç»“æ„æ˜æ˜¾",
        "INE SC2504åŸæ²¹æœŸè´§ä¸WTI crude oilè”åŠ¨æ€§å¢å¼ºï¼Œcorrelation coefficientè¾¾0.85",
        
        # å¤æ‚è´¢åŠ¡å’Œé£é™©ç®¡ç†æœ¯è¯­
        "åæ³°æœŸè´§é£é™©å­å…¬å¸é€šè¿‡CF2505æ£‰èŠ±æœŸè´§è¿›è¡Œbasis tradingï¼Œhedge ratioè®¾å®šä¸º0.8",
        "æ°¸å®‰æœŸè´§èµ„ç®¡éƒ¨é—¨æ„å»ºæ¶µç›–RB2503ã€HC2503ã€I2504çš„é»‘è‰²ç³»æœŸè´§ç»„åˆï¼ŒVaRæ§åˆ¶åœ¨2%ä»¥å†…",
        "ä¸­ä¿¡æœŸè´§é‡åŒ–å›¢é˜ŸåŸºäºCU2504é“œæœŸè´§çš„momentumç­–ç•¥ï¼Œå¹´åŒ–Sharpe ratioè¾¾1.8",
        "å›½æ³°å›å®‰æœŸè´§ç»“æ„åŒ–äº§å“éƒ¨é—¨è®¾è®¡ä¸AU2506é»„é‡‘æœŸè´§æŒ‚é’©çš„capital protected note",
        "å…‰å¤§æœŸè´§æœºæ„ä¸šåŠ¡éƒ¨ä¸ºé’¢é“ä¼ä¸šå®šåˆ¶RB2504èºçº¹é’¢æœŸè´§å¥—æœŸä¿å€¼æ–¹æ¡ˆï¼Œhedge effectivenessè¾¾95%",
        
        # ç›‘ç®¡å’Œåˆè§„æœ¯è¯­
        "è¯ç›‘ä¼šè¦æ±‚æœŸè´§å…¬å¸åŠ å¼ºå¯¹å®¢æˆ·AP2502è‹¹æœæœŸè´§äº¤æ˜“çš„é€‚å½“æ€§ç®¡ç†",
        "ä¸­æœŸåå‘å¸ƒå…³äºTA2505 PTAæœŸè´§é£é™©æ§åˆ¶çš„è‡ªå¾‹è§„åˆ™è¡¥å……æ¡æ¬¾",
        "éƒ‘å•†æ‰€å¯¹CF2504æ£‰èŠ±æœŸè´§å¼‚å¸¸äº¤æ˜“è¡Œä¸ºå®æ–½ç›‘ç®¡æªæ–½ï¼Œæ¶‰åŠ5å®¶æœŸè´§å…¬å¸",
        "æœŸè´§ä¿è¯é‡‘ç›‘æ§ä¸­å¿ƒåŠ å¼ºå¯¹ZC2505åŠ¨åŠ›ç…¤æœŸè´§å¤§æˆ·æŒä»“çš„å®æ—¶ç›‘æ§",
        "ä¸ŠæœŸæ‰€å‘å¸ƒSC2504åŸæ²¹æœŸè´§äº¤å‰²åº“å­˜å‘¨æŠ¥ï¼Œç°è´§åº“å­˜ç¯æ¯”ä¸‹é™3.2%",
        
        # è·¨å¢ƒå’Œå›½é™…åŒ–æœ¯è¯­
        "ä¸­å›½æœŸè´§ä¸šåä¼šä¸èŠå•†æ‰€å°±IF2503è‚¡æŒ‡æœŸè´§è·¨å¢ƒç›‘ç®¡è¾¾æˆåˆä½œåè®®",
        "ä¸Šæµ·å›½é™…èƒ½æºäº¤æ˜“ä¸­å¿ƒSC2504åŸæ²¹æœŸè´§ä¸è¿ªæ‹œå•†å“äº¤æ˜“æ‰€é˜¿æ›¼åŸæ²¹æœŸè´§ä»·å·®åˆ†æ",
        "å¤§å•†æ‰€é“çŸ¿çŸ³æœŸè´§I2504å›½é™…åŒ–è¿›ç¨‹åŠ é€Ÿï¼Œå¢ƒå¤–å‚ä¸è€…æ•°é‡å¢é•¿æ˜¾è‘—",
        "éƒ‘å•†æ‰€PTAæœŸè´§TA2505å¼•å…¥å¢ƒå¤–æŠ•èµ„è€…ï¼Œå¤–èµ„æŒä»“å æ¯”è¾¾8.3%",
        "ä¸­é‡‘æ‰€æ­£åœ¨ç ”ç©¶æ¨å‡ºåŸºäºMSCIä¸­å›½æŒ‡æ•°çš„æœŸè´§åˆçº¦ï¼Œé¢„è®¡2025å¹´ä¸Šå¸‚",
        
        # æå…·æŒ‘æˆ˜æ€§çš„å¤åˆæµ‹è¯•
        "åæ³°æœŸè´§ç ”ç©¶æ‰€é¦–å¸­åˆ†æå¸ˆç‹æ˜åœ¨å…¶æœ€æ–°å‘å¸ƒçš„ã€Š2025å¹´é»‘è‰²ç³»æœŸè´§æŠ•èµ„ç­–ç•¥æŠ¥å‘Šã€‹ä¸­æŒ‡å‡ºï¼ŒåŸºäºDCFä¼°å€¼æ¨¡å‹å’Œè’™ç‰¹å¡æ´›æ¨¡æ‹Ÿï¼ŒRB2503èºçº¹é’¢æœŸè´§fair valueåŒºé—´ä¸º4000-4200å…ƒ/å¨ï¼Œå½“å‰ä»·æ ¼4120å…ƒ/å¨å¤„äºåˆç†ä¼°å€¼åŒºé—´ä¸Šæ²¿ï¼Œå»ºè®®investoré‡‡ç”¨covered callç­–ç•¥è·å–alphaæ”¶ç›Šï¼ŒåŒæ—¶é€šè¿‡dynamic hedgingç®¡ç†portfolioçš„duration riskå’Œconvexity riskã€‚",
        
        "æ°¸å®‰èµ„æœ¬ç®¡ç†æœ‰é™å…¬å¸é‡åŒ–æŠ•èµ„éƒ¨é—¨åŸºäºmachine learningç®—æ³•æ„å»ºçš„multi-factor modelæ˜¾ç¤ºï¼ŒCF2505æ£‰èŠ±æœŸè´§ä»·æ ¼ä¸ç¾æ£‰æœŸè´§ã€äººæ°‘å¸æ±‡ç‡ã€åŸæ²¹ä»·æ ¼çš„beta coefficientsåˆ†åˆ«ä¸º0.75ã€-0.32ã€0.28ï¼Œæ¨¡å‹çš„adjusted R-squaredè¾¾åˆ°0.82ï¼Œout-of-sample testingçš„information ratioä¸º1.65ï¼Œå»ºè®®é€šè¿‡pair tradingç­–ç•¥å¯¹å†²systematic riskã€‚"
    ]
    
    # å¤æ‚å¥æ³•ç»“æ„æµ‹è¯•ï¼ˆä¿æŒåŸæœ‰å¹¶å¢å¼ºï¼‰
    enhanced_complex_syntax_tests = [
        "æ®åæ³°æœŸè´§ç ”ç©¶æ‰€æœ€æ–°å‘å¸ƒçš„æŠ¥å‘Šæ˜¾ç¤ºï¼Œå—å›½é™…åŸæ²¹ä»·æ ¼æ³¢åŠ¨å½±å“ï¼ŒshfeåŸæ²¹æœŸè´§ä¸»åŠ›åˆçº¦sc2024åœ¨æ˜¨æ—¥æ”¶ç›˜æ—¶ä¸Šæ¶¨3.2%",
        "éƒ‘å·å•†å“äº¤æ˜“æ‰€ç™½ç³–æœŸè´§SR2024åˆçº¦åœ¨ç»å†äº†è¿ç»­ä¸‰ä¸ªäº¤æ˜“æ—¥çš„ä¸‹è·Œåï¼Œä»Šæ—¥å¼€ç›˜ä»·æ ¼ä¸º5180å…ƒ/å¨ï¼Œè¾ƒå‰ä¸€äº¤æ˜“æ—¥æ”¶ç›˜ä»·ä¸Šæ¶¨0.8%",
        "ä¸­ä¿¡æœŸè´§åˆ†æå¸ˆè®¤ä¸ºï¼Œåœ¨å½“å‰å®è§‚ç»æµç¯å¢ƒä¸‹ï¼Œå¤§è¿å•†å“äº¤æ˜“æ‰€è±†ç²•æœŸè´§m2024åˆçº¦ä»·æ ¼å°†åœ¨3200-3400å…ƒ/å¨åŒºé—´éœ‡è¡è¿è¡Œ",
        # â­ æ–°å¢å¤æ‚è¯­æ³•æµ‹è¯•
        "æ°¸å®‰æœŸè´§å…¬å¸æ——ä¸‹æ°¸å®‰èµ„æœ¬ç®¡ç†æœ‰é™å…¬å¸é£é™©ç®¡ç†å­å…¬å¸åœ¨ä¸ºæŸå¤§å‹é’¢é“ä¼ä¸šæä¾›åŸºäºRB2505èºçº¹é’¢æœŸè´§çš„å¥—æœŸä¿å€¼æœåŠ¡æ—¶ï¼Œé‡‡ç”¨äº†åŠ¨æ€å¯¹å†²ç­–ç•¥ï¼Œhedge ratioæ ¹æ®realized volatilityå’Œimplied volatilityçš„å·®å¼‚è¿›è¡Œå®æ—¶è°ƒæ•´ï¼Œæœ‰æ•ˆcontroläº†ä¼ä¸šraw materialä»·æ ¼é£é™©exposureã€‚",
        "å›½æ³°å›å®‰æœŸè´§è¡ç”Ÿå“ç ”ç©¶å›¢é˜Ÿé€šè¿‡å¯¹CF2504æ£‰èŠ±æœŸè´§å†å²ä»·æ ¼è¿›è¡Œeconometric analysiså‘ç°ï¼Œè¯¥åˆçº¦ä¸ICEæ£‰èŠ±æœŸè´§çš„é•¿æœŸcointegration relationshipæ˜¾è‘—ï¼Œerror correction modelçš„adjustment coefficientä¸º-0.15ï¼Œæ„å‘³ç€price deviationä¼šåœ¨6.7ä¸ªäº¤æ˜“æ—¥å†…correction 50%ã€‚"
    ]
    
    # è¾¹ç•Œæ­§ä¹‰æµ‹è¯•ï¼ˆå¢å¼ºç‰ˆï¼‰
    enhanced_boundary_ambiguity_tests = [
        "è‹¹æœå…¬å¸è‚¡ä»·ä¸Šæ¶¨ï¼Œä½†è‹¹æœæœŸè´§ä»·æ ¼ä¸‹è·Œ",
        "ä¸­å›½é“¶è¡Œå‘å¸ƒæŠ¥å‘Šï¼Œä¸­å›½é“¶è¡ŒæœŸè´§å­å…¬å¸ä¸šåŠ¡å¢é•¿",
        # â­ æ–°å¢è¾¹ç•Œæ­§ä¹‰æµ‹è¯•
        "åæ³°è¯åˆ¸åæ³°æœŸè´§æ¯å­å…¬å¸åœ¨AP2502è‹¹æœæœŸè´§ä¸šåŠ¡åˆä½œä¸­å‘æŒ¥ååŒæ•ˆåº”",
        "æ°¸å®‰æœŸè´§æ°¸å®‰èµ„æœ¬æ°¸å®‰èµ„ç®¡ä¸‰ä¸ªä¸»ä½“åœ¨RB2503èºçº¹é’¢æœŸè´§å¥—ä¿ä¸šåŠ¡ä¸­åˆ†å·¥æ˜ç¡®",
        "ä¸­ä¿¡æœŸè´§ä¸­ä¿¡è¯åˆ¸ä¸­ä¿¡å»ºæŠ•ä¸‰å®¶æœºæ„è”åˆç ”ç©¶CF2505æ£‰èŠ±æœŸè´§æŠ•èµ„ä»·å€¼",
        "å¤§å•†æ‰€å¤§è¿å•†å“äº¤æ˜“æ‰€å¤§è¿æœŸè´§ä¸‰ä¸ªä¸åŒæ¦‚å¿µéœ€è¦å‡†ç¡®åŒºåˆ†",
        "éƒ‘å•†æ‰€AP2502è‹¹æœæœŸè´§è‹¹æœç°è´§è‹¹æœå…¬å¸ä¸‰ä¸ªä¸åŒå®ä½“çš„market correlation analysis"
    ]
    
    # â­ æ–°å¢ï¼šRegExå¤±è´¥æ¡ˆä¾‹ä¸“é¡¹æµ‹è¯•
    regex_failure_cases = [
        # åˆçº¦ä»£ç ä¸æ–‡å­—æ··åˆ
        "éƒ‘å•†æ‰€AP2502æœŸè´§æ€ä¹ˆæ ·äº†",
        "RB2503èºçº¹é’¢æœŸè´§ä»Šå¤©è¡¨ç°å¦‚ä½•",
        "CF2505æ£‰èŠ±æœŸè´§ä»·æ ¼èµ°åŠ¿åˆ†æ",
        "TA2504PTAæœŸè´§åŸºæœ¬é¢ç ”ç©¶",
        "SC2504åŸæ²¹æœŸè´§æŠ€æœ¯åˆ†ææŠ¥å‘Š",
        
        # å¤æ‚åµŒå¥—ç»“æ„
        "åæ³°æœŸè´§åœ¨éƒ‘å•†æ‰€AP2502æœŸè´§åˆçº¦ä¸Šçš„æŒä»“æƒ…å†µ",
        "æ°¸å®‰æœŸè´§å¯¹ä¸ŠæœŸæ‰€CU2503é“œæœŸè´§çš„æœ€æ–°è§‚ç‚¹",
        "ä¸­ä¿¡æœŸè´§å…³äºå¤§å•†æ‰€M2505è±†ç²•æœŸè´§çš„æŠ•èµ„å»ºè®®",
        "å›½æ³°å›å®‰æœŸè´§é’ˆå¯¹ä¸­é‡‘æ‰€IF2503è‚¡æŒ‡æœŸè´§çš„ç­–ç•¥è°ƒæ•´",
        "å…‰å¤§æœŸè´§åœ¨INE SC2504åŸæ²¹æœŸè´§ä¸Šçš„é£é™©ç®¡ç†æªæ–½",
        
        # å¤šå®ä½“æ··åˆ
        "åæ³°æœŸè´§ã€ä¸­ä¿¡æœŸè´§ã€æ°¸å®‰æœŸè´§ä¸‰å®¶å…¬å¸å¯¹RB2503èºçº¹é’¢æœŸè´§çš„è§‚ç‚¹åˆ†æ­§",
        "éƒ‘å•†æ‰€ã€å¤§å•†æ‰€ã€ä¸ŠæœŸæ‰€ä¸‰å¤§äº¤æ˜“æ‰€çš„AP2502ã€M2505ã€CU2503ä¸»åŠ›åˆçº¦åˆ†æ",
        "æœŸè´§å…¬å¸ã€äº¤æ˜“æ‰€ã€ç›‘ç®¡æœºæ„å¯¹TA2504PTAæœŸè´§å¸‚åœºçš„ä¸åŒæ€åº¦"
    ]
    
    # æ•´åˆæ‰€æœ‰æµ‹è¯•ç±»å‹
    all_enhanced_tests = {
        "åŸºç¡€å®ä½“è¯†åˆ«": basic_entity_tests,
        "ä¸“ä¸šæŒ‘æˆ˜æ€§æµ‹è¯•": professional_challenging_tests,
        "å¢å¼ºå¤æ‚å¥æ³•": enhanced_complex_syntax_tests,
        "å¢å¼ºè¾¹ç•Œæ­§ä¹‰": enhanced_boundary_ambiguity_tests,
        "RegExå¤±è´¥æ¡ˆä¾‹": regex_failure_cases
    }
    
    # å±•å¹³æ‰€æœ‰æµ‹è¯•æ–‡æœ¬
    flat_tests = []
    for category, texts in all_enhanced_tests.items():
        flat_tests.extend(texts)
    
    return flat_tests, all_enhanced_tests

# â­ æ–°å¢ï¼šæ ‡ç­¾æ˜ å°„å‡½æ•°
def create_label_mapping():
    """åˆ›å»ºspaCyæ ‡ç­¾åˆ°æœŸè´§é¢†åŸŸæ ‡ç­¾çš„æ˜ å°„"""
    return {
        # ç»„ç»‡æœºæ„æ˜ å°„
        "ORG": "FUTURES_COMPANY",  # é»˜è®¤ç»„ç»‡æ˜ å°„ä¸ºæœŸè´§å…¬å¸
        "ORGANIZATION": "FUTURES_COMPANY",
        
        # åœ°ç†æ”¿æ²»å®ä½“å¯èƒ½æ˜¯äº¤æ˜“æ‰€
        "GPE": "EXCHANGE",
        
        # è®¾æ–½ä¹Ÿå¯èƒ½æ˜¯äº¤æ˜“æ‰€
        "FAC": "EXCHANGE",
        
        # ä¿æŒä¸å˜çš„æ˜ å°„
        "EXCHANGE": "EXCHANGE",
        "FUTURES_COMPANY": "FUTURES_COMPANY", 
        "PRODUCT": "PRODUCT",
        "CONTRACT_CODE": "CONTRACT_CODE"
    }

# â­ æ–°å¢ï¼šæ™ºèƒ½å®ä½“åå¤„ç†å‡½æ•°
def postprocess_entities_with_domain_knowledge(entities, text):
    """ä½¿ç”¨é¢†åŸŸçŸ¥è¯†å¯¹å®ä½“è¿›è¡Œåå¤„ç†å’Œé‡æ–°æ ‡è®°"""
    
    processed_entities = []
    label_mapping = create_label_mapping()
    
    for start, end, label in entities:
        entity_text = text[start:end]
        new_label = label
        
        # â­ åŸºäºæ–‡æœ¬å†…å®¹çš„æ™ºèƒ½é‡æ–°æ ‡è®°
        if label in ["ORG", "ORGANIZATION"]:
            # æœŸè´§å…¬å¸è¯†åˆ«
            if any(keyword in entity_text for keyword in ["æœŸè´§", "èµ„æœ¬", "èµ„ç®¡"]):
                new_label = "FUTURES_COMPANY"
            # äº¤æ˜“æ‰€è¯†åˆ«  
            elif any(keyword in entity_text for keyword in ["äº¤æ˜“æ‰€", "å•†æ‰€", "æ‰€"]) and "æœŸè´§" not in entity_text:
                new_label = "EXCHANGE"
            else:
                new_label = "FUTURES_COMPANY"  # é»˜è®¤ä¸ºæœŸè´§å…¬å¸
                
        elif label in ["GPE", "FAC"]:
            # äº¤æ˜“æ‰€è¯†åˆ«
            if any(keyword in entity_text for keyword in ["äº¤æ˜“æ‰€", "å•†æ‰€", "æ‰€", "éƒ‘å•†æ‰€", "å¤§å•†æ‰€", "ä¸ŠæœŸæ‰€", "ä¸­é‡‘æ‰€"]):
                new_label = "EXCHANGE"
            else:
                new_label = label_mapping.get(label, label)
                
        # â­ åˆçº¦ä»£ç è¯†åˆ« (æ­£åˆ™è¡¨è¾¾å¼)
        import re
        if re.match(r'^[A-Z]{1,3}\d{4}$', entity_text):
            new_label = "CONTRACT_CODE"

# â­ æ–°å¢ï¼šæ€§èƒ½æŒ‡æ ‡è®¡ç®—ç±»
class DetailedPerformanceMetrics:
    """è¯¦ç»†æ€§èƒ½æŒ‡æ ‡è®¡ç®—ç±»"""
    
    def __init__(self):
        self.true_entities = []
        self.pred_entities = []
        self.entity_type_performance = defaultdict(lambda: {"tp": 0, "fp": 0, "fn": 0})
        self.failure_cases = []
        
    def add_example(self, text, true_entities, pred_entities):
        """æ·»åŠ ä¸€ä¸ªæµ‹è¯•æ ·ä¾‹çš„ç»“æœ"""
        self.true_entities.extend([(text, ent) for ent in true_entities])
        self.pred_entities.extend([(text, ent) for ent in pred_entities])
        
        # è®¡ç®—å®ä½“çº§åˆ«çš„TP, FP, FN
        true_set = set(true_entities)
        pred_set = set(pred_entities)
        
        # æŒ‰ç±»å‹ç»Ÿè®¡
        for ent in true_set:
            if len(ent) >= 3:
                entity_type = ent[2]
                if ent in pred_set:
                    self.entity_type_performance[entity_type]["tp"] += 1
                else:
                    self.entity_type_performance[entity_type]["fn"] += 1
                    self.failure_cases.append({
                        "text": text,
                        "type": "FN",
                        "entity": ent,
                        "entity_type": entity_type
                    })
        
        for ent in pred_set:
            if len(ent) >= 3:
                entity_type = ent[2]
                if ent not in true_set:
                    self.entity_type_performance[entity_type]["fp"] += 1
                    self.failure_cases.append({
                        "text": text,
                        "type": "FP", 
                        "entity": ent,
                        "entity_type": entity_type
                    })
    
    def calculate_metrics(self):
        """è®¡ç®—è¯¦ç»†çš„æ€§èƒ½æŒ‡æ ‡"""
        results = {}
        
        for entity_type, counts in self.entity_type_performance.items():
            tp = counts["tp"]
            fp = counts["fp"] 
            fn = counts["fn"]
            
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
            
            results[entity_type] = {
                "precision": precision,
                "recall": recall,
                "f1": f1,
                "tp": tp,
                "fp": fp,
                "fn": fn,
                "support": tp + fn
            }
        
        # è®¡ç®—å®å¹³å‡å’Œå¾®å¹³å‡
        total_tp = sum(counts["tp"] for counts in self.entity_type_performance.values())
        total_fp = sum(counts["fp"] for counts in self.entity_type_performance.values())
        total_fn = sum(counts["fn"] for counts in self.entity_type_performance.values())
        
        micro_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0
        micro_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0
        micro_f1 = 2 * micro_precision * micro_recall / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0
        
        precisions = [r["precision"] for r in results.values() if r["support"] > 0]
        recalls = [r["recall"] for r in results.values() if r["support"] > 0]
        f1s = [r["f1"] for r in results.values() if r["support"] > 0]
        
        macro_precision = np.mean(precisions) if precisions else 0
        macro_recall = np.mean(recalls) if recalls else 0
        macro_f1 = np.mean(f1s) if f1s else 0
        
        results["micro_avg"] = {
            "precision": micro_precision,
            "recall": micro_recall,
            "f1": micro_f1,
            "support": total_tp + total_fn
        }
        
        results["macro_avg"] = {
            "precision": macro_precision,
            "recall": macro_recall,
            "f1": macro_f1,
            "support": total_tp + total_fn
        }
        
        return results
    
    def analyze_failure_modes(self):
        """åˆ†æå¤±è´¥æ¨¡å¼"""
        failure_analysis = {
            "by_type": defaultdict(lambda: {"FP": 0, "FN": 0}),
            "by_text_length": defaultdict(lambda: {"FP": 0, "FN": 0}),
            "common_errors": [],
            "detailed_cases": self.failure_cases
        }
        
        for case in self.failure_cases:
            entity_type = case["entity_type"]
            error_type = case["type"]
            text_length = len(case["text"])
            
            failure_analysis["by_type"][entity_type][error_type] += 1
            
            if text_length < 20:
                length_category = "short"
            elif text_length < 50:
                length_category = "medium"
            else:
                length_category = "long"
            
            failure_analysis["by_text_length"][length_category][error_type] += 1
        
        return failure_analysis

# åˆ›å»ºå¢å¼ºæµ‹è¯•è¯­æ–™
print("ğŸ”„ åˆ›å»ºå¢å¼ºæµ‹è¯•è¯­æ–™åº“...")
enhanced_test_texts, enhanced_categorized_tests = create_enhanced_professional_test_corpus()
gold_standard = create_gold_standard_annotations()

print(f"ğŸ“Š å¢å¼ºæµ‹è¯•è¯­æ–™åº“ç»Ÿè®¡")
print("=" * 60)
print(f"æ€»æ–‡æœ¬æ•°é‡: {len(enhanced_test_texts)}")
print(f"æµ‹è¯•ç±»åˆ«: {len(enhanced_categorized_tests)}")
print(f"é»„é‡‘æ ‡å‡†æ ·æœ¬: {len(gold_standard)}")
print("\nå„ç±»åˆ«æ–‡æœ¬æ•°é‡:")
for category, texts in enhanced_categorized_tests.items():
    print(f"  {category}: {len(texts)} æ¡")

print(f"\nğŸ“ æ–°å¢ä¸“ä¸šæŒ‘æˆ˜æ€§æµ‹è¯•ç¤ºä¾‹:")
print("-" * 40)
for i, text in enumerate(enhanced_categorized_tests["ä¸“ä¸šæŒ‘æˆ˜æ€§æµ‹è¯•"][:3]):
    print(f"{i+1}. {text}")

# %% [markdown]
# ## 3. å¢å¼ºåŸºå‡†æµ‹è¯•æ‰§è¡Œ

# %%
def enhanced_benchmark_with_detailed_metrics():
    """å¢å¼ºçš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«è¯¦ç»†æ€§èƒ½æŒ‡æ ‡"""
    
    configurations = {
        "sm_full": {
            "model": "zh_core_web_sm",
            "exclude": [],
            "description": "å°å‹æ¨¡å‹å®Œæ•´é…ç½® (46MB)"
        },
        "sm_ner_only": {
            "model": "zh_core_web_sm", 
            "exclude": ["parser", "tagger", "lemmatizer", "attribute_ruler"],
            "description": "å°å‹æ¨¡å‹ä»…NER (46MB)"
        },
        "md_full": {
            "model": "zh_core_web_md",
            "exclude": [],
            "description": "ä¸­å‹æ¨¡å‹å®Œæ•´é…ç½® (74MB)"
        },
        "md_ner_only": {
            "model": "zh_core_web_md", 
            "exclude": ["parser", "tagger", "lemmatizer", "attribute_ruler"],
            "description": "ä¸­å‹æ¨¡å‹ä»…NER (74MB)"
        },
        "trf_full": {
            "model": "zh_core_web_trf",
            "exclude": [],
            "description": "Transformeræ¨¡å‹å®Œæ•´é…ç½® (396MB)"
        },
        "trf_ner_only": {
            "model": "zh_core_web_trf",
            "exclude": ["parser", "tagger", "lemmatizer", "attribute_ruler"],
            "description": "Transformeræ¨¡å‹ä»…NER (396MB)"
        }
    }
    
    results = {}
    
    print("ğŸš€ å¼€å§‹å¢å¼ºç‰ˆNERæ¨¡å‹è´¨é‡åŸºå‡†æµ‹è¯•")
    print("=" * 80)
    print(f"ğŸ“Š æµ‹è¯•è¯­æ–™: {len(enhanced_test_texts)} ä¸ªæ–‡æœ¬æ ·æœ¬")
    print(f"ğŸ¯ æµ‹è¯•é…ç½®: {len(configurations)} ä¸ª")
    print(f"ğŸ† å…³æ³¨æŒ‡æ ‡: ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°ã€å¤±è´¥æ¨¡å¼åˆ†æ")
    print("=" * 80)
    
    for config_name, config in configurations.items():
        print(f"\nğŸ“Š æµ‹è¯•é…ç½®: {config['description']}")
        print("-" * 60)
        
        try:
            # åŠ è½½æ¨¡å‹
            print(f"â³ æ­£åœ¨åŠ è½½æ¨¡å‹...")
            start_load = time.time()
            nlp = spacy.load(config["model"], exclude=config["exclude"])
            load_time = time.time() - start_load
            
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {load_time:.2f}ç§’")
            
            # é¢„çƒ­æ¨¡å‹
            print(f"ğŸ”¥ é¢„çƒ­æ¨¡å‹...")
            for sample in enhanced_test_texts[:3]:
                _ = nlp(sample)
            
            # â­ è´¨é‡åˆ†ææµ‹è¯• - ä¸“æ³¨äºè¯†åˆ«å‡†ç¡®æ€§
            print(f"âš¡ å¼€å§‹è´¨é‡åˆ†ææµ‹è¯•...")
            
            # åˆå§‹åŒ–æ€§èƒ½æŒ‡æ ‡è®¡ç®—å™¨
            performance_metrics = DetailedPerformanceMetrics()
            
            all_entities = []
            entity_type_counts = {}
            category_performance = {}
            
            # æŒ‰ç±»åˆ«æµ‹è¯•
            for category, texts in enhanced_categorized_tests.items():
                category_metrics = DetailedPerformanceMetrics()
                
                for text in texts:
                    doc = nlp(text)
                    
                    # æå–é¢„æµ‹å®ä½“
                    pred_entities = [(ent.start_char, ent.end_char, ent.label_) 
                                   for ent in doc.ents]
                    
                    # è·å–é»„é‡‘æ ‡å‡†ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    true_entities = gold_standard.get(text, [])
                    
                    # ç»Ÿè®¡å®ä½“ç±»å‹
                    for _, _, label in pred_entities:
                        entity_type_counts[label] = entity_type_counts.get(label, 0) + 1
                    
                    # æ·»åŠ åˆ°æ€§èƒ½æŒ‡æ ‡è®¡ç®—
                    if true_entities:  # åªå¯¹æœ‰æ ‡æ³¨çš„æ ·æœ¬è®¡ç®—è¯¦ç»†æŒ‡æ ‡
                        performance_metrics.add_example(text, true_entities, pred_entities)
                        category_metrics.add_example(text, true_entities, pred_entities)
                    
                    all_entities.append({
                        "text_id": len(all_entities),
                        "text": text[:100] + "..." if len(text) > 100 else text,
                        "category": category,
                        "true_entities": true_entities,
                        "pred_entities": pred_entities,
                        "entity_count": len(pred_entities),
                        "text_length": len(text)
                    })
                
                # è®¡ç®—åˆ†ç±»åˆ«æ€§èƒ½
                if category in ["ä¸“ä¸šæŒ‘æˆ˜æ€§æµ‹è¯•", "RegExå¤±è´¥æ¡ˆä¾‹"]:  # é‡ç‚¹å…³æ³¨çš„ç±»åˆ«
                    category_performance[category] = category_metrics.calculate_metrics()
            
            # â­ è®¡ç®—è¯¦ç»†æ€§èƒ½æŒ‡æ ‡
            detailed_metrics = performance_metrics.calculate_metrics()
            failure_analysis = performance_metrics.analyze_failure_modes()
            
            # è®¡ç®—åŸºæœ¬ç»Ÿè®¡æŒ‡æ ‡
            total_entities = sum(item["entity_count"] for item in all_entities)
            
            results[config_name] = {
                "model": config["model"],
                "description": config["description"],
                "excluded_components": config["exclude"],
                "active_pipes": nlp.pipe_names,
                
                # åŸºæœ¬è¯†åˆ«ç»Ÿè®¡
                "load_time": load_time,
                "total_entities_found": total_entities,
                "avg_entities_per_text": total_entities / len(enhanced_test_texts),
                "entity_type_counts": entity_type_counts,
                
                # â­ æ ¸å¿ƒè´¨é‡æŒ‡æ ‡
                "detailed_metrics": detailed_metrics,
                "failure_analysis": failure_analysis,
                "category_performance": category_performance,
                
                # æ ·æœ¬ç»“æœï¼ˆé™åˆ¶æ•°é‡ï¼‰
                "detailed_results": all_entities[:5]
            }
            
            print(f"âœ… æµ‹è¯•å®Œæˆ!")
            print(f"ğŸ“Š å¤„ç†æ–‡æœ¬æ€»æ•°: {len(enhanced_test_texts)} ä¸ª")
            print(f"ğŸ¯ å‘ç°å®ä½“æ€»æ•°: {total_entities}")
            
            # â­ æ˜¾ç¤ºè¯¦ç»†è´¨é‡æŒ‡æ ‡
            if detailed_metrics:
                micro_f1 = detailed_metrics.get("micro_avg", {}).get("f1", 0)
                macro_f1 = detailed_metrics.get("macro_avg", {}).get("f1", 0)
                micro_precision = detailed_metrics.get("micro_avg", {}).get("precision", 0)
                micro_recall = detailed_metrics.get("micro_avg", {}).get("recall", 0)
                print(f"ğŸ“Š Micro Precision: {micro_precision:.3f}")
                print(f"ğŸ“Š Micro Recall: {micro_recall:.3f}")
                print(f"ğŸ“Š Micro F1-Score: {micro_f1:.3f}")
                print(f"ğŸ“Š Macro F1-Score: {macro_f1:.3f}")
                
                # æ˜¾ç¤ºä¸»è¦å®ä½“ç±»å‹çš„æ€§èƒ½
                main_types = ["EXCHANGE", "FUTURES_COMPANY", "PRODUCT", "CONTRACT_CODE"]
                for entity_type in main_types:
                    if entity_type in detailed_metrics:
                        metrics = detailed_metrics[entity_type]
                        print(f"  {entity_type}: P={metrics['precision']:.3f}, R={metrics['recall']:.3f}, F1={metrics['f1']:.3f}")
            
        except OSError as e:
            print(f"âŒ æ— æ³•åŠ è½½æ¨¡å‹ {config['model']}: {e}")
            results[config_name] = None
        except Exception as e:
            print(f"âŒ æµ‹è¯•è¿‡ç¨‹å‡ºé”™: {e}")
            results[config_name] = None
    
    return results

# æ‰§è¡Œå¢å¼ºåŸºå‡†æµ‹è¯•
print("ğŸ”„ å¼€å§‹æ‰§è¡Œå¢å¼ºåŸºå‡†æµ‹è¯•...")
print("âš ï¸  æ³¨æ„: æµ‹è¯•å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
enhanced_benchmark_results = enhanced_benchmark_with_detailed_metrics()

# %% [markdown]
# ## 4. å¢å¼ºç»“æœåˆ†æå’Œå¯è§†åŒ–

# %%
def analyze_enhanced_results(results):
    """åˆ†æå¢å¼ºçš„æµ‹è¯•ç»“æœ"""
    
    print(f"\nğŸ“ˆ å¢å¼ºæµ‹è¯•ç»“æœåˆ†æ")
    print("=" * 80)
    
    valid_results = {k: v for k, v in results.items() if v is not None}
    
    if not valid_results:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æµ‹è¯•ç»“æœ")
        return None, None
    
    print(f"âœ… æˆåŠŸæµ‹è¯•é…ç½®: {len(valid_results)}/{len(results)}")
    
    # â­ åˆ›å»ºè´¨é‡å¯¼å‘çš„æ€§èƒ½å¯¹æ¯”DataFrame
    comparison_data = []
    detailed_metrics_data = []
    
    for config_name, result in valid_results.items():
        # æ¨¡å‹åŸºæœ¬ä¿¡æ¯
        size_map = {"sm": "46MB", "md": "74MB", "lg": "575MB", "trf": "396MB"}
        model_size = next((size for key, size in size_map.items() if key in config_name), "æœªçŸ¥")
        
        # â­ è·å–è¯¦ç»†æ€§èƒ½æŒ‡æ ‡
        detailed_metrics = result.get("detailed_metrics", {})
        micro_avg = detailed_metrics.get("micro_avg", {})
        macro_avg = detailed_metrics.get("macro_avg", {})
        
        basic_data = {
            "é…ç½®åç§°": config_name,
            "æ¨¡å‹æè¿°": result["description"],
            "æ¨¡å‹å¤§å°": model_size,
            "åŠ è½½æ—¶é—´(ç§’)": result['load_time'],
            "å‘ç°å®ä½“æ•°": result['total_entities_found'],
            "å¹³å‡å®ä½“æ•°": result['avg_entities_per_text'],
            # â­ æ ¸å¿ƒè´¨é‡æŒ‡æ ‡
            "Micro_Precision": micro_avg.get("precision", 0),
            "Micro_Recall": micro_avg.get("recall", 0),
            "Micro_F1": micro_avg.get("f1", 0),
            "Macro_Precision": macro_avg.get("precision", 0),
            "Macro_Recall": macro_avg.get("recall", 0),
            "Macro_F1": macro_avg.get("f1", 0),
            "è´¨é‡è¯„åˆ†": (micro_avg.get("precision", 0) + micro_avg.get("recall", 0) + micro_avg.get("f1", 0)) * 100 / 3
        }
        comparison_data.append(basic_data)
        
        # â­ å®ä½“ç±»å‹è¯¦ç»†æŒ‡æ ‡
        for entity_type, metrics in detailed_metrics.items():
            if entity_type not in ["micro_avg", "macro_avg"] and isinstance(metrics, dict):
                detailed_metrics_data.append({
                    "é…ç½®åç§°": config_name,
                    "å®ä½“ç±»å‹": entity_type,
                    "ç²¾ç¡®ç‡": metrics.get("precision", 0),
                    "å¬å›ç‡": metrics.get("recall", 0),
                    "F1åˆ†æ•°": metrics.get("f1", 0),
                    "æ”¯æŒåº¦": metrics.get("support", 0),
                    "True_Positive": metrics.get("tp", 0),
                    "False_Positive": metrics.get("fp", 0),
                    "False_Negative": metrics.get("fn", 0)
                })
    
    df_basic = pd.DataFrame(comparison_data)
    df_detailed = pd.DataFrame(detailed_metrics_data)
    
    # æ˜¾ç¤ºåŸºæœ¬æ€§èƒ½å¯¹æ¯”
    print(f"\nğŸ“Š åŸºæœ¬æ€§èƒ½å¯¹æ¯”è¡¨")
    print("-" * 80)
    display(df_basic.round(3))
    
    # æ˜¾ç¤ºè¯¦ç»†æ€§èƒ½æŒ‡æ ‡
    if not df_detailed.empty:
        print(f"\nğŸ“Š å®ä½“ç±»å‹è¯¦ç»†æ€§èƒ½æŒ‡æ ‡")
        print("-" * 80)
        # åªæ˜¾ç¤ºä¸»è¦å®ä½“ç±»å‹
        main_types = ["EXCHANGE", "FUTURES_COMPANY", "PRODUCT", "CONTRACT_CODE", "ORG"]
        df_main = df_detailed[df_detailed["å®ä½“ç±»å‹"].isin(main_types)]
        if not df_main.empty:
            display(df_main.round(3))
    
    return df_basic, df_detailed

# â­ æ–°å¢ï¼šå¤±è´¥æ¨¡å¼è¯¦ç»†åˆ†æå‡½æ•°
def analyze_failure_modes_detailed(results):
    """è¯¦ç»†åˆ†æå¤±è´¥æ¨¡å¼"""
    
    print(f"\nğŸ” å¤±è´¥æ¨¡å¼è¯¦ç»†åˆ†æ")
    print("=" * 80)
    
    valid_results = {k: v for k, v in results.items() if v is not None}
    
    for config_name, result in valid_results.items():
        failure_analysis = result.get("failure_analysis", {})
        
        if not failure_analysis:
            continue
            
        print(f"\nğŸ“‹ æ¨¡å‹: {result['description']}")
        print("-" * 50)
        
        # æŒ‰å®ä½“ç±»å‹çš„å¤±è´¥åˆ†æ
        by_type = failure_analysis.get("by_type", {})
        if by_type:
            print("ğŸ¯ æŒ‰å®ä½“ç±»å‹çš„é”™è¯¯åˆ†å¸ƒ:")
            for entity_type, errors in by_type.items():
                total_errors = errors["FP"] + errors["FN"]
                if total_errors > 0:
                    print(f"  {entity_type}: FP={errors['FP']}, FN={errors['FN']}, æ€»é”™è¯¯={total_errors}")
        
        # æŒ‰æ–‡æœ¬é•¿åº¦çš„å¤±è´¥åˆ†æ
        by_length = failure_analysis.get("by_text_length", {})
        if by_length:
            print("\nğŸ“ æŒ‰æ–‡æœ¬é•¿åº¦çš„é”™è¯¯åˆ†å¸ƒ:")
            for length_cat, errors in by_length.items():
                total_errors = errors["FP"] + errors["FN"]
                if total_errors > 0:
                    print(f"  {length_cat}: FP={errors['FP']}, FN={errors['FN']}, æ€»é”™è¯¯={total_errors}")
        
        # æ˜¾ç¤ºä¸€äº›å…·ä½“å¤±è´¥æ¡ˆä¾‹
        detailed_cases = failure_analysis.get("detailed_cases", [])
        if detailed_cases:
            print(f"\nâŒ å…¸å‹å¤±è´¥æ¡ˆä¾‹ (æ˜¾ç¤ºå‰3ä¸ª):")
            for i, case in enumerate(detailed_cases[:3]):
                print(f"  {i+1}. ç±»å‹: {case['type']}, å®ä½“ç±»å‹: {case['entity_type']}")
                print(f"     æ–‡æœ¬: {case['text'][:80]}...")
                print(f"     å®ä½“: {case['entity']}")

# â­ æ–°å¢ï¼šä¸“ä¸šæµ‹è¯•è¯­æ–™æ€§èƒ½åˆ†æ
def analyze_professional_test_performance(results):
    """åˆ†æä¸“ä¸šæµ‹è¯•è¯­æ–™çš„æ€§èƒ½"""
    
    print(f"\nğŸ“ ä¸“ä¸šæµ‹è¯•è¯­æ–™æ€§èƒ½åˆ†æ")
    print("=" * 80)
    
    valid_results = {k: v for k, v in results.items() if v is not None}
    
    # åˆ†æå„æ¨¡å‹åœ¨ä¸“ä¸šæµ‹è¯•ä¸Šçš„è¡¨ç°
    professional_performance = {}
    
    for config_name, result in valid_results.items():
        category_performance = result.get("category_performance", {})
        
        # é‡ç‚¹å…³æ³¨ä¸“ä¸šæŒ‘æˆ˜æ€§æµ‹è¯•å’ŒRegExå¤±è´¥æ¡ˆä¾‹
        professional_cats = ["ä¸“ä¸šæŒ‘æˆ˜æ€§æµ‹è¯•", "RegExå¤±è´¥æ¡ˆä¾‹"]
        
        for cat in professional_cats:
            if cat in category_performance:
                metrics = category_performance[cat]
                micro_avg = metrics.get("micro_avg", {})
                
                if config_name not in professional_performance:
                    professional_performance[config_name] = {}
                
                professional_performance[config_name][cat] = {
                    "precision": micro_avg.get("precision", 0),
                    "recall": micro_avg.get("recall", 0),
                    "f1": micro_avg.get("f1", 0)
                }
    
    # åˆ›å»ºä¸“ä¸šæµ‹è¯•æ€§èƒ½å¯¹æ¯”è¡¨
    if professional_performance:
        prof_data = []
        for config_name, categories in professional_performance.items():
            for category, metrics in categories.items():
                prof_data.append({
                    "æ¨¡å‹é…ç½®": config_name,
                    "æµ‹è¯•ç±»åˆ«": category,
                    "ç²¾ç¡®ç‡": metrics["precision"],
                    "å¬å›ç‡": metrics["recall"],
                    "F1åˆ†æ•°": metrics["f1"]
                })
        
        if prof_data:
            df_prof = pd.DataFrame(prof_data)
            print("ğŸ“Š ä¸“ä¸šæµ‹è¯•è¯­æ–™æ€§èƒ½å¯¹æ¯”:")
            display(df_prof.round(3))
            
            # åˆ†æå“ªä¸ªæ¨¡å‹åœ¨ä¸“ä¸šæµ‹è¯•ä¸Šè¡¨ç°æœ€å¥½
            pivot_f1 = df_prof.pivot(index="æ¨¡å‹é…ç½®", columns="æµ‹è¯•ç±»åˆ«", values="F1åˆ†æ•°")
            if not pivot_f1.empty:
                print(f"\nğŸ† ä¸“ä¸šæµ‹è¯•F1åˆ†æ•°æ’å:")
                for category in pivot_f1.columns:
                    best_model = pivot_f1[category].idxmax()
                    best_score = pivot_f1[category].max()
                    print(f"  {category}: {best_model} (F1={best_score:.3f})")

# æ‰§è¡Œå¢å¼ºåˆ†æ
df_basic_enhanced, df_detailed_enhanced = analyze_enhanced_results(enhanced_benchmark_results)
analyze_failure_modes_detailed(enhanced_benchmark_results)
analyze_professional_test_performance(enhanced_benchmark_results)

# %% [markdown]
# ## 5. å¢å¼ºå¯è§†åŒ–å›¾è¡¨

# %%
def create_quality_focused_visualizations(df_basic, df_detailed, results):
    """åˆ›å»ºä¸“æ³¨äºè´¨é‡çš„å¯è§†åŒ–å›¾è¡¨"""
    
    if df_basic.empty:
        print("âŒ æ²¡æœ‰æ•°æ®å¯ç”¨äºå¯è§†åŒ–")
        return
    
    # è®¾ç½®å›¾è¡¨å¸ƒå±€ - ä¸“æ³¨äºè´¨é‡æŒ‡æ ‡
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('NER Model Quality Analysis (Accuracy-Focused)', fontsize=18, fontweight='bold')
    
    # 1. â­ Micro F1-Score å¯¹æ¯”
    ax1 = axes[0, 0]
    bars1 = ax1.bar(range(len(df_basic)), df_basic['Micro_F1'], color='lightgreen', alpha=0.7)
    ax1.set_title('Micro F1-Score Comparison', fontweight='bold')
    ax1.set_ylabel('F1-Score')
    ax1.set_xticks(range(len(df_basic)))
    ax1.set_xticklabels(df_basic['é…ç½®åç§°'], rotation=45, ha='right')
    ax1.set_ylim(0, 1)
    
    for i, bar in enumerate(bars1):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{height:.3f}', ha='center', va='bottom', fontsize=8)
    
    # 2. â­ Macro F1-Score å¯¹æ¯”
    ax2 = axes[0, 1]
    bars2 = ax2.bar(range(len(df_basic)), df_basic['Macro_F1'], color='orange', alpha=0.7)
    ax2.set_title('Macro F1-Score Comparison', fontweight='bold')
    ax2.set_ylabel('F1-Score')
    ax2.set_xticks(range(len(df_basic)))
    ax2.set_xticklabels(df_basic['é…ç½®åç§°'], rotation=45, ha='right')
    ax2.set_ylim(0, 1)
    
    for i, bar in enumerate(bars2):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{height:.3f}', ha='center', va='bottom', fontsize=8)
    
    # 3. â­ ç²¾ç¡®ç‡ vs å¬å›ç‡æ•£ç‚¹å›¾
    ax3 = axes[0, 2]
    scatter = ax3.scatter(df_basic['Micro_Precision'], df_basic['Micro_Recall'], 
                         c=df_basic['Micro_F1'], cmap='viridis', 
                         s=150, alpha=0.7, edgecolors='black')
    ax3.set_xlabel('Micro Precision')
    ax3.set_ylabel('Micro Recall')
    ax3.set_title('Precision vs Recall (colored by F1-Score)', fontweight='bold')
    ax3.set_xlim(0, 1)
    ax3.set_ylim(0, 1)
    
    # æ·»åŠ å¯¹è§’çº¿
    ax3.plot([0, 1], [0, 1], 'k--', alpha=0.3)
    
    for i, txt in enumerate(df_basic['é…ç½®åç§°']):
        ax3.annotate(txt, (df_basic['Micro_Precision'].iloc[i], df_basic['Micro_Recall'].iloc[i]), 
                    xytext=(5, 5), textcoords='offset points', fontsize=8)
    
    plt.colorbar(scatter, ax=ax3, label='F1-Score')
    
    # 4. â­ å®ä½“ç±»å‹æ€§èƒ½çƒ­åŠ›å›¾
    ax4 = axes[1, 0]
    if not df_detailed.empty:
        # åˆ›å»ºé€è§†è¡¨
        heatmap_data = df_detailed.pivot_table(
            index='å®ä½“ç±»å‹', 
            columns='é…ç½®åç§°', 
            values='F1åˆ†æ•°', 
            aggfunc='mean'
        ).fillna(0)
        
        if not heatmap_data.empty:
            sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='YlOrRd', 
                       ax=ax4, cbar_kws={'label': 'F1-Score'})
            ax4.set_title('F1-Score by Entity Type', fontweight='bold')
            ax4.set_xlabel('Model Configuration')
            ax4.set_ylabel('Entity Type')
    
    # 5. â­ è´¨é‡æŒ‡æ ‡é›·è¾¾å›¾
    ax5 = axes[1, 1]
    
    # é€‰æ‹©è´¨é‡æŒ‡æ ‡è¿›è¡Œé›·è¾¾å›¾å±•ç¤º
    metrics = ['Micro_F1', 'Macro_F1', 'Micro_Precision', 'Micro_Recall']
    metric_labels = ['Micro F1', 'Macro F1', 'Precision', 'Recall']
    
    angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
    angles += angles[:1]  # å®Œæˆåœ†å½¢
    
    ax5 = plt.subplot(2, 3, 5, projection='polar')
    
    colors = plt.cm.Set2(np.linspace(0, 1, len(df_basic)))
    
    for i, (_, row) in enumerate(df_basic.iterrows()):
        values = [row[metric] for metric in metrics]
        values += values[:1]
        
        ax5.plot(angles, values, 'o-', linewidth=2, 
                label=row['é…ç½®åç§°'], color=colors[i])
        ax5.fill(angles, values, alpha=0.1, color=colors[i])
    
    ax5.set_xticks(angles[:-1])
    ax5.set_xticklabels(metric_labels)
    ax5.set_ylim(0, 1)
    ax5.set_title('Quality Metrics Radar Chart', fontweight='bold', pad=20)
    ax5.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
    
    # 6. â­ å¤±è´¥æ¨¡å¼åˆ†æå›¾
    ax6 = axes[1, 2]
    
    # ç»Ÿè®¡å„æ¨¡å‹çš„å¤±è´¥æ¡ˆä¾‹æ•°é‡
    failure_counts = {}
    for config_name, result in results.items():
        if result is not None:
            failure_analysis = result.get("failure_analysis", {})
            detailed_cases = failure_analysis.get("detailed_cases", [])
            failure_counts[config_name] = len(detailed_cases)
    
    if failure_counts:
        configs = list(failure_counts.keys())
        counts = list(failure_counts.values())
        
        bars6 = ax6.bar(configs, counts, color='lightcoral', alpha=0.7)
        ax6.set_title('Failure Cases Count', fontweight='bold')
        ax6.set_ylabel('Number of Failures')
        ax6.tick_params(axis='x', rotation=45)
        
        for i, bar in enumerate(bars6):
            height = bar.get_height()
            ax6.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                    f'{int(height)}', ha='center', va='bottom', fontsize=8)
    
    plt.tight_layout()
    plt.show()
    
    # â­ æ˜¾ç¤ºè´¨é‡å¯¼å‘çš„Top 3 é…ç½®æ’å
    print(f"\nğŸ† Quality-Focused Performance Ranking")
    print("=" * 60)
    
    # æŒ‰è´¨é‡æŒ‡æ ‡æ’å
    rankings = {
        "Overall F1-Score": df_basic.nlargest(3, 'Micro_F1'),
        "Precision": df_basic.nlargest(3, 'Micro_Precision'),
        "Recall": df_basic.nlargest(3, 'Micro_Recall'),
        "Macro F1": df_basic.nlargest(3, 'Macro_F1')
    }
    
    medals = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰"]
    
    for ranking_type, top_3 in rankings.items():
        print(f"\nğŸ¯ {ranking_type} Top 3:")
        for i, (_, row) in enumerate(top_3.iterrows()):
            if ranking_type == "Overall F1-Score":
                score = f"F1={row['Micro_F1']:.3f}"
            elif ranking_type == "Precision":
                score = f"Precision={row['Micro_Precision']:.3f}"
            elif ranking_type == "Recall":
                score = f"Recall={row['Micro_Recall']:.3f}"
            else:  # Macro F1
                score = f"Macro F1={row['Macro_F1']:.3f}"
            
            print(f"  {medals[i]} {row['æ¨¡å‹æè¿°']}")
            print(f"     {score}")

# åˆ›å»ºè´¨é‡å¯¼å‘çš„å¯è§†åŒ–
if df_basic_enhanced is not None:
    create_quality_focused_visualizations(df_basic_enhanced, df_detailed_enhanced, enhanced_benchmark_results)

# %% [markdown]
# ## 6. å¢å¼ºä¼˜åŒ–å»ºè®®

# %%
def generate_quality_focused_recommendations(df_basic, df_detailed, results):
    """ç”Ÿæˆä¸“æ³¨äºè´¨é‡çš„ä¼˜åŒ–å»ºè®®"""
    
    if df_basic.empty:
        print("âŒ æ²¡æœ‰æ•°æ®å¯ç”¨äºç”Ÿæˆå»ºè®®")
        return
    
    print(f"\nğŸ’¡ è´¨é‡å¯¼å‘çš„æ¨¡å‹ä¼˜åŒ–å»ºè®®")
    print("=" * 80)
    
    # åˆ†ææœ€ä¼˜é…ç½®
    best_f1_config = df_basic.loc[df_basic['Micro_F1'].idxmax()]
    best_precision_config = df_basic.loc[df_basic['Micro_Precision'].idxmax()]
    best_recall_config = df_basic.loc[df_basic['Micro_Recall'].idxmax()]
    best_macro_f1_config = df_basic.loc[df_basic['Macro_F1'].idxmax()]
    best_overall_config = df_basic.loc[df_basic['è´¨é‡è¯„åˆ†'].idxmax()]
    
    print(f"ğŸ¯ **æœ€ä½³F1åˆ†æ•°é…ç½®**: {best_f1_config['æ¨¡å‹æè¿°']}")
    print(f"   ğŸ“Š Micro F1: {best_f1_config['Micro_F1']:.3f}")
    print(f"   ğŸ“ˆ Precision: {best_f1_config['Micro_Precision']:.3f}")
    print(f"   ğŸ“‰ Recall: {best_f1_config['Micro_Recall']:.3f}")
    print(f"   ğŸ¯ å‘ç°å®ä½“: {best_f1_config['å‘ç°å®ä½“æ•°']} ä¸ª")
    
    print(f"\nğŸ–ï¸ **æœ€ä½³ç²¾ç¡®ç‡é…ç½®**: {best_precision_config['æ¨¡å‹æè¿°']}")
    print(f"   ğŸ“ˆ Precision: {best_precision_config['Micro_Precision']:.3f}")
    print(f"   ğŸ“Š F1: {best_precision_config['Micro_F1']:.3f}")
    print(f"   ğŸ“‰ Recall: {best_precision_config['Micro_Recall']:.3f}")
    
    print(f"\nğŸ” **æœ€ä½³å¬å›ç‡é…ç½®**: {best_recall_config['æ¨¡å‹æè¿°']}")
    print(f"   ğŸ“‰ Recall: {best_recall_config['Micro_Recall']:.3f}")
    print(f"   ğŸ“Š F1: {best_recall_config['Micro_F1']:.3f}")
    print(f"   ğŸ“ˆ Precision: {best_recall_config['Micro_Precision']:.3f}")
    
    print(f"\nğŸŒŸ **æœ€ä½³å®å¹³å‡F1é…ç½®**: {best_macro_f1_config['æ¨¡å‹æè¿°']}")
    print(f"   ğŸ“Š Macro F1: {best_macro_f1_config['Macro_F1']:.3f}")
    print(f"   ğŸ“Š Micro F1: {best_macro_f1_config['Micro_F1']:.3f}")
    
    print(f"\nğŸ† **ç»¼åˆè´¨é‡æœ€ä¼˜é…ç½®**: {best_overall_config['æ¨¡å‹æè¿°']}")
    print(f"   ğŸ–ï¸ è´¨é‡è¯„åˆ†: {best_overall_config['è´¨é‡è¯„åˆ†']:.1f}")
    print(f"   ğŸ“Š Micro F1: {best_overall_config['Micro_F1']:.3f}")
    print(f"   ğŸ“ˆ Precision: {best_overall_config['Micro_Precision']:.3f}")
    print(f"   ğŸ“‰ Recall: {best_overall_config['Micro_Recall']:.3f}")
    
    # â­ ä¸“ä¸šåœºæ™¯è´¨é‡ä¼˜åŒ–å»ºè®®
    print(f"\nğŸ“‹ **ä¸“ä¸šæœŸè´§äº¤æ˜“åœºæ™¯è´¨é‡ä¼˜åŒ–å»ºè®®**:")
    print(f"=" * 60)
    
    scenarios = [
        {
            "scenario": "ğŸ¯ ç²¾ç¡®åˆçº¦è¯†åˆ« (å‡†ç¡®ç‡ä¼˜å…ˆ)",
            "recommendation": best_precision_config['é…ç½®åç§°'],
            "rationale": "åˆçº¦ä»£ç è¯†åˆ«éœ€è¦æé«˜çš„ç²¾ç¡®ç‡ï¼Œé¿å…è¯¯è¯†åˆ«",
            "config": f"spacy.load('{best_precision_config['é…ç½®åç§°'].split('_')[0]}_core_web_{best_precision_config['é…ç½®åç§°'].split('_')[1]}', exclude={['parser','tagger','lemmatizer','attribute_ruler'] if 'ner_only' in best_precision_config['é…ç½®åç§°'] else []})",
            "use_cases": ["åˆçº¦ä»£ç è‡ªåŠ¨è¯†åˆ«", "äº¤æ˜“æŒ‡ä»¤è§£æ", "é£é™©æ•å£è®¡ç®—"],
            "metrics": f"Precision={best_precision_config['Micro_Precision']:.3f}, F1={best_precision_config['Micro_F1']:.3f}"
        },
        {
            "scenario": "ğŸ” å…¨é¢ä¿¡æ¯æå– (å¬å›ç‡ä¼˜å…ˆ)",
            "recommendation": best_recall_config['é…ç½®åç§°'],
            "rationale": "éœ€è¦å°½å¯èƒ½å¤šåœ°è¯†åˆ«å‡ºæ‰€æœ‰ç›¸å…³å®ä½“ï¼Œé¿å…é—æ¼",
            "config": f"spacy.load('{best_recall_config['é…ç½®åç§°'].split('_')[0]}_core_web_{best_recall_config['é…ç½®åç§°'].split('_')[1]}', exclude={['parser','tagger','lemmatizer','attribute_ruler'] if 'ner_only' in best_recall_config['é…ç½®åç§°'] else []})",
            "use_cases": ["ç›‘ç®¡åˆè§„æ£€æŸ¥", "å…¨é‡æ•°æ®æŒ–æ˜", "å†å²æ–‡æ¡£åˆ†æ"],
            "metrics": f"Recall={best_recall_config['Micro_Recall']:.3f}, F1={best_recall_config['Micro_F1']:.3f}"
        },
        {
            "scenario": "âš–ï¸ å¹³è¡¡æ€§èƒ½åº”ç”¨ (F1åˆ†æ•°æœ€ä¼˜)",
            "recommendation": best_f1_config['é…ç½®åç§°'],
            "rationale": "åœ¨ç²¾ç¡®ç‡å’Œå¬å›ç‡ä¹‹é—´æ‰¾åˆ°æœ€ä½³å¹³è¡¡ç‚¹",
            "config": f"spacy.load('{best_f1_config['é…ç½®åç§°'].split('_')[0]}_core_web_{best_f1_config['é…ç½®åç§°'].split('_')[1]}', exclude={['parser','tagger','lemmatizer','attribute_ruler'] if 'ner_only' in best_f1_config['é…ç½®åç§°'] else []})",
            "use_cases": ["ç ”ç©¶æŠ¥å‘Šè§£æ", "å®¢æˆ·æŸ¥è¯¢å“åº”", "æ™ºèƒ½é—®ç­”ç³»ç»Ÿ"],
            "metrics": f"F1={best_f1_config['Micro_F1']:.3f}, P={best_f1_config['Micro_Precision']:.3f}, R={best_f1_config['Micro_Recall']:.3f}"
        },
        {
            "scenario": "ğŸŒˆ å¤šç±»åˆ«å‡è¡¡ (å®å¹³å‡F1ä¼˜å…ˆ)",
            "recommendation": best_macro_f1_config['é…ç½®åç§°'],
            "rationale": "ç¡®ä¿å„ç§å®ä½“ç±»å‹éƒ½æœ‰è¾ƒå¥½çš„è¯†åˆ«æ•ˆæœ",
            "config": f"spacy.load('{best_macro_f1_config['é…ç½®åç§°'].split('_')[0]}_core_web_{best_macro_f1_config['é…ç½®åç§°'].split('_')[1]}', exclude={['parser','tagger','lemmatizer','attribute_ruler'] if 'ner_only' in best_macro_f1_config['é…ç½®åç§°'] else []})",
            "use_cases": ["å¤šå…ƒåŒ–ä¿¡æ¯æå–", "è·¨ç±»åˆ«åˆ†æ", "å®Œæ•´æ€§æ£€æŸ¥"],
            "metrics": f"Macro F1={best_macro_f1_config['Macro_F1']:.3f}, Micro F1={best_macro_f1_config['Micro_F1']:.3f}"
        }
    ]
    
    for scenario in scenarios:
        print(f"\n{scenario['scenario']}")
        print(f"   ğŸ”§ æ¨èé…ç½®: {scenario['recommendation']}")
        print(f"   ğŸ’¡ é€‰æ‹©ç†ç”±: {scenario['rationale']}")
        print(f"   ğŸ“Š æ€§èƒ½æŒ‡æ ‡: {scenario['metrics']}")
        print(f"   ğŸ’» ä»£ç ç¤ºä¾‹: {scenario['config']}")
        print(f"   ğŸ“ é€‚ç”¨åœºæ™¯: {', '.join(scenario['use_cases'])}")
    
    # â­ å¤±è´¥æ¨¡å¼ç‰¹å®šä¼˜åŒ–å»ºè®®
    print(f"\nğŸ› ï¸ **é’ˆå¯¹å¤±è´¥æ¨¡å¼çš„è´¨é‡æå‡ç­–ç•¥**:")
    print(f"=" * 60)
    
    # åˆ†æä¸»è¦å¤±è´¥æ¨¡å¼
    main_failure_types = set()
    for config_name, result in results.items():
        if result is not None:
            failure_analysis = result.get("failure_analysis", {})
            by_type = failure_analysis.get("by_type", {})
            main_failure_types.update(by_type.keys())
    
    optimization_strategies = {
        "CONTRACT_CODE": [
            "âœ… ä½¿ç”¨è‡ªå®šä¹‰è§„åˆ™åŒ¹é…å¢å¼ºåˆçº¦ä»£ç è¯†åˆ«å‡†ç¡®æ€§",
            "âœ… æ„å»ºä¸“é—¨çš„åˆçº¦ä»£ç è¯å…¸è¿›è¡Œåå¤„ç†éªŒè¯",
            "âœ… è®­ç»ƒé’ˆå¯¹æœŸè´§åˆçº¦ä»£ç çš„ç‰¹åŒ–æ¨¡å‹",
            "âœ… ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼é¢„ç­›é€‰å€™é€‰å®ä½“ï¼Œæé«˜ç²¾ç¡®ç‡",
            "âœ… å»ºç«‹åˆçº¦ä»£ç æ ¼å¼éªŒè¯æœºåˆ¶"
        ],
        "EXCHANGE": [
            "âœ… å»ºç«‹äº¤æ˜“æ‰€åˆ«åæ˜ å°„è¡¨ï¼Œç»Ÿä¸€ä¸åŒè¡¨è¿°",
            "âœ… å¢åŠ ä¸­è‹±æ–‡æ··åˆè¡¨è¾¾çš„è®­ç»ƒæ ·æœ¬", 
            "âœ… ä½¿ç”¨åŸºäºè§„åˆ™çš„åå¤„ç†çº æ­£è¯†åˆ«é”™è¯¯",
            "âœ… è€ƒè™‘ä¸Šä¸‹æ–‡ä¿¡æ¯æé«˜æ­§ä¹‰æ¶ˆè§£èƒ½åŠ›",
            "âœ… ç»´æŠ¤äº¤æ˜“æ‰€å®˜æ–¹åç§°ä¸ç®€ç§°å¯¹ç…§è¡¨"
        ],
        "FUTURES_COMPANY": [
            "âœ… ç»´æŠ¤æœŸè´§å…¬å¸å…¨ç§°ä¸ç®€ç§°å¯¹ç…§è¡¨",
            "âœ… å¤„ç†å¤æ‚çš„ä¼ä¸šç»„ç»‡æ¶æ„å…³ç³»",
            "âœ… å¢å¼ºå¯¹å­å…¬å¸ã€åˆ†æ”¯æœºæ„çš„è¯†åˆ«",
            "âœ… ä½¿ç”¨å®ä½“é“¾æ¥æŠ€æœ¯ç»Ÿä¸€ä¸åŒè¡¨è¿°",
            "âœ… å»ºç«‹æœŸè´§å…¬å¸ä¸šåŠ¡èŒƒå›´è¯†åˆ«è§„åˆ™"
        ],
        "PRODUCT": [
            "âœ… åŒºåˆ†æœŸè´§å“ç§ä¸å…¶ä»–åŒåå®ä½“",
            "âœ… å»ºç«‹å“ç§ä»£ç ä¸ä¸­æ–‡åç§°æ˜ å°„å…³ç³»",
            "âœ… å¤„ç†å“ç§åç§°çš„å¤šç§å˜ä½“è¡¨è¾¾",
            "âœ… ç»“åˆä¸Šä¸‹æ–‡åˆ¤æ–­å®ä½“çš„çœŸå®å«ä¹‰",
            "âœ… ä½¿ç”¨å“ç§åˆ†ç±»è§„åˆ™æé«˜è¯†åˆ«å‡†ç¡®æ€§"
        ]
    }
    
    for entity_type, strategies in optimization_strategies.items():
        if entity_type in main_failure_types:
            print(f"\nğŸ“‹ {entity_type} å®ä½“è´¨é‡æå‡ç­–ç•¥:")
            for strategy in strategies:
                print(f"   {strategy}")
    
    # â­ è´¨é‡ä¼˜åŒ–ä»£ç å®ç°
    print(f"\nğŸ’» **è´¨é‡ä¼˜åŒ–ä»£ç å®ç°å»ºè®®**:")
    print(f"=" * 60)
    
    code_examples = f"""
# 1. é«˜ç²¾ç¡®ç‡NERé…ç½® (é€‚ç”¨äºå…³é”®ä¸šåŠ¡)
import spacy
from typing import List, Tuple, Dict

def setup_high_precision_ner():
    '''è®¾ç½®é«˜ç²¾ç¡®ç‡çš„NERç®¡é“'''
    # ä½¿ç”¨ç²¾ç¡®ç‡æœ€é«˜çš„é…ç½®
    nlp = spacy.load("{best_precision_config['é…ç½®åç§°'].split('_')[0]}_core_web_{best_precision_config['é…ç½®åç§°'].split('_')[1]}")
    
    # ç²¾ç¡®ç‡ä¼˜å…ˆçš„åå¤„ç†
    def high_precision_postprocess(entities: List[Tuple], text: str, confidence_threshold: float = 0.8) -> List[Tuple]:
        '''åå¤„ç†ï¼šæé«˜ç²¾ç¡®ç‡ï¼Œé™ä½è¯¯æŠ¥'''
        validated_entities = []
        
        for start, end, label in entities:
            entity_text = text[start:end]
            confidence = calculate_entity_confidence(entity_text, label)
            
            # åªä¿ç•™é«˜ç½®ä¿¡åº¦çš„å®ä½“
            if confidence >= confidence_threshold:
                validated_entities.append((start, end, label))
        
        return validated_entities
    
    def calculate_entity_confidence(entity_text: str, label: str) -> float:
        '''è®¡ç®—å®ä½“ç½®ä¿¡åº¦'''
        # åŸºäºè§„åˆ™çš„ç½®ä¿¡åº¦è®¡ç®—
        confidence = 0.5  # åŸºç¡€ç½®ä¿¡åº¦
        
        if label == "CONTRACT_CODE":
            # åˆçº¦ä»£ç æ ¼å¼æ£€æŸ¥
            import re
            if re.match(r'^[A-Z]{{1,3}}\\d{{4}}

# ç”Ÿæˆè´¨é‡å¯¼å‘çš„ä¼˜åŒ–å»ºè®®
if df_basic_enhanced is not None:
    generate_quality_focused_recommendations(df_basic_enhanced, df_detailed_enhanced, enhanced_benchmark_results)

# â­ ä¿å­˜è´¨é‡åˆ†æç»“æœ
def save_quality_focused_results(basic_results, detailed_results, benchmark_results):
    """ä¿å­˜è´¨é‡å¯¼å‘çš„æµ‹è¯•ç»“æœ"""
    
    print(f"\nğŸ’¾ ä¿å­˜è´¨é‡åˆ†æç»“æœ")
    print("=" * 60)
    
    # ä¿å­˜åŸºæœ¬è´¨é‡å¯¹æ¯”
    if basic_results is not None and not basic_results.empty:
        basic_results.to_csv("quality_focused_ner_comparison.csv", index=False, encoding="utf-8")
        print(f"âœ… åŸºæœ¬è´¨é‡å¯¹æ¯”å·²ä¿å­˜åˆ°: quality_focused_ner_comparison.csv")
    
    # ä¿å­˜è¯¦ç»†å®ä½“æ€§èƒ½
    if detailed_results is not None and not detailed_results.empty:
        detailed_results.to_csv("detailed_entity_quality_metrics.csv", index=False, encoding="utf-8")
        print(f"âœ… è¯¦ç»†å®ä½“æ€§èƒ½å·²ä¿å­˜åˆ°: detailed_entity_quality_metrics.csv")
    
    # ä¿å­˜åŸå§‹åŸºå‡†æµ‹è¯•ç»“æœ
    clean_results = {}
    for config_name, result in benchmark_results.items():
        if result is not None:
            clean_result = result.copy()
            # ç§»é™¤ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡
            clean_result.pop("detailed_results", None)
            # ç®€åŒ–å¤±è´¥åˆ†ææ•°æ®
            if "failure_analysis" in clean_result:
                failure_analysis = clean_result["failure_analysis"]
                if "detailed_cases" in failure_analysis:
                    # åªä¿ç•™å‰10ä¸ªå¤±è´¥æ¡ˆä¾‹
                    failure_analysis["detailed_cases"] = failure_analysis["detailed_cases"][:10]
            clean_results[config_name] = clean_result
    
    with open("quality_focused_benchmark_results.json", "w", encoding="utf-8") as f:
        json.dump(clean_results, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"âœ… å®Œæ•´è´¨é‡åˆ†æç»“æœå·²ä¿å­˜åˆ°: quality_focused_benchmark_results.json")
    print(f"ğŸ“Š ç»“æœæ–‡ä»¶ä¸“æ³¨äºè¯†åˆ«è´¨é‡åˆ†æï¼Œå¯åœ¨Excelä¸­è¿›ä¸€æ­¥åˆ†æ")

# ä¿å­˜ç»“æœ
save_quality_focused_results(df_basic_enhanced, df_detailed_enhanced, enhanced_benchmark_results)

# æ€»ç»“
print(f"\nğŸ‰ **è´¨é‡å¯¼å‘NERæ¨¡å‹åŸºå‡†æµ‹è¯•å®Œæˆ!**")
print("=" * 80)
print(f"ğŸ“Š æœ¬æ¬¡æµ‹è¯•è¯„ä¼°äº† {len([r for r in enhanced_benchmark_results.values() if r is not None])} ä¸ªæœ‰æ•ˆé…ç½®")
print(f"ğŸ“ ä½¿ç”¨äº† {len(enhanced_test_texts)} ä¸ªå¢å¼ºæµ‹è¯•æ–‡æœ¬ï¼ŒåŒ…å«ä¸“ä¸šæœŸè´§åˆçº¦ä»£ç æµ‹è¯•")
print(f"ğŸ¯ é‡ç‚¹åˆ†æäº†ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°ç­‰è´¨é‡æŒ‡æ ‡")
print(f"ğŸ” æä¾›äº†è¯¦ç»†çš„å¤±è´¥æ¨¡å¼åˆ†æå’Œè´¨é‡æå‡å»ºè®®")
print(f"ğŸ’¡ é’ˆå¯¹æœŸè´§äº¤æ˜“åœºæ™¯ç»™å‡ºäº†ä¸“ä¸šçš„è´¨é‡ä¼˜åŒ–ç­–ç•¥")
print(f"\nğŸ† ä¸»è¦å‘ç°:")
if df_basic_enhanced is not None and not df_basic_enhanced.empty:
    best_f1_model = df_basic_enhanced.loc[df_basic_enhanced['Micro_F1'].idxmax()]
    best_precision_model = df_basic_enhanced.loc[df_basic_enhanced['Micro_Precision'].idxmax()]
    best_recall_model = df_basic_enhanced.loc[df_basic_enhanced['Micro_Recall'].idxmax()]
    print(f"   ğŸ“ˆ æœ€ä½³F1åˆ†æ•°: {best_f1_model['æ¨¡å‹æè¿°']} (F1={best_f1_model['Micro_F1']:.3f})")
    print(f"   ğŸ¯ æœ€ä½³ç²¾ç¡®ç‡: {best_precision_model['æ¨¡å‹æè¿°']} (Precision={best_precision_model['Micro_Precision']:.3f})")
    print(f"   ğŸ” æœ€ä½³å¬å›ç‡: {best_recall_model['æ¨¡å‹æè¿°']} (Recall={best_recall_model['Micro_Recall']:.3f})")
    print(f"   ğŸ“ ä¸“ä¸šåˆçº¦ä»£ç è¯†åˆ«èƒ½åŠ›å¾—åˆ°é‡ç‚¹æµ‹è¯•å’Œåˆ†æ")
print(f"\nğŸ’» å»ºè®®æ ¹æ®å…·ä½“è´¨é‡è¦æ±‚é€‰æ‹©åˆé€‚çš„æ¨¡å‹é…ç½®è¿›è¡Œéƒ¨ç½²!")
print(f"ğŸ¯ é‡ç‚¹å…³æ³¨precision/recallæƒè¡¡ï¼Œé’ˆå¯¹ä¸šåŠ¡åœºæ™¯ä¼˜åŒ–è¯†åˆ«è´¨é‡!")
, entity_text):
                confidence += 0.4
        elif label == "EXCHANGE":
            # äº¤æ˜“æ‰€åç§°éªŒè¯
            exchange_keywords = ["äº¤æ˜“æ‰€", "å•†æ‰€", "æ‰€"]
            if any(keyword in entity_text for keyword in exchange_keywords):
                confidence += 0.3
        elif label == "FUTURES_COMPANY":
            # æœŸè´§å…¬å¸åç§°éªŒè¯
            if "æœŸè´§" in entity_text:
                confidence += 0.3
        
        return min(confidence, 1.0)
    
    return nlp, high_precision_postprocess

# 2. é«˜å¬å›ç‡NERé…ç½® (é€‚ç”¨äºä¿¡æ¯æ”¶é›†)
def setup_high_recall_ner():
    '''è®¾ç½®é«˜å¬å›ç‡çš„NERç®¡é“'''
    # ä½¿ç”¨å¬å›ç‡æœ€é«˜çš„é…ç½®
    nlp = spacy.load("{best_recall_config['é…ç½®åç§°'].split('_')[0]}_core_web_{best_recall_config['é…ç½®åç§°'].split('_')[1]}")
    
    # å¤šæ¨¡å¼åŒ¹é…å¢å¼ºå¬å›ç‡
    from spacy.matcher import Matcher
    matcher = Matcher(nlp.vocab)
    
    # æ·»åŠ æ›´å¤šåŒ¹é…æ¨¡å¼
    patterns = {{
        "CONTRACT_CODE": [
            [{{"TEXT": {{"REGEX": r"[A-Z]{{1,3}}\\d{{4}}"}}}}],  # æ ‡å‡†åˆçº¦ä»£ç 
            [{{"TEXT": {{"REGEX": r"[A-Za-z]{{1,3}}\\d{{4}}"}}}}],  # åŒ…å«å°å†™å­—æ¯
        ],
        "EXCHANGE_ALIAS": [
            [{{"LOWER": "ä¸ŠæœŸæ‰€"}}],
            [{{"LOWER": "å¤§å•†æ‰€"}}],
            [{{"LOWER": "éƒ‘å•†æ‰€"}}],
            [{{"LOWER": "ä¸­é‡‘æ‰€"}}],
        ]
    }}
    
    for label, pattern_list in patterns.items():
        matcher.add(label, pattern_list)
    
    def high_recall_extract(text: str) -> List[Tuple]:
        '''é«˜å¬å›ç‡å®ä½“æå–'''
        doc = nlp(text)
        entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]
        
        # æ·»åŠ è§„åˆ™åŒ¹é…çš„ç»“æœ
        matches = matcher(doc)
        for match_id, start, end in matches:
            span = doc[start:end]
            label = nlp.vocab.strings[match_id]
            entities.append((span.start_char, span.end_char, label))
        
        # å»é‡
        entities = list(set(entities))
        return sorted(entities)
    
    return high_recall_extract

# 3. å¹³è¡¡F1åˆ†æ•°çš„NERé…ç½® (æ¨èç”¨äºç”Ÿäº§ç¯å¢ƒ)
def setup_balanced_ner():
    '''è®¾ç½®å¹³è¡¡F1åˆ†æ•°çš„NERç®¡é“'''
    # ä½¿ç”¨F1åˆ†æ•°æœ€é«˜çš„é…ç½®
    nlp = spacy.load("{best_f1_config['é…ç½®åç§°'].split('_')[0]}_core_web_{best_f1_config['é…ç½®åç§°'].split('_')[1]}")
    
    def balanced_ner_pipeline(text: str) -> Dict[str, any]:
        '''å¹³è¡¡çš„NERç®¡é“ï¼Œè¿”å›è¯¦ç»†ç»“æœ'''
        doc = nlp(text)
        
        entities = []
        for ent in doc.ents:
            entity_info = {{
                "text": ent.text,
                "label": ent.label_,
                "start": ent.start_char,
                "end": ent.end_char,
                "confidence": calculate_entity_confidence(ent.text, ent.label_)
            }}
            entities.append(entity_info)
        
        return {{
            "entities": entities,
            "entity_count": len(entities),
            "text_length": len(text),
            "model_info": "{{}}".format("{best_f1_config['æ¨¡å‹æè¿°']}")
        }}
    
    return balanced_ner_pipeline

# 4. è´¨é‡ç›‘æ§å’Œè¯„ä¼°å‡½æ•°
def monitor_ner_quality(predictions: List[Tuple], ground_truth: List[Tuple]) -> Dict[str, float]:
    '''ç›‘æ§NERè´¨é‡'''
    from sklearn.metrics import precision_recall_fscore_support
    
    # è½¬æ¢ä¸ºæ ‡ç­¾åºåˆ—è¿›è¡Œè¯„ä¼°
    pred_labels = [label for _, _, label in predictions]
    true_labels = [label for _, _, label in ground_truth]
    
    if len(pred_labels) == 0 and len(true_labels) == 0:
        return {{"precision": 1.0, "recall": 1.0, "f1": 1.0}}
    
    precision, recall, f1, _ = precision_recall_fscore_support(
        true_labels, pred_labels, average='micro', zero_division=0
    )
    
    return {{
        "precision": precision,
        "recall": recall, 
        "f1": f1,
        "total_predictions": len(predictions),
        "total_ground_truth": len(ground_truth)
    }}

# 5. ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²ç¤ºä¾‹
class ProductionNERService:
    '''ç”Ÿäº§ç¯å¢ƒNERæœåŠ¡'''
    
    def __init__(self, model_type: str = "balanced"):
        self.model_type = model_type
        self.quality_stats = {{
            "total_processed": 0,
            "total_entities": 0,
            "avg_confidence": 0.0
        }}
        
        if model_type == "precision":
            self.nlp, self.postprocess = setup_high_precision_ner()
        elif model_type == "recall":
            self.extract_func = setup_high_recall_ner()
        else:  # balanced
            self.pipeline = setup_balanced_ner()
    
    def extract_entities(self, text: str) -> Dict[str, any]:
        '''æå–å®ä½“'''
        if self.model_type == "balanced":
            result = self.pipeline(text)
        else:
            # å…¶ä»–ç±»å‹çš„å¤„ç†é€»è¾‘
            entities = self.extract_func(text) if self.model_type == "recall" else []
            result = {{"entities": entities}}
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.quality_stats["total_processed"] += 1
        self.quality_stats["total_entities"] += len(result["entities"])
        
        return result
    
    def get_quality_report(self) -> Dict[str, any]:
        '''è·å–è´¨é‡æŠ¥å‘Š'''
        avg_entities = (self.quality_stats["total_entities"] / 
                       max(self.quality_stats["total_processed"], 1))
        
        return {{
            "model_type": self.model_type,
            "total_processed": self.quality_stats["total_processed"],
            "avg_entities_per_text": avg_entities,
            "recommended_for": self._get_recommendation()
        }}
    
    def _get_recommendation(self) -> str:
        '''è·å–ä½¿ç”¨å»ºè®®'''
        recommendations = {{
            "precision": "å…³é”®ä¸šåŠ¡åœºæ™¯ï¼Œéœ€è¦é«˜å‡†ç¡®ç‡",
            "recall": "ä¿¡æ¯æ”¶é›†åœºæ™¯ï¼Œéœ€è¦é«˜è¦†ç›–ç‡", 
            "balanced": "ä¸€èˆ¬ä¸šåŠ¡åœºæ™¯ï¼Œå¹³è¡¡å‡†ç¡®ç‡å’Œè¦†ç›–ç‡"
        }}
        return recommendations.get(self.model_type, "é€šç”¨åœºæ™¯")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æµ‹è¯•æ–‡æœ¬
    test_text = "éƒ‘å•†æ‰€AP2502æœŸè´§æ€ä¹ˆæ ·äº†"
    
    # ä¸åŒè´¨é‡ç›®æ ‡çš„å¤„ç†
    print("é«˜ç²¾ç¡®ç‡å¤„ç†:")
    nlp_precision, postprocess = setup_high_precision_ner()
    doc = nlp_precision(test_text)
    entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]
    validated_entities = postprocess(entities, test_text)
    print(f"ç»“æœ: {{validated_entities}}")
    
    print("\\né«˜å¬å›ç‡å¤„ç†:")
    extract_recall = setup_high_recall_ner()
    recall_entities = extract_recall(test_text)
    print(f"ç»“æœ: {{recall_entities}}")
    
    print("\\nå¹³è¡¡å¤„ç†:")
    balanced_pipeline = setup_balanced_ner()
    balanced_result = balanced_pipeline(test_text)
    print(f"ç»“æœ: {{balanced_result}}")
    
    # ç”Ÿäº§æœåŠ¡ç¤ºä¾‹
    service = ProductionNERService("balanced")
    result = service.extract_entities(test_text)
    quality_report = service.get_quality_report()
    print(f"\\nç”Ÿäº§æœåŠ¡ç»“æœ: {{result}}")
    print(f"è´¨é‡æŠ¥å‘Š: {{quality_report}}")
"""
    
    print(code_examples)

# ç”Ÿæˆå¢å¼ºä¼˜åŒ–å»ºè®®
if df_basic_enhanced is not None:
    generate_enhanced_optimization_recommendations(df_basic_enhanced, df_detailed_enhanced, enhanced_benchmark_results)

# â­ ä¿å­˜å¢å¼ºç»“æœ
def save_enhanced_results(basic_results, detailed_results, benchmark_results):
    """ä¿å­˜å¢å¼ºçš„æµ‹è¯•ç»“æœ"""
    
    print(f"\nğŸ’¾ ä¿å­˜å¢å¼ºæµ‹è¯•ç»“æœ")
    print("=" * 60)
    
    # ä¿å­˜åŸºæœ¬æ€§èƒ½å¯¹æ¯”
    if basic_results is not None and not basic_results.empty:
        basic_results.to_csv("enhanced_ner_performance_comparison.csv", index=False, encoding="utf-8")
        print(f"âœ… åŸºæœ¬æ€§èƒ½å¯¹æ¯”å·²ä¿å­˜åˆ°: enhanced_ner_performance_comparison.csv")
    
    # ä¿å­˜è¯¦ç»†å®ä½“æ€§èƒ½
    if detailed_results is not None and not detailed_results.empty:
        detailed_results.to_csv("detailed_entity_performance.csv", index=False, encoding="utf-8")
        print(f"âœ… è¯¦ç»†å®ä½“æ€§èƒ½å·²ä¿å­˜åˆ°: detailed_entity_performance.csv")
    
    # ä¿å­˜åŸå§‹åŸºå‡†æµ‹è¯•ç»“æœ
    clean_results = {}
    for config_name, result in benchmark_results.items():
        if result is not None:
            clean_result = result.copy()
            # ç§»é™¤ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡
            clean_result.pop("detailed_results", None)
            # ç®€åŒ–å¤±è´¥åˆ†ææ•°æ®
            if "failure_analysis" in clean_result:
                failure_analysis = clean_result["failure_analysis"]
                if "detailed_cases" in failure_analysis:
                    # åªä¿ç•™å‰10ä¸ªå¤±è´¥æ¡ˆä¾‹
                    failure_analysis["detailed_cases"] = failure_analysis["detailed_cases"][:10]
            clean_results[config_name] = clean_result
    
    with open("enhanced_ner_benchmark_results.json", "w", encoding="utf-8") as f:
        json.dump(clean_results, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"âœ… å®Œæ•´åŸºå‡†æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: enhanced_ner_benchmark_results.json")
    print(f"ğŸ“Š ç»“æœæ–‡ä»¶å¯åœ¨Excelæˆ–å…¶ä»–å·¥å…·ä¸­è¿›ä¸€æ­¥åˆ†æ")

# ä¿å­˜ç»“æœ
save_enhanced_results(df_basic_enhanced, df_detailed_enhanced, enhanced_benchmark_results)

# æ€»ç»“
print(f"\nğŸ‰ **å¢å¼ºç‰ˆNERæ¨¡å‹åŸºå‡†æµ‹è¯•å®Œæˆ!**")
print("=" * 80)
print(f"ğŸ“Š æœ¬æ¬¡æµ‹è¯•è¯„ä¼°äº† {len([r for r in enhanced_benchmark_results.values() if r is not None])} ä¸ªæœ‰æ•ˆé…ç½®")
print(f"ğŸ“ ä½¿ç”¨äº† {len(enhanced_test_texts)} ä¸ªå¢å¼ºæµ‹è¯•æ–‡æœ¬ï¼ŒåŒ…å«ä¸“ä¸šæœŸè´§åˆçº¦ä»£ç æµ‹è¯•")
print(f"ğŸ¯ æ–°å¢äº†ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°ç­‰è¯¦ç»†æ€§èƒ½æŒ‡æ ‡")
print(f"ğŸ” æä¾›äº†è¯¦ç»†çš„å¤±è´¥æ¨¡å¼åˆ†æå’Œä¼˜åŒ–å»ºè®®")
print(f"ğŸ’¡ é’ˆå¯¹æœŸè´§äº¤æ˜“åœºæ™¯ç»™å‡ºäº†ä¸“ä¸šçš„æ¨¡å‹é€‰æ‹©å’Œéƒ¨ç½²å»ºè®®")
print(f"\nğŸ† ä¸»è¦å‘ç°:")
if df_basic_enhanced is not None and not df_basic_enhanced.empty:
    best_f1_model = df_basic_enhanced.loc[df_basic_enhanced['Micro_F1'].idxmax()]
    fastest_model = df_basic_enhanced.loc[df_basic_enhanced['å¹³å‡å»¶è¿Ÿ(æ¯«ç§’)'].idxmin()]
    print(f"   ğŸ“ˆ æœ€ä½³F1åˆ†æ•°: {best_f1_model['æ¨¡å‹æè¿°']} (F1={best_f1_model['Micro_F1']:.3f})")
    print(f"   âš¡ æœ€å¿«å¤„ç†é€Ÿåº¦: {fastest_model['æ¨¡å‹æè¿°']} ({fastest_model['å¹³å‡å»¶è¿Ÿ(æ¯«ç§’)']:.1f}ms)")
    print(f"   ğŸ¯ ä¸“ä¸šåˆçº¦ä»£ç è¯†åˆ«èƒ½åŠ›å¾—åˆ°é‡ç‚¹æµ‹è¯•å’Œåˆ†æ")
print(f"\nğŸ’» å»ºè®®æ ¹æ®å…·ä½“ä¸šåŠ¡åœºæ™¯é€‰æ‹©åˆé€‚çš„æ¨¡å‹é…ç½®è¿›è¡Œéƒ¨ç½²!")
, entity_text):
            new_label = "CONTRACT_CODE"
            
        # â­ äº§å“/å“ç§è¯†åˆ«
        elif any(keyword in entity_text for keyword in ["æœŸè´§", "åˆçº¦"]) and label not in ["FUTURES_COMPANY", "EXCHANGE"]:
            new_label = "PRODUCT"
            
        # ä½¿ç”¨æ˜ å°„è¡¨è¿›è¡Œæœ€ç»ˆè½¬æ¢
        final_label = label_mapping.get(new_label, new_label)
        processed_entities.append((start, end, final_label))
    
    return processed_entities

# â­ æ–°å¢ï¼šé»„é‡‘æ ‡å‡†å®ä½“æ ‡æ³¨å‡½æ•°  
def create_gold_standard_annotations():
    """åˆ›å»ºé»„é‡‘æ ‡å‡†å®ä½“æ ‡æ³¨ï¼Œç”¨äºç²¾ç¡®è®¡ç®—æ€§èƒ½æŒ‡æ ‡"""
    
    gold_annotations = {
        # åŸºç¡€æµ‹è¯•æ ·æœ¬çš„é»„é‡‘æ ‡å‡†
        "è‹¹æœæœŸè´§åœ¨éƒ‘å·å•†å“äº¤æ˜“æ‰€äº¤æ˜“ï¼Œåæ³°æœŸè´§å…¬å¸å‚ä¸å…¶ä¸­": [
            (0, 2, "PRODUCT"),           # è‹¹æœ
            (5, 12, "EXCHANGE"),         # éƒ‘å·å•†å“äº¤æ˜“æ‰€
            (15, 22, "FUTURES_COMPANY")  # åæ³°æœŸè´§å…¬å¸
        ],
        
        "ä¸Šæµ·æœŸè´§äº¤æ˜“æ‰€çš„é“œæœŸè´§ä»·æ ¼ä¸Šæ¶¨ï¼Œä¸­ä¿¡æœŸè´§å‘å¸ƒç ”ç©¶æŠ¥å‘Š": [
            (0, 7, "EXCHANGE"),          # ä¸Šæµ·æœŸè´§äº¤æ˜“æ‰€
            (8, 10, "PRODUCT"),          # é“œæœŸè´§
            (17, 21, "FUTURES_COMPANY")  # ä¸­ä¿¡æœŸè´§
        ],
        
        # â­ ä¸“ä¸šæŒ‘æˆ˜æ€§æµ‹è¯•çš„é»„é‡‘æ ‡å‡†
        "éƒ‘å•†æ‰€AP2502æœŸè´§æ€ä¹ˆæ ·äº†": [
            (0, 3, "EXCHANGE"),          # éƒ‘å•†æ‰€
            (3, 9, "CONTRACT_CODE"),     # AP2502
            (9, 11, "PRODUCT")           # æœŸè´§
        ],
        
        "ä¸ŠæœŸæ‰€CU2503åˆçº¦ä»Šæ—¥æ¶¨åœï¼Œåæ³°æœŸè´§å»ºè®®å…³æ³¨": [
            (0, 3, "EXCHANGE"),          # ä¸ŠæœŸæ‰€
            (3, 9, "CONTRACT_CODE"),     # CU2503
            (9, 11, "PRODUCT"),          # åˆçº¦
            (16, 20, "FUTURES_COMPANY")  # åæ³°æœŸè´§
        ],
        
        "å¤§å•†æ‰€M2505è±†ç²•æœŸè´§ä¸»åŠ›åˆçº¦æˆäº¤æ´»è·ƒ": [
            (0, 3, "EXCHANGE"),          # å¤§å•†æ‰€
            (3, 8, "CONTRACT_CODE"),     # M2505
            (8, 10, "PRODUCT"),          # è±†ç²•
            (10, 12, "PRODUCT")          # æœŸè´§
        ],
        
        "æ–¹æ­£ä¸­æœŸæœŸè´§é£é™©ç®¡ç†å­å…¬å¸åœ¨PTA2504åˆçº¦ä¸Šå»ºç«‹ç©ºå¤´å¥—ä¿å¤´å¯¸": [
            (0, 6, "FUTURES_COMPANY"),   # æ–¹æ­£ä¸­æœŸæœŸè´§
            (6, 13, "ORGANIZATION"),     # é£é™©ç®¡ç†å­å…¬å¸
            (15, 22, "CONTRACT_CODE"),   # PTA2504
            (22, 24, "PRODUCT")          # åˆçº¦
        ],
        
        "åæ³°æœŸè´§ç ”ç©¶æ‰€åˆ†æå¸ˆè®¤ä¸ºZC2505åŠ¨åŠ›ç…¤æœŸè´§ä»·æ ¼å°†éœ‡è¡ä¸Šè¡Œ": [
            (0, 7, "FUTURES_COMPANY"),   # åæ³°æœŸè´§ç ”ç©¶æ‰€
            (13, 19, "CONTRACT_CODE"),   # ZC2505
            (19, 22, "PRODUCT"),         # åŠ¨åŠ›ç…¤
            (22, 24, "PRODUCT")          # æœŸè´§
        ]
    }
    
    return gold_annotations

# â­ æ–°å¢ï¼šæ€§èƒ½æŒ‡æ ‡è®¡ç®—ç±»
class DetailedPerformanceMetrics:
    """è¯¦ç»†æ€§èƒ½æŒ‡æ ‡è®¡ç®—ç±»"""
    
    def __init__(self):
        self.true_entities = []
        self.pred_entities = []
        self.entity_type_performance = defaultdict(lambda: {"tp": 0, "fp": 0, "fn": 0})
        self.failure_cases = []
        
    def add_example(self, text, true_entities, pred_entities):
        """æ·»åŠ ä¸€ä¸ªæµ‹è¯•æ ·ä¾‹çš„ç»“æœ"""
        self.true_entities.extend([(text, ent) for ent in true_entities])
        self.pred_entities.extend([(text, ent) for ent in pred_entities])
        
        # è®¡ç®—å®ä½“çº§åˆ«çš„TP, FP, FN
        true_set = set(true_entities)
        pred_set = set(pred_entities)
        
        # æŒ‰ç±»å‹ç»Ÿè®¡
        for ent in true_set:
            if len(ent) >= 3:
                entity_type = ent[2]
                if ent in pred_set:
                    self.entity_type_performance[entity_type]["tp"] += 1
                else:
                    self.entity_type_performance[entity_type]["fn"] += 1
                    self.failure_cases.append({
                        "text": text,
                        "type": "FN",
                        "entity": ent,
                        "entity_type": entity_type
                    })
        
        for ent in pred_set:
            if len(ent) >= 3:
                entity_type = ent[2]
                if ent not in true_set:
                    self.entity_type_performance[entity_type]["fp"] += 1
                    self.failure_cases.append({
                        "text": text,
                        "type": "FP", 
                        "entity": ent,
                        "entity_type": entity_type
                    })
    
    def calculate_metrics(self):
        """è®¡ç®—è¯¦ç»†çš„æ€§èƒ½æŒ‡æ ‡"""
        results = {}
        
        for entity_type, counts in self.entity_type_performance.items():
            tp = counts["tp"]
            fp = counts["fp"] 
            fn = counts["fn"]
            
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
            
            results[entity_type] = {
                "precision": precision,
                "recall": recall,
                "f1": f1,
                "tp": tp,
                "fp": fp,
                "fn": fn,
                "support": tp + fn
            }
        
        # è®¡ç®—å®å¹³å‡å’Œå¾®å¹³å‡
        total_tp = sum(counts["tp"] for counts in self.entity_type_performance.values())
        total_fp = sum(counts["fp"] for counts in self.entity_type_performance.values())
        total_fn = sum(counts["fn"] for counts in self.entity_type_performance.values())
        
        micro_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0
        micro_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0
        micro_f1 = 2 * micro_precision * micro_recall / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0
        
        precisions = [r["precision"] for r in results.values() if r["support"] > 0]
        recalls = [r["recall"] for r in results.values() if r["support"] > 0]
        f1s = [r["f1"] for r in results.values() if r["support"] > 0]
        
        macro_precision = np.mean(precisions) if precisions else 0
        macro_recall = np.mean(recalls) if recalls else 0
        macro_f1 = np.mean(f1s) if f1s else 0
        
        results["micro_avg"] = {
            "precision": micro_precision,
            "recall": micro_recall,
            "f1": micro_f1,
            "support": total_tp + total_fn
        }
        
        results["macro_avg"] = {
            "precision": macro_precision,
            "recall": macro_recall,
            "f1": macro_f1,
            "support": total_tp + total_fn
        }
        
        return results
    
    def analyze_failure_modes(self):
        """åˆ†æå¤±è´¥æ¨¡å¼"""
        failure_analysis = {
            "by_type": defaultdict(lambda: {"FP": 0, "FN": 0}),
            "by_text_length": defaultdict(lambda: {"FP": 0, "FN": 0}),
            "common_errors": [],
            "detailed_cases": self.failure_cases
        }
        
        for case in self.failure_cases:
            entity_type = case["entity_type"]
            error_type = case["type"]
            text_length = len(case["text"])
            
            failure_analysis["by_type"][entity_type][error_type] += 1
            
            if text_length < 20:
                length_category = "short"
            elif text_length < 50:
                length_category = "medium"
            else:
                length_category = "long"
            
            failure_analysis["by_text_length"][length_category][error_type] += 1
        
        return failure_analysis

# åˆ›å»ºå¢å¼ºæµ‹è¯•è¯­æ–™
print("ğŸ”„ åˆ›å»ºå¢å¼ºæµ‹è¯•è¯­æ–™åº“...")
enhanced_test_texts, enhanced_categorized_tests = create_enhanced_professional_test_corpus()
gold_standard = create_gold_standard_annotations()

print(f"ğŸ“Š å¢å¼ºæµ‹è¯•è¯­æ–™åº“ç»Ÿè®¡")
print("=" * 60)
print(f"æ€»æ–‡æœ¬æ•°é‡: {len(enhanced_test_texts)}")
print(f"æµ‹è¯•ç±»åˆ«: {len(enhanced_categorized_tests)}")
print(f"é»„é‡‘æ ‡å‡†æ ·æœ¬: {len(gold_standard)}")
print("\nå„ç±»åˆ«æ–‡æœ¬æ•°é‡:")
for category, texts in enhanced_categorized_tests.items():
    print(f"  {category}: {len(texts)} æ¡")

print(f"\nğŸ“ æ–°å¢ä¸“ä¸šæŒ‘æˆ˜æ€§æµ‹è¯•ç¤ºä¾‹:")
print("-" * 40)
for i, text in enumerate(enhanced_categorized_tests["ä¸“ä¸šæŒ‘æˆ˜æ€§æµ‹è¯•"][:3]):
    print(f"{i+1}. {text}")

# %% [markdown]
# ## 3. å¢å¼ºåŸºå‡†æµ‹è¯•æ‰§è¡Œ

# %%
def enhanced_benchmark_with_detailed_metrics():
    """å¢å¼ºçš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«è¯¦ç»†æ€§èƒ½æŒ‡æ ‡"""
    
    configurations = {
        "sm_full": {
            "model": "zh_core_web_sm",
            "exclude": [],
            "description": "å°å‹æ¨¡å‹å®Œæ•´é…ç½® (46MB)"
        },
        "sm_ner_only": {
            "model": "zh_core_web_sm", 
            "exclude": ["parser", "tagger", "lemmatizer", "attribute_ruler"],
            "description": "å°å‹æ¨¡å‹ä»…NER (46MB)"
        },
        "md_full": {
            "model": "zh_core_web_md",
            "exclude": [],
            "description": "ä¸­å‹æ¨¡å‹å®Œæ•´é…ç½® (74MB)"
        },
        "md_ner_only": {
            "model": "zh_core_web_md", 
            "exclude": ["parser", "tagger", "lemmatizer", "attribute_ruler"],
            "description": "ä¸­å‹æ¨¡å‹ä»…NER (74MB)"
        },
        "trf_full": {
            "model": "zh_core_web_trf",
            "exclude": [],
            "description": "Transformeræ¨¡å‹å®Œæ•´é…ç½® (396MB)"
        },
        "trf_ner_only": {
            "model": "zh_core_web_trf",
            "exclude": ["parser", "tagger", "lemmatizer", "attribute_ruler"],
            "description": "Transformeræ¨¡å‹ä»…NER (396MB)"
        }
    }
    
    results = {}
    
    print("ğŸš€ å¼€å§‹å¢å¼ºç‰ˆNERæ¨¡å‹è´¨é‡åŸºå‡†æµ‹è¯•")
    print("=" * 80)
    print(f"ğŸ“Š æµ‹è¯•è¯­æ–™: {len(enhanced_test_texts)} ä¸ªæ–‡æœ¬æ ·æœ¬")
    print(f"ğŸ¯ æµ‹è¯•é…ç½®: {len(configurations)} ä¸ª")
    print(f"ğŸ† å…³æ³¨æŒ‡æ ‡: ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°ã€å¤±è´¥æ¨¡å¼åˆ†æ")
    print("=" * 80)
    
    for config_name, config in configurations.items():
        print(f"\nğŸ“Š æµ‹è¯•é…ç½®: {config['description']}")
        print("-" * 60)
        
        try:
            # åŠ è½½æ¨¡å‹
            print(f"â³ æ­£åœ¨åŠ è½½æ¨¡å‹...")
            start_load = time.time()
            nlp = spacy.load(config["model"], exclude=config["exclude"])
            load_time = time.time() - start_load
            
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {load_time:.2f}ç§’")
            
            # é¢„çƒ­æ¨¡å‹
            print(f"ğŸ”¥ é¢„çƒ­æ¨¡å‹...")
            for sample in enhanced_test_texts[:3]:
                _ = nlp(sample)
            
            # â­ è´¨é‡åˆ†ææµ‹è¯• - ä¸“æ³¨äºè¯†åˆ«å‡†ç¡®æ€§
            print(f"âš¡ å¼€å§‹è´¨é‡åˆ†ææµ‹è¯•...")
            
            # åˆå§‹åŒ–æ€§èƒ½æŒ‡æ ‡è®¡ç®—å™¨
            performance_metrics = DetailedPerformanceMetrics()
            
            all_entities = []
            entity_type_counts = {}
            category_performance = {}
            
            # æŒ‰ç±»åˆ«æµ‹è¯•
            for category, texts in enhanced_categorized_tests.items():
                category_metrics = DetailedPerformanceMetrics()
                
                for text in texts:
                    doc = nlp(text)
                    
                    # æå–é¢„æµ‹å®ä½“
                    pred_entities = [(ent.start_char, ent.end_char, ent.label_) 
                                   for ent in doc.ents]
                    
                    # è·å–é»„é‡‘æ ‡å‡†ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    true_entities = gold_standard.get(text, [])
                    
                    # ç»Ÿè®¡å®ä½“ç±»å‹
                    for _, _, label in pred_entities:
                        entity_type_counts[label] = entity_type_counts.get(label, 0) + 1
                    
                    # æ·»åŠ åˆ°æ€§èƒ½æŒ‡æ ‡è®¡ç®—
                    if true_entities:  # åªå¯¹æœ‰æ ‡æ³¨çš„æ ·æœ¬è®¡ç®—è¯¦ç»†æŒ‡æ ‡
                        performance_metrics.add_example(text, true_entities, pred_entities)
                        category_metrics.add_example(text, true_entities, pred_entities)
                    
                    all_entities.append({
                        "text_id": len(all_entities),
                        "text": text[:100] + "..." if len(text) > 100 else text,
                        "category": category,
                        "true_entities": true_entities,
                        "pred_entities": pred_entities,
                        "entity_count": len(pred_entities),
                        "text_length": len(text)
                    })
                
                # è®¡ç®—åˆ†ç±»åˆ«æ€§èƒ½
                if category in ["ä¸“ä¸šæŒ‘æˆ˜æ€§æµ‹è¯•", "RegExå¤±è´¥æ¡ˆä¾‹"]:  # é‡ç‚¹å…³æ³¨çš„ç±»åˆ«
                    category_performance[category] = category_metrics.calculate_metrics()
            
            # â­ è®¡ç®—è¯¦ç»†æ€§èƒ½æŒ‡æ ‡
            detailed_metrics = performance_metrics.calculate_metrics()
            failure_analysis = performance_metrics.analyze_failure_modes()
            
            # è®¡ç®—åŸºæœ¬ç»Ÿè®¡æŒ‡æ ‡
            total_entities = sum(item["entity_count"] for item in all_entities)
            
            results[config_name] = {
                "model": config["model"],
                "description": config["description"],
                "excluded_components": config["exclude"],
                "active_pipes": nlp.pipe_names,
                
                # åŸºæœ¬è¯†åˆ«ç»Ÿè®¡
                "load_time": load_time,
                "total_entities_found": total_entities,
                "avg_entities_per_text": total_entities / len(enhanced_test_texts),
                "entity_type_counts": entity_type_counts,
                
                # â­ æ ¸å¿ƒè´¨é‡æŒ‡æ ‡
                "detailed_metrics": detailed_metrics,
                "failure_analysis": failure_analysis,
                "category_performance": category_performance,
                
                # æ ·æœ¬ç»“æœï¼ˆé™åˆ¶æ•°é‡ï¼‰
                "detailed_results": all_entities[:5]
            }
            
            print(f"âœ… æµ‹è¯•å®Œæˆ!")
            print(f"ğŸ“Š å¤„ç†æ–‡æœ¬æ€»æ•°: {len(enhanced_test_texts)} ä¸ª")
            print(f"ğŸ¯ å‘ç°å®ä½“æ€»æ•°: {total_entities}")
            
            # â­ æ˜¾ç¤ºè¯¦ç»†è´¨é‡æŒ‡æ ‡
            if detailed_metrics:
                micro_f1 = detailed_metrics.get("micro_avg", {}).get("f1", 0)
                macro_f1 = detailed_metrics.get("macro_avg", {}).get("f1", 0)
                micro_precision = detailed_metrics.get("micro_avg", {}).get("precision", 0)
                micro_recall = detailed_metrics.get("micro_avg", {}).get("recall", 0)
                print(f"ğŸ“Š Micro Precision: {micro_precision:.3f}")
                print(f"ğŸ“Š Micro Recall: {micro_recall:.3f}")
                print(f"ğŸ“Š Micro F1-Score: {micro_f1:.3f}")
                print(f"ğŸ“Š Macro F1-Score: {macro_f1:.3f}")
                
                # æ˜¾ç¤ºä¸»è¦å®ä½“ç±»å‹çš„æ€§èƒ½
                main_types = ["EXCHANGE", "FUTURES_COMPANY", "PRODUCT", "CONTRACT_CODE"]
                for entity_type in main_types:
                    if entity_type in detailed_metrics:
                        metrics = detailed_metrics[entity_type]
                        print(f"  {entity_type}: P={metrics['precision']:.3f}, R={metrics['recall']:.3f}, F1={metrics['f1']:.3f}")
            
        except OSError as e:
            print(f"âŒ æ— æ³•åŠ è½½æ¨¡å‹ {config['model']}: {e}")
            results[config_name] = None
        except Exception as e:
            print(f"âŒ æµ‹è¯•è¿‡ç¨‹å‡ºé”™: {e}")
            results[config_name] = None
    
    return results

# æ‰§è¡Œå¢å¼ºåŸºå‡†æµ‹è¯•
print("ğŸ”„ å¼€å§‹æ‰§è¡Œå¢å¼ºåŸºå‡†æµ‹è¯•...")
print("âš ï¸  æ³¨æ„: æµ‹è¯•å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
enhanced_benchmark_results = enhanced_benchmark_with_detailed_metrics()

# %% [markdown]
# ## 4. å¢å¼ºç»“æœåˆ†æå’Œå¯è§†åŒ–

# %%
def analyze_enhanced_results(results):
    """åˆ†æå¢å¼ºçš„æµ‹è¯•ç»“æœ"""
    
    print(f"\nğŸ“ˆ å¢å¼ºæµ‹è¯•ç»“æœåˆ†æ")
    print("=" * 80)
    
    valid_results = {k: v for k, v in results.items() if v is not None}
    
    if not valid_results:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æµ‹è¯•ç»“æœ")
        return None, None
    
    print(f"âœ… æˆåŠŸæµ‹è¯•é…ç½®: {len(valid_results)}/{len(results)}")
    
    # â­ åˆ›å»ºè´¨é‡å¯¼å‘çš„æ€§èƒ½å¯¹æ¯”DataFrame
    comparison_data = []
    detailed_metrics_data = []
    
    for config_name, result in valid_results.items():
        # æ¨¡å‹åŸºæœ¬ä¿¡æ¯
        size_map = {"sm": "46MB", "md": "74MB", "lg": "575MB", "trf": "396MB"}
        model_size = next((size for key, size in size_map.items() if key in config_name), "æœªçŸ¥")
        
        # â­ è·å–è¯¦ç»†æ€§èƒ½æŒ‡æ ‡
        detailed_metrics = result.get("detailed_metrics", {})
        micro_avg = detailed_metrics.get("micro_avg", {})
        macro_avg = detailed_metrics.get("macro_avg", {})
        
        basic_data = {
            "é…ç½®åç§°": config_name,
            "æ¨¡å‹æè¿°": result["description"],
            "æ¨¡å‹å¤§å°": model_size,
            "åŠ è½½æ—¶é—´(ç§’)": result['load_time'],
            "å‘ç°å®ä½“æ•°": result['total_entities_found'],
            "å¹³å‡å®ä½“æ•°": result['avg_entities_per_text'],
            # â­ æ ¸å¿ƒè´¨é‡æŒ‡æ ‡
            "Micro_Precision": micro_avg.get("precision", 0),
            "Micro_Recall": micro_avg.get("recall", 0),
            "Micro_F1": micro_avg.get("f1", 0),
            "Macro_Precision": macro_avg.get("precision", 0),
            "Macro_Recall": macro_avg.get("recall", 0),
            "Macro_F1": macro_avg.get("f1", 0),
            "è´¨é‡è¯„åˆ†": (micro_avg.get("precision", 0) + micro_avg.get("recall", 0) + micro_avg.get("f1", 0)) * 100 / 3
        }
        comparison_data.append(basic_data)
        
        # â­ å®ä½“ç±»å‹è¯¦ç»†æŒ‡æ ‡
        for entity_type, metrics in detailed_metrics.items():
            if entity_type not in ["micro_avg", "macro_avg"] and isinstance(metrics, dict):
                detailed_metrics_data.append({
                    "é…ç½®åç§°": config_name,
                    "å®ä½“ç±»å‹": entity_type,
                    "ç²¾ç¡®ç‡": metrics.get("precision", 0),
                    "å¬å›ç‡": metrics.get("recall", 0),
                    "F1åˆ†æ•°": metrics.get("f1", 0),
                    "æ”¯æŒåº¦": metrics.get("support", 0),
                    "True_Positive": metrics.get("tp", 0),
                    "False_Positive": metrics.get("fp", 0),
                    "False_Negative": metrics.get("fn", 0)
                })
    
    df_basic = pd.DataFrame(comparison_data)
    df_detailed = pd.DataFrame(detailed_metrics_data)
    
    # æ˜¾ç¤ºåŸºæœ¬æ€§èƒ½å¯¹æ¯”
    print(f"\nğŸ“Š åŸºæœ¬æ€§èƒ½å¯¹æ¯”è¡¨")
    print("-" * 80)
    display(df_basic.round(3))
    
    # æ˜¾ç¤ºè¯¦ç»†æ€§èƒ½æŒ‡æ ‡
    if not df_detailed.empty:
        print(f"\nğŸ“Š å®ä½“ç±»å‹è¯¦ç»†æ€§èƒ½æŒ‡æ ‡")
        print("-" * 80)
        # åªæ˜¾ç¤ºä¸»è¦å®ä½“ç±»å‹
        main_types = ["EXCHANGE", "FUTURES_COMPANY", "PRODUCT", "CONTRACT_CODE", "ORG"]
        df_main = df_detailed[df_detailed["å®ä½“ç±»å‹"].isin(main_types)]
        if not df_main.empty:
            display(df_main.round(3))
    
    return df_basic, df_detailed

# â­ æ–°å¢ï¼šå¤±è´¥æ¨¡å¼è¯¦ç»†åˆ†æå‡½æ•°
def analyze_failure_modes_detailed(results):
    """è¯¦ç»†åˆ†æå¤±è´¥æ¨¡å¼"""
    
    print(f"\nğŸ” å¤±è´¥æ¨¡å¼è¯¦ç»†åˆ†æ")
    print("=" * 80)
    
    valid_results = {k: v for k, v in results.items() if v is not None}
    
    for config_name, result in valid_results.items():
        failure_analysis = result.get("failure_analysis", {})
        
        if not failure_analysis:
            continue
            
        print(f"\nğŸ“‹ æ¨¡å‹: {result['description']}")
        print("-" * 50)
        
        # æŒ‰å®ä½“ç±»å‹çš„å¤±è´¥åˆ†æ
        by_type = failure_analysis.get("by_type", {})
        if by_type:
            print("ğŸ¯ æŒ‰å®ä½“ç±»å‹çš„é”™è¯¯åˆ†å¸ƒ:")
            for entity_type, errors in by_type.items():
                total_errors = errors["FP"] + errors["FN"]
                if total_errors > 0:
                    print(f"  {entity_type}: FP={errors['FP']}, FN={errors['FN']}, æ€»é”™è¯¯={total_errors}")
        
        # æŒ‰æ–‡æœ¬é•¿åº¦çš„å¤±è´¥åˆ†æ
        by_length = failure_analysis.get("by_text_length", {})
        if by_length:
            print("\nğŸ“ æŒ‰æ–‡æœ¬é•¿åº¦çš„é”™è¯¯åˆ†å¸ƒ:")
            for length_cat, errors in by_length.items():
                total_errors = errors["FP"] + errors["FN"]
                if total_errors > 0:
                    print(f"  {length_cat}: FP={errors['FP']}, FN={errors['FN']}, æ€»é”™è¯¯={total_errors}")
        
        # æ˜¾ç¤ºä¸€äº›å…·ä½“å¤±è´¥æ¡ˆä¾‹
        detailed_cases = failure_analysis.get("detailed_cases", [])
        if detailed_cases:
            print(f"\nâŒ å…¸å‹å¤±è´¥æ¡ˆä¾‹ (æ˜¾ç¤ºå‰3ä¸ª):")
            for i, case in enumerate(detailed_cases[:3]):
                print(f"  {i+1}. ç±»å‹: {case['type']}, å®ä½“ç±»å‹: {case['entity_type']}")
                print(f"     æ–‡æœ¬: {case['text'][:80]}...")
                print(f"     å®ä½“: {case['entity']}")

# â­ æ–°å¢ï¼šä¸“ä¸šæµ‹è¯•è¯­æ–™æ€§èƒ½åˆ†æ
def analyze_professional_test_performance(results):
    """åˆ†æä¸“ä¸šæµ‹è¯•è¯­æ–™çš„æ€§èƒ½"""
    
    print(f"\nğŸ“ ä¸“ä¸šæµ‹è¯•è¯­æ–™æ€§èƒ½åˆ†æ")
    print("=" * 80)
    
    valid_results = {k: v for k, v in results.items() if v is not None}
    
    # åˆ†æå„æ¨¡å‹åœ¨ä¸“ä¸šæµ‹è¯•ä¸Šçš„è¡¨ç°
    professional_performance = {}
    
    for config_name, result in valid_results.items():
        category_performance = result.get("category_performance", {})
        
        # é‡ç‚¹å…³æ³¨ä¸“ä¸šæŒ‘æˆ˜æ€§æµ‹è¯•å’ŒRegExå¤±è´¥æ¡ˆä¾‹
        professional_cats = ["ä¸“ä¸šæŒ‘æˆ˜æ€§æµ‹è¯•", "RegExå¤±è´¥æ¡ˆä¾‹"]
        
        for cat in professional_cats:
            if cat in category_performance:
                metrics = category_performance[cat]
                micro_avg = metrics.get("micro_avg", {})
                
                if config_name not in professional_performance:
                    professional_performance[config_name] = {}
                
                professional_performance[config_name][cat] = {
                    "precision": micro_avg.get("precision", 0),
                    "recall": micro_avg.get("recall", 0),
                    "f1": micro_avg.get("f1", 0)
                }
    
    # åˆ›å»ºä¸“ä¸šæµ‹è¯•æ€§èƒ½å¯¹æ¯”è¡¨
    if professional_performance:
        prof_data = []
        for config_name, categories in professional_performance.items():
            for category, metrics in categories.items():
                prof_data.append({
                    "æ¨¡å‹é…ç½®": config_name,
                    "æµ‹è¯•ç±»åˆ«": category,
                    "ç²¾ç¡®ç‡": metrics["precision"],
                    "å¬å›ç‡": metrics["recall"],
                    "F1åˆ†æ•°": metrics["f1"]
                })
        
        if prof_data:
            df_prof = pd.DataFrame(prof_data)
            print("ğŸ“Š ä¸“ä¸šæµ‹è¯•è¯­æ–™æ€§èƒ½å¯¹æ¯”:")
            display(df_prof.round(3))
            
            # åˆ†æå“ªä¸ªæ¨¡å‹åœ¨ä¸“ä¸šæµ‹è¯•ä¸Šè¡¨ç°æœ€å¥½
            pivot_f1 = df_prof.pivot(index="æ¨¡å‹é…ç½®", columns="æµ‹è¯•ç±»åˆ«", values="F1åˆ†æ•°")
            if not pivot_f1.empty:
                print(f"\nğŸ† ä¸“ä¸šæµ‹è¯•F1åˆ†æ•°æ’å:")
                for category in pivot_f1.columns:
                    best_model = pivot_f1[category].idxmax()
                    best_score = pivot_f1[category].max()
                    print(f"  {category}: {best_model} (F1={best_score:.3f})")

# æ‰§è¡Œå¢å¼ºåˆ†æ
df_basic_enhanced, df_detailed_enhanced = analyze_enhanced_results(enhanced_benchmark_results)
analyze_failure_modes_detailed(enhanced_benchmark_results)
analyze_professional_test_performance(enhanced_benchmark_results)

# %% [markdown]
# ## 5. å¢å¼ºå¯è§†åŒ–å›¾è¡¨

# %%
def create_quality_focused_visualizations(df_basic, df_detailed, results):
    """åˆ›å»ºä¸“æ³¨äºè´¨é‡çš„å¯è§†åŒ–å›¾è¡¨"""
    
    if df_basic.empty:
        print("âŒ æ²¡æœ‰æ•°æ®å¯ç”¨äºå¯è§†åŒ–")
        return
    
    # è®¾ç½®å›¾è¡¨å¸ƒå±€ - ä¸“æ³¨äºè´¨é‡æŒ‡æ ‡
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('NER Model Quality Analysis (Accuracy-Focused)', fontsize=18, fontweight='bold')
    
    # 1. â­ Micro F1-Score å¯¹æ¯”
    ax1 = axes[0, 0]
    bars1 = ax1.bar(range(len(df_basic)), df_basic['Micro_F1'], color='lightgreen', alpha=0.7)
    ax1.set_title('Micro F1-Score Comparison', fontweight='bold')
    ax1.set_ylabel('F1-Score')
    ax1.set_xticks(range(len(df_basic)))
    ax1.set_xticklabels(df_basic['é…ç½®åç§°'], rotation=45, ha='right')
    ax1.set_ylim(0, 1)
    
    for i, bar in enumerate(bars1):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{height:.3f}', ha='center', va='bottom', fontsize=8)
    
    # 2. â­ Macro F1-Score å¯¹æ¯”
    ax2 = axes[0, 1]
    bars2 = ax2.bar(range(len(df_basic)), df_basic['Macro_F1'], color='orange', alpha=0.7)
    ax2.set_title('Macro F1-Score Comparison', fontweight='bold')
    ax2.set_ylabel('F1-Score')
    ax2.set_xticks(range(len(df_basic)))
    ax2.set_xticklabels(df_basic['é…ç½®åç§°'], rotation=45, ha='right')
    ax2.set_ylim(0, 1)
    
    for i, bar in enumerate(bars2):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{height:.3f}', ha='center', va='bottom', fontsize=8)
    
    # 3. â­ ç²¾ç¡®ç‡ vs å¬å›ç‡æ•£ç‚¹å›¾
    ax3 = axes[0, 2]
    scatter = ax3.scatter(df_basic['Micro_Precision'], df_basic['Micro_Recall'], 
                         c=df_basic['Micro_F1'], cmap='viridis', 
                         s=150, alpha=0.7, edgecolors='black')
    ax3.set_xlabel('Micro Precision')
    ax3.set_ylabel('Micro Recall')
    ax3.set_title('Precision vs Recall (colored by F1-Score)', fontweight='bold')
    ax3.set_xlim(0, 1)
    ax3.set_ylim(0, 1)
    
    # æ·»åŠ å¯¹è§’çº¿
    ax3.plot([0, 1], [0, 1], 'k--', alpha=0.3)
    
    for i, txt in enumerate(df_basic['é…ç½®åç§°']):
        ax3.annotate(txt, (df_basic['Micro_Precision'].iloc[i], df_basic['Micro_Recall'].iloc[i]), 
                    xytext=(5, 5), textcoords='offset points', fontsize=8)
    
    plt.colorbar(scatter, ax=ax3, label='F1-Score')
    
    # 4. â­ å®ä½“ç±»å‹æ€§èƒ½çƒ­åŠ›å›¾
    ax4 = axes[1, 0]
    if not df_detailed.empty:
        # åˆ›å»ºé€è§†è¡¨
        heatmap_data = df_detailed.pivot_table(
            index='å®ä½“ç±»å‹', 
            columns='é…ç½®åç§°', 
            values='F1åˆ†æ•°', 
            aggfunc='mean'
        ).fillna(0)
        
        if not heatmap_data.empty:
            sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='YlOrRd', 
                       ax=ax4, cbar_kws={'label': 'F1-Score'})
            ax4.set_title('F1-Score by Entity Type', fontweight='bold')
            ax4.set_xlabel('Model Configuration')
            ax4.set_ylabel('Entity Type')
    
    # 5. â­ è´¨é‡æŒ‡æ ‡é›·è¾¾å›¾
    ax5 = axes[1, 1]
    
    # é€‰æ‹©è´¨é‡æŒ‡æ ‡è¿›è¡Œé›·è¾¾å›¾å±•ç¤º
    metrics = ['Micro_F1', 'Macro_F1', 'Micro_Precision', 'Micro_Recall']
    metric_labels = ['Micro F1', 'Macro F1', 'Precision', 'Recall']
    
    angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
    angles += angles[:1]  # å®Œæˆåœ†å½¢
    
    ax5 = plt.subplot(2, 3, 5, projection='polar')
    
    colors = plt.cm.Set2(np.linspace(0, 1, len(df_basic)))
    
    for i, (_, row) in enumerate(df_basic.iterrows()):
        values = [row[metric] for metric in metrics]
        values += values[:1]
        
        ax5.plot(angles, values, 'o-', linewidth=2, 
                label=row['é…ç½®åç§°'], color=colors[i])
        ax5.fill(angles, values, alpha=0.1, color=colors[i])
    
    ax5.set_xticks(angles[:-1])
    ax5.set_xticklabels(metric_labels)
    ax5.set_ylim(0, 1)
    ax5.set_title('Quality Metrics Radar Chart', fontweight='bold', pad=20)
    ax5.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
    
    # 6. â­ å¤±è´¥æ¨¡å¼åˆ†æå›¾
    ax6 = axes[1, 2]
    
    # ç»Ÿè®¡å„æ¨¡å‹çš„å¤±è´¥æ¡ˆä¾‹æ•°é‡
    failure_counts = {}
    for config_name, result in results.items():
        if result is not None:
            failure_analysis = result.get("failure_analysis", {})
            detailed_cases = failure_analysis.get("detailed_cases", [])
            failure_counts[config_name] = len(detailed_cases)
    
    if failure_counts:
        configs = list(failure_counts.keys())
        counts = list(failure_counts.values())
        
        bars6 = ax6.bar(configs, counts, color='lightcoral', alpha=0.7)
        ax6.set_title('Failure Cases Count', fontweight='bold')
        ax6.set_ylabel('Number of Failures')
        ax6.tick_params(axis='x', rotation=45)
        
        for i, bar in enumerate(bars6):
            height = bar.get_height()
            ax6.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                    f'{int(height)}', ha='center', va='bottom', fontsize=8)
    
    plt.tight_layout()
    plt.show()
    
    # â­ æ˜¾ç¤ºè´¨é‡å¯¼å‘çš„Top 3 é…ç½®æ’å
    print(f"\nğŸ† Quality-Focused Performance Ranking")
    print("=" * 60)
    
    # æŒ‰è´¨é‡æŒ‡æ ‡æ’å
    rankings = {
        "Overall F1-Score": df_basic.nlargest(3, 'Micro_F1'),
        "Precision": df_basic.nlargest(3, 'Micro_Precision'),
        "Recall": df_basic.nlargest(3, 'Micro_Recall'),
        "Macro F1": df_basic.nlargest(3, 'Macro_F1')
    }
    
    medals = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰"]
    
    for ranking_type, top_3 in rankings.items():
        print(f"\nğŸ¯ {ranking_type} Top 3:")
        for i, (_, row) in enumerate(top_3.iterrows()):
            if ranking_type == "Overall F1-Score":
                score = f"F1={row['Micro_F1']:.3f}"
            elif ranking_type == "Precision":
                score = f"Precision={row['Micro_Precision']:.3f}"
            elif ranking_type == "Recall":
                score = f"Recall={row['Micro_Recall']:.3f}"
            else:  # Macro F1
                score = f"Macro F1={row['Macro_F1']:.3f}"
            
            print(f"  {medals[i]} {row['æ¨¡å‹æè¿°']}")
            print(f"     {score}")

# åˆ›å»ºè´¨é‡å¯¼å‘çš„å¯è§†åŒ–
if df_basic_enhanced is not None:
    create_quality_focused_visualizations(df_basic_enhanced, df_detailed_enhanced, enhanced_benchmark_results)

# %% [markdown]
# ## 6. å¢å¼ºä¼˜åŒ–å»ºè®®

# %%
def generate_quality_focused_recommendations(df_basic, df_detailed, results):
    """ç”Ÿæˆä¸“æ³¨äºè´¨é‡çš„ä¼˜åŒ–å»ºè®®"""
    
    if df_basic.empty:
        print("âŒ æ²¡æœ‰æ•°æ®å¯ç”¨äºç”Ÿæˆå»ºè®®")
        return
    
    print(f"\nğŸ’¡ è´¨é‡å¯¼å‘çš„æ¨¡å‹ä¼˜åŒ–å»ºè®®")
    print("=" * 80)
    
    # åˆ†ææœ€ä¼˜é…ç½®
    best_f1_config = df_basic.loc[df_basic['Micro_F1'].idxmax()]
    best_precision_config = df_basic.loc[df_basic['Micro_Precision'].idxmax()]
    best_recall_config = df_basic.loc[df_basic['Micro_Recall'].idxmax()]
    best_macro_f1_config = df_basic.loc[df_basic['Macro_F1'].idxmax()]
    best_overall_config = df_basic.loc[df_basic['è´¨é‡è¯„åˆ†'].idxmax()]
    
    print(f"ğŸ¯ **æœ€ä½³F1åˆ†æ•°é…ç½®**: {best_f1_config['æ¨¡å‹æè¿°']}")
    print(f"   ğŸ“Š Micro F1: {best_f1_config['Micro_F1']:.3f}")
    print(f"   ğŸ“ˆ Precision: {best_f1_config['Micro_Precision']:.3f}")
    print(f"   ğŸ“‰ Recall: {best_f1_config['Micro_Recall']:.3f}")
    print(f"   ğŸ¯ å‘ç°å®ä½“: {best_f1_config['å‘ç°å®ä½“æ•°']} ä¸ª")
    
    print(f"\nğŸ–ï¸ **æœ€ä½³ç²¾ç¡®ç‡é…ç½®**: {best_precision_config['æ¨¡å‹æè¿°']}")
    print(f"   ğŸ“ˆ Precision: {best_precision_config['Micro_Precision']:.3f}")
    print(f"   ğŸ“Š F1: {best_precision_config['Micro_F1']:.3f}")
    print(f"   ğŸ“‰ Recall: {best_precision_config['Micro_Recall']:.3f}")
    
    print(f"\nğŸ” **æœ€ä½³å¬å›ç‡é…ç½®**: {best_recall_config['æ¨¡å‹æè¿°']}")
    print(f"   ğŸ“‰ Recall: {best_recall_config['Micro_Recall']:.3f}")
    print(f"   ğŸ“Š F1: {best_recall_config['Micro_F1']:.3f}")
    print(f"   ğŸ“ˆ Precision: {best_recall_config['Micro_Precision']:.3f}")
    
    print(f"\nğŸŒŸ **æœ€ä½³å®å¹³å‡F1é…ç½®**: {best_macro_f1_config['æ¨¡å‹æè¿°']}")
    print(f"   ğŸ“Š Macro F1: {best_macro_f1_config['Macro_F1']:.3f}")
    print(f"   ğŸ“Š Micro F1: {best_macro_f1_config['Micro_F1']:.3f}")
    
    print(f"\nğŸ† **ç»¼åˆè´¨é‡æœ€ä¼˜é…ç½®**: {best_overall_config['æ¨¡å‹æè¿°']}")
    print(f"   ğŸ–ï¸ è´¨é‡è¯„åˆ†: {best_overall_config['è´¨é‡è¯„åˆ†']:.1f}")
    print(f"   ğŸ“Š Micro F1: {best_overall_config['Micro_F1']:.3f}")
    print(f"   ğŸ“ˆ Precision: {best_overall_config['Micro_Precision']:.3f}")
    print(f"   ğŸ“‰ Recall: {best_overall_config['Micro_Recall']:.3f}")
    
    # â­ ä¸“ä¸šåœºæ™¯è´¨é‡ä¼˜åŒ–å»ºè®®
    print(f"\nğŸ“‹ **ä¸“ä¸šæœŸè´§äº¤æ˜“åœºæ™¯è´¨é‡ä¼˜åŒ–å»ºè®®**:")
    print(f"=" * 60)
    
    scenarios = [
        {
            "scenario": "ğŸ¯ ç²¾ç¡®åˆçº¦è¯†åˆ« (å‡†ç¡®ç‡ä¼˜å…ˆ)",
            "recommendation": best_precision_config['é…ç½®åç§°'],
            "rationale": "åˆçº¦ä»£ç è¯†åˆ«éœ€è¦æé«˜çš„ç²¾ç¡®ç‡ï¼Œé¿å…è¯¯è¯†åˆ«",
            "config": f"spacy.load('{best_precision_config['é…ç½®åç§°'].split('_')[0]}_core_web_{best_precision_config['é…ç½®åç§°'].split('_')[1]}', exclude={['parser','tagger','lemmatizer','attribute_ruler'] if 'ner_only' in best_precision_config['é…ç½®åç§°'] else []})",
            "use_cases": ["åˆçº¦ä»£ç è‡ªåŠ¨è¯†åˆ«", "äº¤æ˜“æŒ‡ä»¤è§£æ", "é£é™©æ•å£è®¡ç®—"],
            "metrics": f"Precision={best_precision_config['Micro_Precision']:.3f}, F1={best_precision_config['Micro_F1']:.3f}"
        },
        {
            "scenario": "ğŸ” å…¨é¢ä¿¡æ¯æå– (å¬å›ç‡ä¼˜å…ˆ)",
            "recommendation": best_recall_config['é…ç½®åç§°'],
            "rationale": "éœ€è¦å°½å¯èƒ½å¤šåœ°è¯†åˆ«å‡ºæ‰€æœ‰ç›¸å…³å®ä½“ï¼Œé¿å…é—æ¼",
            "config": f"spacy.load('{best_recall_config['é…ç½®åç§°'].split('_')[0]}_core_web_{best_recall_config['é…ç½®åç§°'].split('_')[1]}', exclude={['parser','tagger','lemmatizer','attribute_ruler'] if 'ner_only' in best_recall_config['é…ç½®åç§°'] else []})",
            "use_cases": ["ç›‘ç®¡åˆè§„æ£€æŸ¥", "å…¨é‡æ•°æ®æŒ–æ˜", "å†å²æ–‡æ¡£åˆ†æ"],
            "metrics": f"Recall={best_recall_config['Micro_Recall']:.3f}, F1={best_recall_config['Micro_F1']:.3f}"
        },
        {
            "scenario": "âš–ï¸ å¹³è¡¡æ€§èƒ½åº”ç”¨ (F1åˆ†æ•°æœ€ä¼˜)",
            "recommendation": best_f1_config['é…ç½®åç§°'],
            "rationale": "åœ¨ç²¾ç¡®ç‡å’Œå¬å›ç‡ä¹‹é—´æ‰¾åˆ°æœ€ä½³å¹³è¡¡ç‚¹",
            "config": f"spacy.load('{best_f1_config['é…ç½®åç§°'].split('_')[0]}_core_web_{best_f1_config['é…ç½®åç§°'].split('_')[1]}', exclude={['parser','tagger','lemmatizer','attribute_ruler'] if 'ner_only' in best_f1_config['é…ç½®åç§°'] else []})",
            "use_cases": ["ç ”ç©¶æŠ¥å‘Šè§£æ", "å®¢æˆ·æŸ¥è¯¢å“åº”", "æ™ºèƒ½é—®ç­”ç³»ç»Ÿ"],
            "metrics": f"F1={best_f1_config['Micro_F1']:.3f}, P={best_f1_config['Micro_Precision']:.3f}, R={best_f1_config['Micro_Recall']:.3f}"
        },
        {
            "scenario": "ğŸŒˆ å¤šç±»åˆ«å‡è¡¡ (å®å¹³å‡F1ä¼˜å…ˆ)",
            "recommendation": best_macro_f1_config['é…ç½®åç§°'],
            "rationale": "ç¡®ä¿å„ç§å®ä½“ç±»å‹éƒ½æœ‰è¾ƒå¥½çš„è¯†åˆ«æ•ˆæœ",
            "config": f"spacy.load('{best_macro_f1_config['é…ç½®åç§°'].split('_')[0]}_core_web_{best_macro_f1_config['é…ç½®åç§°'].split('_')[1]}', exclude={['parser','tagger','lemmatizer','attribute_ruler'] if 'ner_only' in best_macro_f1_config['é…ç½®åç§°'] else []})",
            "use_cases": ["å¤šå…ƒåŒ–ä¿¡æ¯æå–", "è·¨ç±»åˆ«åˆ†æ", "å®Œæ•´æ€§æ£€æŸ¥"],
            "metrics": f"Macro F1={best_macro_f1_config['Macro_F1']:.3f}, Micro F1={best_macro_f1_config['Micro_F1']:.3f}"
        }
    ]
    
    for scenario in scenarios:
        print(f"\n{scenario['scenario']}")
        print(f"   ğŸ”§ æ¨èé…ç½®: {scenario['recommendation']}")
        print(f"   ğŸ’¡ é€‰æ‹©ç†ç”±: {scenario['rationale']}")
        print(f"   ğŸ“Š æ€§èƒ½æŒ‡æ ‡: {scenario['metrics']}")
        print(f"   ğŸ’» ä»£ç ç¤ºä¾‹: {scenario['config']}")
        print(f"   ğŸ“ é€‚ç”¨åœºæ™¯: {', '.join(scenario['use_cases'])}")
    
    # â­ å¤±è´¥æ¨¡å¼ç‰¹å®šä¼˜åŒ–å»ºè®®
    print(f"\nğŸ› ï¸ **é’ˆå¯¹å¤±è´¥æ¨¡å¼çš„è´¨é‡æå‡ç­–ç•¥**:")
    print(f"=" * 60)
    
    # åˆ†æä¸»è¦å¤±è´¥æ¨¡å¼
    main_failure_types = set()
    for config_name, result in results.items():
        if result is not None:
            failure_analysis = result.get("failure_analysis", {})
            by_type = failure_analysis.get("by_type", {})
            main_failure_types.update(by_type.keys())
    
    optimization_strategies = {
        "CONTRACT_CODE": [
            "âœ… ä½¿ç”¨è‡ªå®šä¹‰è§„åˆ™åŒ¹é…å¢å¼ºåˆçº¦ä»£ç è¯†åˆ«å‡†ç¡®æ€§",
            "âœ… æ„å»ºä¸“é—¨çš„åˆçº¦ä»£ç è¯å…¸è¿›è¡Œåå¤„ç†éªŒè¯",
            "âœ… è®­ç»ƒé’ˆå¯¹æœŸè´§åˆçº¦ä»£ç çš„ç‰¹åŒ–æ¨¡å‹",
            "âœ… ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼é¢„ç­›é€‰å€™é€‰å®ä½“ï¼Œæé«˜ç²¾ç¡®ç‡",
            "âœ… å»ºç«‹åˆçº¦ä»£ç æ ¼å¼éªŒè¯æœºåˆ¶"
        ],
        "EXCHANGE": [
            "âœ… å»ºç«‹äº¤æ˜“æ‰€åˆ«åæ˜ å°„è¡¨ï¼Œç»Ÿä¸€ä¸åŒè¡¨è¿°",
            "âœ… å¢åŠ ä¸­è‹±æ–‡æ··åˆè¡¨è¾¾çš„è®­ç»ƒæ ·æœ¬", 
            "âœ… ä½¿ç”¨åŸºäºè§„åˆ™çš„åå¤„ç†çº æ­£è¯†åˆ«é”™è¯¯",
            "âœ… è€ƒè™‘ä¸Šä¸‹æ–‡ä¿¡æ¯æé«˜æ­§ä¹‰æ¶ˆè§£èƒ½åŠ›",
            "âœ… ç»´æŠ¤äº¤æ˜“æ‰€å®˜æ–¹åç§°ä¸ç®€ç§°å¯¹ç…§è¡¨"
        ],
        "FUTURES_COMPANY": [
            "âœ… ç»´æŠ¤æœŸè´§å…¬å¸å…¨ç§°ä¸ç®€ç§°å¯¹ç…§è¡¨",
            "âœ… å¤„ç†å¤æ‚çš„ä¼ä¸šç»„ç»‡æ¶æ„å…³ç³»",
            "âœ… å¢å¼ºå¯¹å­å…¬å¸ã€åˆ†æ”¯æœºæ„çš„è¯†åˆ«",
            "âœ… ä½¿ç”¨å®ä½“é“¾æ¥æŠ€æœ¯ç»Ÿä¸€ä¸åŒè¡¨è¿°",
            "âœ… å»ºç«‹æœŸè´§å…¬å¸ä¸šåŠ¡èŒƒå›´è¯†åˆ«è§„åˆ™"
        ],
        "PRODUCT": [
            "âœ… åŒºåˆ†æœŸè´§å“ç§ä¸å…¶ä»–åŒåå®ä½“",
            "âœ… å»ºç«‹å“ç§ä»£ç ä¸ä¸­æ–‡åç§°æ˜ å°„å…³ç³»",
            "âœ… å¤„ç†å“ç§åç§°çš„å¤šç§å˜ä½“è¡¨è¾¾",
            "âœ… ç»“åˆä¸Šä¸‹æ–‡åˆ¤æ–­å®ä½“çš„çœŸå®å«ä¹‰",
            "âœ… ä½¿ç”¨å“ç§åˆ†ç±»è§„åˆ™æé«˜è¯†åˆ«å‡†ç¡®æ€§"
        ]
    }
    
    for entity_type, strategies in optimization_strategies.items():
        if entity_type in main_failure_types:
            print(f"\nğŸ“‹ {entity_type} å®ä½“è´¨é‡æå‡ç­–ç•¥:")
            for strategy in strategies:
                print(f"   {strategy}")
    
    # â­ è´¨é‡ä¼˜åŒ–ä»£ç å®ç°
    print(f"\nğŸ’» **è´¨é‡ä¼˜åŒ–ä»£ç å®ç°å»ºè®®**:")
    print(f"=" * 60)
    
#     code_examples = f"""
# # 1. é«˜ç²¾ç¡®ç‡NERé…ç½® (é€‚ç”¨äºå…³é”®ä¸šåŠ¡)
# import spacy
# from typing import List, Tuple, Dict

# def setup_high_precision_ner():
#     '''è®¾ç½®é«˜ç²¾ç¡®ç‡çš„NERç®¡é“'''
#     # ä½¿ç”¨ç²¾ç¡®ç‡æœ€é«˜çš„é…ç½®
#     nlp = spacy.load("{best_precision_config['é…ç½®åç§°'].split('_')[0]}_core_web_{best_precision_config['é…ç½®åç§°'].split('_')[1]}")
    
#     # ç²¾ç¡®ç‡ä¼˜å…ˆçš„åå¤„ç†
#     def high_precision_postprocess(entities: List[Tuple], text: str, confidence_threshold: float = 0.8) -> List[Tuple]:
#         '''åå¤„ç†ï¼šæé«˜ç²¾ç¡®ç‡ï¼Œé™ä½è¯¯æŠ¥'''
#         validated_entities = []
        
#         for start, end, label in entities:
#             entity_text = text[start:end]
#             confidence = calculate_entity_confidence(entity_text, label)
            
#             # åªä¿ç•™é«˜ç½®ä¿¡åº¦çš„å®ä½“
#             if confidence >= confidence_threshold:
#                 validated_entities.append((start, end, label))
        
#         return validated_entities
    
#     def calculate_entity_confidence(entity_text: str, label: str) -> float:
#         '''è®¡ç®—å®ä½“ç½®ä¿¡åº¦'''
#         # åŸºäºè§„åˆ™çš„ç½®ä¿¡åº¦è®¡ç®—
#         confidence = 0.5  # åŸºç¡€ç½®ä¿¡åº¦
        
#         if label == "CONTRACT_CODE":
#             # åˆçº¦ä»£ç æ ¼å¼æ£€æŸ¥
#             import re
#             if re.match(r'^[A-Z]{{1,3}}\\d{{4}}

# # ç”Ÿæˆè´¨é‡å¯¼å‘çš„ä¼˜åŒ–å»ºè®®
# if df_basic_enhanced is not None:
#     generate_quality_focused_recommendations(df_basic_enhanced, df_detailed_enhanced, enhanced_benchmark_results)

# # â­ ä¿å­˜è´¨é‡åˆ†æç»“æœ
# def save_quality_focused_results(basic_results, detailed_results, benchmark_results):
#     """ä¿å­˜è´¨é‡å¯¼å‘çš„æµ‹è¯•ç»“æœ"""
    
#     print(f"\nğŸ’¾ ä¿å­˜è´¨é‡åˆ†æç»“æœ")
#     print("=" * 60)
    
#     # ä¿å­˜åŸºæœ¬è´¨é‡å¯¹æ¯”
#     if basic_results is not None and not basic_results.empty:
#         basic_results.to_csv("quality_focused_ner_comparison.csv", index=False, encoding="utf-8")
#         print(f"âœ… åŸºæœ¬è´¨é‡å¯¹æ¯”å·²ä¿å­˜åˆ°: quality_focused_ner_comparison.csv")
    
#     # ä¿å­˜è¯¦ç»†å®ä½“æ€§èƒ½
#     if detailed_results is not None and not detailed_results.empty:
#         detailed_results.to_csv("detailed_entity_quality_metrics.csv", index=False, encoding="utf-8")
#         print(f"âœ… è¯¦ç»†å®ä½“æ€§èƒ½å·²ä¿å­˜åˆ°: detailed_entity_quality_metrics.csv")
    
#     # ä¿å­˜åŸå§‹åŸºå‡†æµ‹è¯•ç»“æœ
#     clean_results = {}
#     for config_name, result in benchmark_results.items():
#         if result is not None:
#             clean_result = result.copy()
#             # ç§»é™¤ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡
#             clean_result.pop("detailed_results", None)
#             # ç®€åŒ–å¤±è´¥åˆ†ææ•°æ®
#             if "failure_analysis" in clean_result:
#                 failure_analysis = clean_result["failure_analysis"]
#                 if "detailed_cases" in failure_analysis:
#                     # åªä¿ç•™å‰10ä¸ªå¤±è´¥æ¡ˆä¾‹
#                     failure_analysis["detailed_cases"] = failure_analysis["detailed_cases"][:10]
#             clean_results[config_name] = clean_result
    
#     with open("quality_focused_benchmark_results.json", "w", encoding="utf-8") as f:
#         json.dump(clean_results, f, ensure_ascii=False, indent=2, default=str)
    
#     print(f"âœ… å®Œæ•´è´¨é‡åˆ†æç»“æœå·²ä¿å­˜åˆ°: quality_focused_benchmark_results.json")
#     print(f"ğŸ“Š ç»“æœæ–‡ä»¶ä¸“æ³¨äºè¯†åˆ«è´¨é‡åˆ†æï¼Œå¯åœ¨Excelä¸­è¿›ä¸€æ­¥åˆ†æ")

# # ä¿å­˜ç»“æœ
# save_quality_focused_results(df_basic_enhanced, df_detailed_enhanced, enhanced_benchmark_results)

# # æ€»ç»“
# print(f"\nğŸ‰ **è´¨é‡å¯¼å‘NERæ¨¡å‹åŸºå‡†æµ‹è¯•å®Œæˆ!**")
# print("=" * 80)
# print(f"ğŸ“Š æœ¬æ¬¡æµ‹è¯•è¯„ä¼°äº† {len([r for r in enhanced_benchmark_results.values() if r is not None])} ä¸ªæœ‰æ•ˆé…ç½®")
# print(f"ğŸ“ ä½¿ç”¨äº† {len(enhanced_test_texts)} ä¸ªå¢å¼ºæµ‹è¯•æ–‡æœ¬ï¼ŒåŒ…å«ä¸“ä¸šæœŸè´§åˆçº¦ä»£ç æµ‹è¯•")
# print(f"ğŸ¯ é‡ç‚¹åˆ†æäº†ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°ç­‰è´¨é‡æŒ‡æ ‡")
# print(f"ğŸ” æä¾›äº†è¯¦ç»†çš„å¤±è´¥æ¨¡å¼åˆ†æå’Œè´¨é‡æå‡å»ºè®®")
# print(f"ğŸ’¡ é’ˆå¯¹æœŸè´§äº¤æ˜“åœºæ™¯ç»™å‡ºäº†ä¸“ä¸šçš„è´¨é‡ä¼˜åŒ–ç­–ç•¥")
# print(f"\nğŸ† ä¸»è¦å‘ç°:")
# if df_basic_enhanced is not None and not df_basic_enhanced.empty:
#     best_f1_model = df_basic_enhanced.loc[df_basic_enhanced['Micro_F1'].idxmax()]
#     best_precision_model = df_basic_enhanced.loc[df_basic_enhanced['Micro_Precision'].idxmax()]
#     best_recall_model = df_basic_enhanced.loc[df_basic_enhanced['Micro_Recall'].idxmax()]
#     print(f"   ğŸ“ˆ æœ€ä½³F1åˆ†æ•°: {best_f1_model['æ¨¡å‹æè¿°']} (F1={best_f1_model['Micro_F1']:.3f})")
#     print(f"   ğŸ¯ æœ€ä½³ç²¾ç¡®ç‡: {best_precision_model['æ¨¡å‹æè¿°']} (Precision={best_precision_model['Micro_Precision']:.3f})")
#     print(f"   ğŸ” æœ€ä½³å¬å›ç‡: {best_recall_model['æ¨¡å‹æè¿°']} (Recall={best_recall_model['Micro_Recall']:.3f})")
#     print(f"   ğŸ“ ä¸“ä¸šåˆçº¦ä»£ç è¯†åˆ«èƒ½åŠ›å¾—åˆ°é‡ç‚¹æµ‹è¯•å’Œåˆ†æ")
# print(f"\nğŸ’» å»ºè®®æ ¹æ®å…·ä½“è´¨é‡è¦æ±‚é€‰æ‹©åˆé€‚çš„æ¨¡å‹é…ç½®è¿›è¡Œéƒ¨ç½²!")
# print(f"ğŸ¯ é‡ç‚¹å…³æ³¨precision/recallæƒè¡¡ï¼Œé’ˆå¯¹ä¸šåŠ¡åœºæ™¯ä¼˜åŒ–è¯†åˆ«è´¨é‡!")
# , entity_text):
#                 confidence += 0.4
#         elif label == "EXCHANGE":
#             # äº¤æ˜“æ‰€åç§°éªŒè¯
#             exchange_keywords = ["äº¤æ˜“æ‰€", "å•†æ‰€", "æ‰€"]
#             if any(keyword in entity_text for keyword in exchange_keywords):
#                 confidence += 0.3
#         elif label == "FUTURES_COMPANY":
#             # æœŸè´§å…¬å¸åç§°éªŒè¯
#             if "æœŸè´§" in entity_text:
#                 confidence += 0.3
        
#         return min(confidence, 1.0)
    
#     return nlp, high_precision_postprocess

# # 2. é«˜å¬å›ç‡NERé…ç½® (é€‚ç”¨äºä¿¡æ¯æ”¶é›†)
# def setup_high_recall_ner():
#     '''è®¾ç½®é«˜å¬å›ç‡çš„NERç®¡é“'''
#     # ä½¿ç”¨å¬å›ç‡æœ€é«˜çš„é…ç½®
#     nlp = spacy.load("{best_recall_config['é…ç½®åç§°'].split('_')[0]}_core_web_{best_recall_config['é…ç½®åç§°'].split('_')[1]}")
    
#     # å¤šæ¨¡å¼åŒ¹é…å¢å¼ºå¬å›ç‡
#     from spacy.matcher import Matcher
#     matcher = Matcher(nlp.vocab)
    
#     # æ·»åŠ æ›´å¤šåŒ¹é…æ¨¡å¼
#     patterns = {{
#         "CONTRACT_CODE": [
#             [{{"TEXT": {{"REGEX": r"[A-Z]{{1,3}}\\d{{4}}"}}}}],  # æ ‡å‡†åˆçº¦ä»£ç 
#             [{{"TEXT": {{"REGEX": r"[A-Za-z]{{1,3}}\\d{{4}}"}}}}],  # åŒ…å«å°å†™å­—æ¯
#         ],
#         "EXCHANGE_ALIAS": [
#             [{{"LOWER": "ä¸ŠæœŸæ‰€"}}],
#             [{{"LOWER": "å¤§å•†æ‰€"}}],
#             [{{"LOWER": "éƒ‘å•†æ‰€"}}],
#             [{{"LOWER": "ä¸­é‡‘æ‰€"}}],
#         ]
#     }}
    
#     for label, pattern_list in patterns.items():
#         matcher.add(label, pattern_list)
    
#     def high_recall_extract(text: str) -> List[Tuple]:
#         '''é«˜å¬å›ç‡å®ä½“æå–'''
#         doc = nlp(text)
#         entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]
        
#         # æ·»åŠ è§„åˆ™åŒ¹é…çš„ç»“æœ
#         matches = matcher(doc)
#         for match_id, start, end in matches:
#             span = doc[start:end]
#             label = nlp.vocab.strings[match_id]
#             entities.append((span.start_char, span.end_char, label))
        
#         # å»é‡
#         entities = list(set(entities))
#         return sorted(entities)
    
#     return high_recall_extract

# # 3. å¹³è¡¡F1åˆ†æ•°çš„NERé…ç½® (æ¨èç”¨äºç”Ÿäº§ç¯å¢ƒ)
# def setup_balanced_ner():
#     '''è®¾ç½®å¹³è¡¡F1åˆ†æ•°çš„NERç®¡é“'''
#     # ä½¿ç”¨F1åˆ†æ•°æœ€é«˜çš„é…ç½®
#     nlp = spacy.load("{best_f1_config['é…ç½®åç§°'].split('_')[0]}_core_web_{best_f1_config['é…ç½®åç§°'].split('_')[1]}")
    
#     def balanced_ner_pipeline(text: str) -> Dict[str, any]:
#         '''å¹³è¡¡çš„NERç®¡é“ï¼Œè¿”å›è¯¦ç»†ç»“æœ'''
#         doc = nlp(text)
        
#         entities = []
#         for ent in doc.ents:
#             entity_info = {{
#                 "text": ent.text,
#                 "label": ent.label_,
#                 "start": ent.start_char,
#                 "end": ent.end_char,
#                 "confidence": calculate_entity_confidence(ent.text, ent.label_)
#             }}
#             entities.append(entity_info)
        
#         return {{
#             "entities": entities,
#             "entity_count": len(entities),
#             "text_length": len(text),
#             "model_info": "{{}}".format("{best_f1_config['æ¨¡å‹æè¿°']}")
#         }}
    
#     return balanced_ner_pipeline

# # 4. è´¨é‡ç›‘æ§å’Œè¯„ä¼°å‡½æ•°
# def monitor_ner_quality(predictions: List[Tuple], ground_truth: List[Tuple]) -> Dict[str, float]:
#     '''ç›‘æ§NERè´¨é‡'''
#     from sklearn.metrics import precision_recall_fscore_support
    
#     # è½¬æ¢ä¸ºæ ‡ç­¾åºåˆ—è¿›è¡Œè¯„ä¼°
#     pred_labels = [label for _, _, label in predictions]
#     true_labels = [label for _, _, label in ground_truth]
    
#     if len(pred_labels) == 0 and len(true_labels) == 0:
#         return {{"precision": 1.0, "recall": 1.0, "f1": 1.0}}
    
#     precision, recall, f1, _ = precision_recall_fscore_support(
#         true_labels, pred_labels, average='micro', zero_division=0
#     )
    
#     return {{
#         "precision": precision,
#         "recall": recall, 
#         "f1": f1,
#         "total_predictions": len(predictions),
#         "total_ground_truth": len(ground_truth)
#     }}

# # 5. ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²ç¤ºä¾‹
# class ProductionNERService:
#     '''ç”Ÿäº§ç¯å¢ƒNERæœåŠ¡'''
    
#     def __init__(self, model_type: str = "balanced"):
#         self.model_type = model_type
#         self.quality_stats = {{
#             "total_processed": 0,
#             "total_entities": 0,
#             "avg_confidence": 0.0
#         }}
        
#         if model_type == "precision":
#             self.nlp, self.postprocess = setup_high_precision_ner()
#         elif model_type == "recall":
#             self.extract_func = setup_high_recall_ner()
#         else:  # balanced
#             self.pipeline = setup_balanced_ner()
    
#     def extract_entities(self, text: str) -> Dict[str, any]:
#         '''æå–å®ä½“'''
#         if self.model_type == "balanced":
#             result = self.pipeline(text)
#         else:
#             # å…¶ä»–ç±»å‹çš„å¤„ç†é€»è¾‘
#             entities = self.extract_func(text) if self.model_type == "recall" else []
#             result = {{"entities": entities}}
        
#         # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
#         self.quality_stats["total_processed"] += 1
#         self.quality_stats["total_entities"] += len(result["entities"])
        
#         return result
    
#     def get_quality_report(self) -> Dict[str, any]:
#         '''è·å–è´¨é‡æŠ¥å‘Š'''
#         avg_entities = (self.quality_stats["total_entities"] / 
#                        max(self.quality_stats["total_processed"], 1))
        
#         return {{
#             "model_type": self.model_type,
#             "total_processed": self.quality_stats["total_processed"],
#             "avg_entities_per_text": avg_entities,
#             "recommended_for": self._get_recommendation()
#         }}
    
#     def _get_recommendation(self) -> str:
#         '''è·å–ä½¿ç”¨å»ºè®®'''
#         recommendations = {{
#             "precision": "å…³é”®ä¸šåŠ¡åœºæ™¯ï¼Œéœ€è¦é«˜å‡†ç¡®ç‡",
#             "recall": "ä¿¡æ¯æ”¶é›†åœºæ™¯ï¼Œéœ€è¦é«˜è¦†ç›–ç‡", 
#             "balanced": "ä¸€èˆ¬ä¸šåŠ¡åœºæ™¯ï¼Œå¹³è¡¡å‡†ç¡®ç‡å’Œè¦†ç›–ç‡"
#         }}
#         return recommendations.get(self.model_type, "é€šç”¨åœºæ™¯")

# # ä½¿ç”¨ç¤ºä¾‹
# if __name__ == "__main__":
#     # æµ‹è¯•æ–‡æœ¬
#     test_text = "éƒ‘å•†æ‰€AP2502æœŸè´§æ€ä¹ˆæ ·äº†"
    
#     # ä¸åŒè´¨é‡ç›®æ ‡çš„å¤„ç†
#     print("é«˜ç²¾ç¡®ç‡å¤„ç†:")
#     nlp_precision, postprocess = setup_high_precision_ner()
#     doc = nlp_precision(test_text)
#     entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]
#     validated_entities = postprocess(entities, test_text)
#     print(f"ç»“æœ: {{validated_entities}}")
    
#     print("\\né«˜å¬å›ç‡å¤„ç†:")
#     extract_recall = setup_high_recall_ner()
#     recall_entities = extract_recall(test_text)
#     print(f"ç»“æœ: {{recall_entities}}")
    
#     print("\\nå¹³è¡¡å¤„ç†:")
#     balanced_pipeline = setup_balanced_ner()
#     balanced_result = balanced_pipeline(test_text)
#     print(f"ç»“æœ: {{balanced_result}}")
    
#     # ç”Ÿäº§æœåŠ¡ç¤ºä¾‹
#     service = ProductionNERService("balanced")
#     result = service.extract_entities(test_text)
#     quality_report = service.get_quality_report()
#     print(f"\\nç”Ÿäº§æœåŠ¡ç»“æœ: {{result}}")
#     print(f"è´¨é‡æŠ¥å‘Š: {{quality_report}}")
# """
    
#     print(code_examples)

# ç”Ÿæˆå¢å¼ºä¼˜åŒ–å»ºè®®
if df_basic_enhanced is not None:
    generate_enhanced_optimization_recommendations(df_basic_enhanced, df_detailed_enhanced, enhanced_benchmark_results)

# â­ ä¿å­˜å¢å¼ºç»“æœ
def save_enhanced_results(basic_results, detailed_results, benchmark_results):
    """ä¿å­˜å¢å¼ºçš„æµ‹è¯•ç»“æœ"""
    
    print(f"\nğŸ’¾ ä¿å­˜å¢å¼ºæµ‹è¯•ç»“æœ")
    print("=" * 60)
    
    # ä¿å­˜åŸºæœ¬æ€§èƒ½å¯¹æ¯”
    if basic_results is not None and not basic_results.empty:
        basic_results.to_csv("enhanced_ner_performance_comparison.csv", index=False, encoding="utf-8")
        print(f"âœ… åŸºæœ¬æ€§èƒ½å¯¹æ¯”å·²ä¿å­˜åˆ°: enhanced_ner_performance_comparison.csv")
    
    # ä¿å­˜è¯¦ç»†å®ä½“æ€§èƒ½
    if detailed_results is not None and not detailed_results.empty:
        detailed_results.to_csv("detailed_entity_performance.csv", index=False, encoding="utf-8")
        print(f"âœ… è¯¦ç»†å®ä½“æ€§èƒ½å·²ä¿å­˜åˆ°: detailed_entity_performance.csv")
    
    # ä¿å­˜åŸå§‹åŸºå‡†æµ‹è¯•ç»“æœ
    clean_results = {}
    for config_name, result in benchmark_results.items():
        if result is not None:
            clean_result = result.copy()
            # ç§»é™¤ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡
            clean_result.pop("detailed_results", None)
            # ç®€åŒ–å¤±è´¥åˆ†ææ•°æ®
            if "failure_analysis" in clean_result:
                failure_analysis = clean_result["failure_analysis"]
                if "detailed_cases" in failure_analysis:
                    # åªä¿ç•™å‰10ä¸ªå¤±è´¥æ¡ˆä¾‹
                    failure_analysis["detailed_cases"] = failure_analysis["detailed_cases"][:10]
            clean_results[config_name] = clean_result
    
    with open("enhanced_ner_benchmark_results.json", "w", encoding="utf-8") as f:
        json.dump(clean_results, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"âœ… å®Œæ•´åŸºå‡†æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: enhanced_ner_benchmark_results.json")
    print(f"ğŸ“Š ç»“æœæ–‡ä»¶å¯åœ¨Excelæˆ–å…¶ä»–å·¥å…·ä¸­è¿›ä¸€æ­¥åˆ†æ")

# ä¿å­˜ç»“æœ
save_enhanced_results(df_basic_enhanced, df_detailed_enhanced, enhanced_benchmark_results)

# æ€»ç»“
print(f"\nğŸ‰ **å¢å¼ºç‰ˆNERæ¨¡å‹åŸºå‡†æµ‹è¯•å®Œæˆ!**")
print("=" * 80)
print(f"ğŸ“Š æœ¬æ¬¡æµ‹è¯•è¯„ä¼°äº† {len([r for r in enhanced_benchmark_results.values() if r is not None])} ä¸ªæœ‰æ•ˆé…ç½®")
print(f"ğŸ“ ä½¿ç”¨äº† {len(enhanced_test_texts)} ä¸ªå¢å¼ºæµ‹è¯•æ–‡æœ¬ï¼ŒåŒ…å«ä¸“ä¸šæœŸè´§åˆçº¦ä»£ç æµ‹è¯•")
print(f"ğŸ¯ æ–°å¢äº†ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°ç­‰è¯¦ç»†æ€§èƒ½æŒ‡æ ‡")
print(f"ğŸ” æä¾›äº†è¯¦ç»†çš„å¤±è´¥æ¨¡å¼åˆ†æå’Œä¼˜åŒ–å»ºè®®")
print(f"ğŸ’¡ é’ˆå¯¹æœŸè´§äº¤æ˜“åœºæ™¯ç»™å‡ºäº†ä¸“ä¸šçš„æ¨¡å‹é€‰æ‹©å’Œéƒ¨ç½²å»ºè®®")
print(f"\nğŸ† ä¸»è¦å‘ç°:")
if df_basic_enhanced is not None and not df_basic_enhanced.empty:
    best_f1_model = df_basic_enhanced.loc[df_basic_enhanced['Micro_F1'].idxmax()]
    fastest_model = df_basic_enhanced.loc[df_basic_enhanced['å¹³å‡å»¶è¿Ÿ(æ¯«ç§’)'].idxmin()]
    print(f"   ğŸ“ˆ æœ€ä½³F1åˆ†æ•°: {best_f1_model['æ¨¡å‹æè¿°']} (F1={best_f1_model['Micro_F1']:.3f})")
    print(f"   âš¡ æœ€å¿«å¤„ç†é€Ÿåº¦: {fastest_model['æ¨¡å‹æè¿°']} ({fastest_model['å¹³å‡å»¶è¿Ÿ(æ¯«ç§’)']:.1f}ms)")
    print(f"   ğŸ¯ ä¸“ä¸šåˆçº¦ä»£ç è¯†åˆ«èƒ½åŠ›å¾—åˆ°é‡ç‚¹æµ‹è¯•å’Œåˆ†æ")
print(f"\nğŸ’» å»ºè®®æ ¹æ®å…·ä½“ä¸šåŠ¡åœºæ™¯é€‰æ‹©åˆé€‚çš„æ¨¡å‹é…ç½®è¿›è¡Œéƒ¨ç½²!")