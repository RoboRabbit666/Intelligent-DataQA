import re
import time
import json
import copy
from typing import Dict, List, Tuple, Set, Optional
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict, Counter

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")

print("ğŸ”§ å®ç”¨å¾®æ”¹è¿›æ–¹æ¡ˆ - çœŸå®æ€§èƒ½æŒ‡æ ‡æµ‹è¯•")
print("=" * 80)

# %% [markdown]
# ## 1. æµ‹è¯•è¯­æ–™åº“

def create_focused_test_corpus():
    """åˆ›å»ºä¸“æ³¨äºåˆçº¦ä»£ç è¯†åˆ«çš„æµ‹è¯•è¯­æ–™åº“"""
    
    # 1. æ ¸å¿ƒåˆçº¦ä»£ç æŸ¥è¯¢
    contract_code_queries = [
        "éƒ‘å•†æ‰€AP2405æœŸè´§æ€ä¹ˆæ ·äº†",
        "ä¸ŠæœŸæ‰€CU2405é“œæœŸè´§ä»·æ ¼å¦‚ä½•",
        "å¤§å•†æ‰€M2405è±†ç²•æœŸè´§èµ°åŠ¿åˆ†æ", 
        "ä¸­é‡‘æ‰€IF2405æœŸè´§ä»Šæ—¥è¡¨ç°",
        "éƒ‘å•†æ‰€SR2405ç™½ç³–æœŸè´§è¡Œæƒ…",
        "ä¸ŠæœŸæ‰€RB2405èºçº¹é’¢æœŸè´§æ•°æ®",
        "å¤§å•†æ‰€I2405é“çŸ¿çŸ³æœŸè´§æŠ¥ä»·",
        "éƒ‘å•†æ‰€TA2405PTAæœŸè´§åˆ†æ",
        "ä¸ŠæœŸæ‰€AL2405é“æœŸè´§èµ°åŠ¿",
        "ä¸­é‡‘æ‰€IC2405ä¸­è¯500æœŸè´§",
        
        # å˜ä½“è¡¨è¾¾
        "è¯·é—®éƒ‘å•†æ‰€AP2405è‹¹æœæœŸè´§æ€ä¹ˆæ ·",
        "ä»Šå¤©ä¸ŠæœŸæ‰€CU2405é“œæœŸè´§ä»·æ ¼",
        "å¤§å•†æ‰€M2405è±†ç²•åˆçº¦å¦‚ä½•",
        "ä¸­é‡‘æ‰€IF2405æ²ªæ·±300æœŸè´§è¡Œæƒ…",
        "éƒ‘å•†æ‰€SR2405ç™½ç³–åˆçº¦åˆ†æ",
        
        # æ›´å¤æ‚çš„è¡¨è¾¾
        "éƒ‘å•†æ‰€AP2405è‹¹æœæœŸè´§ä¸»åŠ›åˆçº¦ä»·æ ¼èµ°åŠ¿",
        "ä¸ŠæœŸæ‰€CU2405é“œæœŸè´§ä»Šæ—¥æ”¶ç›˜ä»·æ ¼",
        "å¤§å•†æ‰€M2405è±†ç²•æœŸè´§å¤œç›˜äº¤æ˜“æƒ…å†µ",
        "ä¸­é‡‘æ‰€IF2405æœŸè´§åŸºå·®å˜åŒ–åˆ†æ",
        "éƒ‘å•†æ‰€SR2405ç™½ç³–æœŸè´§æŒä»“é‡æ•°æ®",
    ]
    
    # 2. å¸¦å…¬å¸ä¿¡æ¯çš„å¤æ‚æŸ¥è¯¢
    company_contract_queries = [
        "åæ³°æœŸè´§å¯¹éƒ‘å•†æ‰€AP2405æœŸè´§çš„åˆ†æ",
        "ä¸­ä¿¡æœŸè´§å‘å¸ƒä¸ŠæœŸæ‰€CU2405é“œæœŸè´§æŠ¥å‘Š", 
        "æ°¸å®‰æœŸè´§å…³äºå¤§å•†æ‰€M2405è±†ç²•æœŸè´§å»ºè®®",
        "å›½æ³°å›å®‰æœŸè´§ç ”ç©¶ä¸­é‡‘æ‰€IF2405æœŸè´§",
        "æµ·é€šæœŸè´§è§£è¯»éƒ‘å•†æ‰€SR2405ç™½ç³–æœŸè´§",
        "æ–¹æ­£ä¸­æœŸæœŸè´§çœ‹å¥½ä¸ŠæœŸæ‰€RB2405èºçº¹é’¢",
        "å…‰å¤§æœŸè´§åˆ†æå¤§å•†æ‰€I2405é“çŸ¿çŸ³æœŸè´§",
        "é“¶æ²³æœŸè´§ç­–ç•¥éƒ‘å•†æ‰€TA2405PTAæœŸè´§",
        "æ‹›å•†æœŸè´§å»ºè®®ä¸ŠæœŸæ‰€AL2405é“æœŸè´§",
        "å¹¿å‘æœŸè´§è§£æä¸­é‡‘æ‰€IC2405æœŸè´§",
    ]
    
    # 3. é‡å¤å®ä½“é—®é¢˜æµ‹è¯•æ¡ˆä¾‹
    duplicate_issue_queries = [
        "è‹¹æœæœŸè´§AP2405è‹¹æœæœŸè´§ä»·æ ¼",  # é‡å¤
        "éƒ‘å•†æ‰€äº¤æ˜“æ‰€AP2405æœŸè´§",  # å†—ä½™
        "AP2405æœŸè´§APè‹¹æœåˆ†æ",  # æ‹†åˆ†
        "CUé“œæœŸè´§2405åˆçº¦ä»·æ ¼",  # éƒ¨åˆ†åŒ¹é…
        "Mè±†ç²•2405æœŸè´§èµ°åŠ¿",  # ç®€åŒ–
        "åæ³°æœŸè´§åæ³°æœŸè´§å…¬å¸ç ”ç©¶æ‰€åˆ†æAP2405",  # åµŒå¥—å®ä½“
        "èºçº¹é’¢é’¢æœŸè´§RB2405ä»·æ ¼æ³¢åŠ¨",  # å“ç§åç§°é”™è¯¯
        "åŸæ²¹æ²¹æœŸè´§äº¤å‰²ç»†åˆ™",  # å“ç§åç§°ç®€åŒ–
        "è‚¡æŒ‡æœŸè´§IF2405æŒ‡æ•°åŸºå·®åˆ†æ",  # æœ¯è¯­æ··åˆ
        "æœŸè´§æœŸè´§å…¬å¸AP2405é£é™©ç®¡ç†",  # æœ¯è¯­é‡å¤
    ]
    
    # 4. è¯¯è¯†åˆ«é—®é¢˜æµ‹è¯•æ¡ˆä¾‹
    false_positive_queries = [
        "è‹¹æœå…¬å¸è‚¡ä»·åˆ†ææŠ¥å‘Š",
        "åæ³°è¯åˆ¸ç ”ç©¶å›¢é˜Ÿå‘å¸ƒæŠ¥å‘Š", 
        "é“œä»·æ ¼èµ°åŠ¿åˆ†æå›¾è¡¨",
        "ç™½ç³–ç°è´§å¸‚åœºè¡Œæƒ…ç ”ç©¶",
        "æœŸè´§å¸‚åœºæ•´ä½“èµ°åŠ¿åˆ†æ",
        "äº¤æ˜“æ‰€ç›‘ç®¡æ”¿ç­–è§£è¯»",
        "å¤§å®—å•†å“ä»·æ ¼æŒ‡æ•°ç ”ç©¶",
        "é‡‘èè¡ç”Ÿå“å¸‚åœºåˆ†æ",
        "æŠ•èµ„ç­–ç•¥å»ºè®®æŠ¥å‘Š",
        "å¸‚åœºé£é™©è¯„ä¼°æŠ¥å‘Š",
    ]
    
    # 5. æ ¼å¼å˜ä½“æµ‹è¯•æ¡ˆä¾‹
    format_variant_queries = [
        "éƒ‘å•†æ‰€AP-2405æœŸè´§åˆçº¦åˆ†æ",  # è¿å­—ç¬¦
        "ä¸ŠæœŸæ‰€CU 2405æœŸè´§äº¤æ˜“",     # ç©ºæ ¼
        "å¤§å•†æ‰€M.2405è±†ç²•æœŸè´§ä»·æ ¼",   # ç‚¹å·
        "ä¸­é‡‘æ‰€IF/2405æœŸè´§èµ°åŠ¿",     # æ–œæ 
        "éƒ‘å•†æ‰€SR_2405ç™½ç³–æœŸè´§",     # ä¸‹åˆ’çº¿
        "ä¸ŠæœŸæ‰€RB(2405)èºçº¹é’¢æœŸè´§",  # æ‹¬å·
        "å¤§å•†æ‰€IæœŸè´§2405é“çŸ¿çŸ³",     # é¡ºåºå˜åŒ–
        "ä¸­é‡‘æ‰€æœŸè´§IC2405èµ°åŠ¿",      # ä½ç½®å˜åŒ–
        "AP 24 05æœŸè´§ä»·æ ¼åˆ†æ",      # å¤šç©ºæ ¼
        "CU-24-05é“œæœŸè´§èµ°åŠ¿",       # å¤šè¿å­—ç¬¦
    ]
    
    # åˆå¹¶æ‰€æœ‰æµ‹è¯•ç±»å‹
    all_test_categories = {
        "æ ¸å¿ƒåˆçº¦ä»£ç æŸ¥è¯¢": contract_code_queries,
        "å…¬å¸ä¸åˆçº¦ç»„åˆ": company_contract_queries,
        "é‡å¤å®ä½“é—®é¢˜": duplicate_issue_queries,
        "è¯¯è¯†åˆ«é—®é¢˜": false_positive_queries,
        "æ ¼å¼å˜ä½“æµ‹è¯•": format_variant_queries,
    }
    
    # å±•å¹³æ‰€æœ‰æµ‹è¯•æ–‡æœ¬
    flat_tests = []
    for category, texts in all_test_categories.items():
        flat_tests.extend(texts)
    
    return flat_tests, all_test_categories

# åˆ›å»ºæµ‹è¯•è¯­æ–™
test_texts, categorized_tests = create_focused_test_corpus()

print(f"ğŸ“Š å¾®æ”¹è¿›ä¸“ç”¨æµ‹è¯•è¯­æ–™åº“ç»Ÿè®¡")
print("=" * 50)
print(f"æ€»æ–‡æœ¬æ•°é‡: {len(test_texts)}")
print(f"æµ‹è¯•ç±»åˆ«: {len(categorized_tests)}")
print("\nå„ç±»åˆ«æ–‡æœ¬æ•°é‡:")
for category, texts in categorized_tests.items():
    print(f"  {category}: {len(texts)} æ¡")

# %% [markdown]
# ## 2. åŸå§‹RegExæ–¹æ³•å®ç°

class OriginalRegexNERMethod:
    """åŸå§‹RegExå®ä½“è¯†åˆ«æ–¹æ³•"""
    
    def __init__(self):
        self.entity_patterns = {
            "EXCHANGE": [
                r"éƒ‘å•†æ‰€|éƒ‘å·å•†å“äº¤æ˜“æ‰€|CZCE|czce",
                r"ä¸ŠæœŸæ‰€|ä¸Šæµ·æœŸè´§äº¤æ˜“æ‰€|SHFE|shfe", 
                r"å¤§å•†æ‰€|å¤§è¿å•†å“äº¤æ˜“æ‰€|DCE|dce",
                r"ä¸­é‡‘æ‰€|ä¸­å›½é‡‘èæœŸè´§äº¤æ˜“æ‰€|CFFEX|cffex|CFE",
                r"ä¸ŠæœŸèƒ½æº|ä¸Šæµ·å›½é™…èƒ½æºäº¤æ˜“ä¸­å¿ƒ|INE|ine",
                r"å¹¿æœŸæ‰€|å¹¿å·æœŸè´§äº¤æ˜“æ‰€|GFEX|gfex"
            ],
            "FUTURES_COMPANY": [
                r"åæ³°æœŸè´§(?:æœ‰é™å…¬å¸)?",
                r"ä¸­ä¿¡æœŸè´§(?:æœ‰é™å…¬å¸)?",
                r"æ°¸å®‰æœŸè´§(?:è‚¡ä»½æœ‰é™å…¬å¸)?",
                r"å›½æ³°å›å®‰æœŸè´§(?:æœ‰é™å…¬å¸)?",
                r"æµ·é€šæœŸè´§(?:æœ‰é™å…¬å¸)?",
                r"æ–¹æ­£ä¸­æœŸæœŸè´§(?:æœ‰é™å…¬å¸)?",
                r"å…‰å¤§æœŸè´§(?:æœ‰é™å…¬å¸)?",
                r"é“¶æ²³æœŸè´§(?:æœ‰é™å…¬å¸)?",
                r"æ‹›å•†æœŸè´§(?:æœ‰é™å…¬å¸)?",
                r"å¹¿å‘æœŸè´§(?:æœ‰é™å…¬å¸)?"
            ],
            "PRODUCT": [
                r"è‹¹æœ|AP|ap",
                r"æ£‰èŠ±|CF|cf", 
                r"ç™½ç³–|SR|sr",
                r"PTA|TA|ta",
                r"ç”²é†‡|MA|ma",
                r"ç»ç’ƒ|FG|fg",
                r"é“œ|CU|cu",
                r"é“|AL|al", 
                r"èºçº¹é’¢|RB|rb",
                r"çƒ­è½§å·æ¿|HC|hc",
                r"è±†ç²•|M|(?<!A)m(?!a)",
                r"è±†æ²¹|Y|(?<!C)y",
                r"ç‰ç±³|C|(?<!D)c(?!u)",
                r"è±†ä¸€|A|(?<!T)a(?!l)",
                r"é“çŸ¿çŸ³|I|(?<!F)i(?!f)",
                r"ç„¦ç‚­|J|(?<!D)j",
                r"ç„¦ç…¤|JM|jm",
                r"æ²ªæ·±300|IF|if",
                r"ä¸Šè¯50|IH|ih", 
                r"ä¸­è¯500|IC|ic"
            ],
            "CONTRACT_CODE": [
                r"(?:AP|ap)(?:24|25)\d{2}",
                r"(?:CU|cu)(?:24|25)\d{2}",
                r"(?:M|(?<!A)m(?!a))(?:24|25)\d{2}",
                r"(?:SR|sr)(?:24|25)\d{2}",
                r"(?:TA|ta)(?:24|25)\d{2}",
                r"(?:RB|rb)(?:24|25)\d{2}",
                r"(?:I|(?<!F)i(?!f))(?:24|25)\d{2}",
                r"(?:IF|if)(?:24|25)\d{2}",
                r"(?:AL|al)(?:24|25)\d{2}",
                r"(?:IC|ic)(?:24|25)\d{2}",
            ],
            "PRICE_VALUE": [
                r"\d+(?:,\d{3})*\.?\d*å…ƒ/å¨",
                r"\d+(?:\.\d+)?%",
                r"\d+(?:,\d{3})*æ‰‹",
                r"\d+(?:\.\d+)?ä¸‡å¨",
                r"\d+(?:,\d{3})*(?:\.\d+)?äº¿å…ƒ"
            ],
            "TIME": [
                r"\d{4}å¹´\d{1,2}æœˆ(?:\d{1,2}æ—¥)?",
                r"\d{4}å¹´Q[1-4]",
                r"\d{1,2}:\d{2}-\d{1,2}:\d{2}",
                r"å¤œç›˜|æ”¶ç›˜|å¼€ç›˜"
            ]
        }
        self.compiled_patterns = self._compile_patterns()
        
    def _compile_patterns(self):
        compiled = {}
        for entity_type, patterns in self.entity_patterns.items():
            compiled[entity_type] = []
            for pattern in patterns:
                try:
                    compiled[entity_type].append(re.compile(pattern, re.IGNORECASE))
                except re.error as e:
                    print(f"è­¦å‘Š: ç¼–è¯‘æ¨¡å¼å¤±è´¥ {pattern}: {e}")
        return compiled
        
    def recognize_entities(self, text: str) -> List[Tuple[str, str, int, int]]:
        entities = []
        for entity_type, patterns in self.compiled_patterns.items():
            for pattern in patterns:
                for match in pattern.finditer(text):
                    entity_text = match.group()
                    start_pos = match.start()
                    end_pos = match.end()
                    
                    is_duplicate = any(
                        start_pos == existing[2] and end_pos == existing[3]
                        for existing in entities
                    )
                    
                    if not is_duplicate:
                        entities.append((entity_text, entity_type, start_pos, end_pos))
        
        entities.sort(key=lambda x: x[2])
        return entities

# %% [markdown]
# ## 3. å¾®å°æ”¹è¿›RegExæ–¹æ³•å®ç°

class MicroImprovedRegexNER:
    """å¾®å°æ”¹è¿›RegExæ–¹æ³• - åŸºäºåå¤„ç†çš„ä¿å®ˆæ”¹è¿›"""
    
    def __init__(self):
        # ä½¿ç”¨ä¸åŸå§‹æ–¹æ³•å®Œå…¨ç›¸åŒçš„åŸºç¡€æ¨¡å¼
        self.original_method = OriginalRegexNERMethod()
        
        # å¾®å°æ”¹è¿›é…ç½®
        self.improvement_config = {
            'deduplicate_overlapping': True,
            'filter_false_positives': True,
            'enhance_contract_formats': True,
        }
    
    def _deduplicate_overlapping_entities(self, entities: List[Tuple]) -> List[Tuple]:
        """å»é™¤é‡å å®ä½“"""
        if not entities:
            return entities
        
        # æŒ‰ä½ç½®æ’åº
        entities.sort(key=lambda x: x[2])
        
        deduplicated = []
        
        for current in entities:
            current_text, current_type, current_start, current_end = current
            
            should_add = True
            for i, existing in enumerate(deduplicated):
                existing_text, existing_type, existing_start, existing_end = existing
                
                # æ£€æŸ¥é‡å 
                if not (current_end <= existing_start or current_start >= existing_end):
                    # æœ‰é‡å ï¼Œä¿ç•™æ›´å®Œæ•´çš„å®ä½“
                    
                    # è§„åˆ™1ï¼šå¦‚æœä¸€ä¸ªå®Œå…¨åŒ…å«å¦ä¸€ä¸ªï¼Œä¿ç•™æ›´é•¿çš„
                    if (current_start >= existing_start and current_end <= existing_end):
                        should_add = False
                        break
                    elif (existing_start >= current_start and existing_end <= current_end):
                        deduplicated.pop(i)
                        break
                    
                    # è§„åˆ™2ï¼šä¼˜å…ˆä¿ç•™åˆçº¦ä»£ç 
                    elif current_type == 'CONTRACT_CODE' and existing_type != 'CONTRACT_CODE':
                        deduplicated.pop(i)
                        break
                    elif existing_type == 'CONTRACT_CODE' and current_type != 'CONTRACT_CODE':
                        should_add = False
                        break
                    
                    # è§„åˆ™3ï¼šä¿ç•™æ›´é•¿çš„æ–‡æœ¬
                    elif len(current_text) > len(existing_text):
                        deduplicated.pop(i)
                        break
                    else:
                        should_add = False
                        break
            
            if should_add:
                deduplicated.append(current)
        
        return deduplicated
    
    def _filter_false_positives(self, entities: List[Tuple], text: str) -> List[Tuple]:
        """è¿‡æ»¤è¯¯è¯†åˆ«"""
        if not entities:
            return entities
        
        filtered = []
        
        # å®šä¹‰æ˜æ˜¾ä¸æ˜¯æœŸè´§ç›¸å…³çš„ä¸Šä¸‹æ–‡
        non_futures_patterns = [
            r'è‹¹æœå…¬å¸',
            r'è‹¹æœè‚¡ä»·',
            r'è‹¹æœæ‰‹æœº',
            r'åæ³°è¯åˆ¸',
            r'é“œä»·æ ¼(?!.*æœŸè´§)',
            r'ç™½ç³–ä»·æ ¼(?!.*æœŸè´§)',
        ]
        
        for entity in entities:
            entity_text, entity_type, start_pos, end_pos = entity
            
            should_keep = True
            
            # ä¸»è¦å¯¹PRODUCTç±»å‹è¿›è¡Œè¿‡æ»¤
            if entity_type == 'PRODUCT':
                # è·å–ä¸Šä¸‹æ–‡
                context_window = 15
                context_start = max(0, start_pos - context_window)
                context_end = min(len(text), end_pos + context_window)
                context = text[context_start:context_end]
                
                # æ£€æŸ¥æ˜¯å¦åŒ¹é…éæœŸè´§æ¨¡å¼
                for pattern in non_futures_patterns:
                    if re.search(pattern, context, re.IGNORECASE):
                        should_keep = False
                        break
            
            if should_keep:
                filtered.append(entity)
        
        return filtered
    
    def _enhance_contract_formats(self, entities: List[Tuple], text: str) -> List[Tuple]:
        """å¢å¼ºåˆçº¦ä»£ç æ ¼å¼æ”¯æŒ"""
        enhanced = copy.deepcopy(entities)
        
        # å®šä¹‰åˆçº¦ä»£ç å˜ä½“æ¨¡å¼
        variant_patterns = [
            r'\b([A-Z]{1,3})\s*-\s*(\d{4})\b',      # AP-2405
            r'\b([A-Z]{1,3})\s+(\d{4})\b',          # AP 2405
            r'\b([A-Z]{1,3})\.(\d{4})\b',           # AP.2405
            r'\b([A-Z]{1,3})\s*/\s*(\d{4})\b',      # AP/2405
            r'\b([A-Z]{1,3})\s*_\s*(\d{4})\b',      # AP_2405
        ]
        
        valid_codes = ['AP', 'CU', 'SR', 'TA', 'MA', 'RB', 'IF', 'AL', 'IC', 
                      'M', 'Y', 'C', 'A', 'I', 'J', 'JM', 'IH']
        
        for pattern in variant_patterns:
            for match in re.finditer(pattern, text, re.IGNORECASE):
                code_part = match.group(1).upper()
                number_part = match.group(2)
                
                if code_part in valid_codes:
                    start_pos = match.start()
                    end_pos = match.end()
                    
                    # æ£€æŸ¥æ˜¯å¦å·²è¢«è¦†ç›–
                    is_covered = any(
                        not (end_pos <= existing[2] or start_pos >= existing[3])
                        for existing in enhanced
                    )
                    
                    if not is_covered:
                        enhanced.append((
                            match.group(),
                            'CONTRACT_CODE',
                            start_pos,
                            end_pos
                        ))
        
        return enhanced
    
    def recognize_entities(self, text: str) -> List[Tuple[str, str, int, int]]:
        """å¾®å°æ”¹è¿›çš„å®ä½“è¯†åˆ«"""

        # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨åŸå§‹æ–¹æ³•è¯†åˆ«
        entities = self.original_method.recognize_entities(text)

        # ç¬¬äºŒæ­¥ï¼šåº”ç”¨å¾®å°æ”¹è¿›
        if self.improvement_config.get('deduplicate_overlapping', False):
            entities = self._deduplicate_overlapping_entities(entities)
        
        if self.improvement_config.get('filter_false_positives', False):
            entities = self._filter_false_positives(entities, text)
        
        if self.improvement_config.get('enhance_contract_formats', False):
            entities = self._enhance_contract_formats(entities, text)
        
        # æœ€ç»ˆæ’åº
        entities.sort(key=lambda x: x[2])
        return entities

# %% [markdown]
# ## 4. æ ‡å‡†ç­”æ¡ˆå’Œæ€§èƒ½è®¡ç®—

def create_ground_truth_mapping():
    """åˆ›å»ºæ ‡å‡†ç­”æ¡ˆæ˜ å°„"""
    return {
        # äº¤æ˜“æ‰€æ ‡å‡†ç­”æ¡ˆ
        "éƒ‘å•†æ‰€": "EXCHANGE", "ä¸ŠæœŸæ‰€": "EXCHANGE", "å¤§å•†æ‰€": "EXCHANGE", 
        "ä¸­é‡‘æ‰€": "EXCHANGE", "CZCE": "EXCHANGE", "SHFE": "EXCHANGE",
        "DCE": "EXCHANGE", "CFFEX": "EXCHANGE",
        
        # æœŸè´§å…¬å¸æ ‡å‡†ç­”æ¡ˆ
        "åæ³°æœŸè´§": "FUTURES_COMPANY", "ä¸­ä¿¡æœŸè´§": "FUTURES_COMPANY",
        "æ°¸å®‰æœŸè´§": "FUTURES_COMPANY", "å›½æ³°å›å®‰æœŸè´§": "FUTURES_COMPANY",
        "æµ·é€šæœŸè´§": "FUTURES_COMPANY", "æ–¹æ­£ä¸­æœŸæœŸè´§": "FUTURES_COMPANY",
        "å…‰å¤§æœŸè´§": "FUTURES_COMPANY", "é“¶æ²³æœŸè´§": "FUTURES_COMPANY",
        "æ‹›å•†æœŸè´§": "FUTURES_COMPANY", "å¹¿å‘æœŸè´§": "FUTURES_COMPANY",
        
        # åˆçº¦ä»£ç æ ‡å‡†ç­”æ¡ˆ
        "AP2405": "CONTRACT_CODE", "CU2405": "CONTRACT_CODE", 
        "M2405": "CONTRACT_CODE", "SR2405": "CONTRACT_CODE",
        "TA2405": "CONTRACT_CODE", "RB2405": "CONTRACT_CODE",
        "I2405": "CONTRACT_CODE", "IF2405": "CONTRACT_CODE",
        "AL2405": "CONTRACT_CODE", "IC2405": "CONTRACT_CODE",
        
        # æ ¼å¼å˜ä½“ä¹Ÿç®—ä½œåˆçº¦ä»£ç 
        "AP-2405": "CONTRACT_CODE", "CU 2405": "CONTRACT_CODE",
        "M.2405": "CONTRACT_CODE", "SR/2405": "CONTRACT_CODE",
        "TA_2405": "CONTRACT_CODE",
        
        # å“ç§åç§°
        "è‹¹æœ": "PRODUCT", "é“œ": "PRODUCT", "è±†ç²•": "PRODUCT",
        "ç™½ç³–": "PRODUCT", "PTA": "PRODUCT", "èºçº¹é’¢": "PRODUCT",
        "é“çŸ¿çŸ³": "PRODUCT", "æ²ªæ·±300": "PRODUCT", "é“": "PRODUCT",
        "ä¸­è¯500": "PRODUCT",
        
        # ä»·æ ¼æ•°å€¼
        "8760å…ƒ/å¨": "PRICE_VALUE", "2.3%": "PRICE_VALUE", 
        "15847æ‰‹": "PRICE_VALUE", "15%": "PRICE_VALUE",
        
        # æ—¶é—´
        "2024å¹´3æœˆ": "TIME", "21:00-23:00": "TIME", "å¤œç›˜": "TIME"
    }

def extract_ground_truth_entities(text: str, ground_truth_mapping: Dict) -> List[Tuple]:
    """ä»æ–‡æœ¬ä¸­æå–æ ‡å‡†ç­”æ¡ˆå®ä½“"""
    entities = []
    for entity_text, entity_type in ground_truth_mapping.items():
        start_pos = 0
        while True:
            pos = text.find(entity_text, start_pos)
            if pos == -1:
                break
            entities.append((entity_text, entity_type, pos, pos + len(entity_text)))
            start_pos = pos + 1
    
    entities = list(set(entities))
    entities.sort(key=lambda x: x[2])
    return entities

def calculate_performance_metrics(predicted_entities: List[Tuple], 
                                true_entities: List[Tuple]) -> Dict:
    """è®¡ç®—æ€§èƒ½æŒ‡æ ‡"""
    pred_set = set([(ent[0], ent[1]) for ent in predicted_entities])
    true_set = set([(ent[0], ent[1]) for ent in true_entities])
    
    tp = len(pred_set & true_set)
    fp = len(pred_set - true_set)
    fn = len(true_set - pred_set)
    
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
    
    # æŒ‰å®ä½“ç±»å‹åˆ†æ
    entity_type_metrics = {}
    all_types = set([ent[1] for ent in true_entities] + [ent[1] for ent in predicted_entities])
    
    for entity_type in all_types:
        true_type = set([(ent[0], ent[1]) for ent in true_entities if ent[1] == entity_type])
        pred_type = set([(ent[0], ent[1]) for ent in predicted_entities if ent[1] == entity_type])
        
        tp_type = len(pred_type & true_type)
        fp_type = len(pred_type - true_type)
        fn_type = len(true_type - pred_type)
        
        precision_type = tp_type / (tp_type + fp_type) if (tp_type + fp_type) > 0 else 0.0
        recall_type = tp_type / (tp_type + fn_type) if (tp_type + fn_type) > 0 else 0.0
        f1_type = 2 * precision_type * recall_type / (precision_type + recall_type) if (precision_type + recall_type) > 0 else 0.0
        
        entity_type_metrics[entity_type] = {
            'precision': precision_type,
            'recall': recall_type, 
            'f1': f1_type,
            'tp': tp_type,
            'fp': fp_type,
            'fn': fn_type,
            'support': len(true_type)
        }
    
    return {
        'overall': {
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'tp': tp,
            'fp': fp,
            'fn': fn,
            'total_true': len(true_entities),
            'total_pred': len(predicted_entities)
        },
        'by_entity_type': entity_type_metrics
    }

# %% [markdown]
# ## 5. æ‰§è¡ŒçœŸå®æ€§èƒ½æµ‹è¯•

def run_micro_improvement_benchmark():
    """è¿è¡Œå¾®æ”¹è¿›åŸºå‡†æµ‹è¯•"""
    
    methods = {
        "original_method": {
            "instance": OriginalRegexNERMethod(),
            "description": "åŸå§‹RegExæ–¹æ³•"
        },
        "micro_improved": {
            "instance": MicroImprovedRegexNER(), 
            "description": "å¾®æ”¹è¿›RegExæ–¹æ³•"
        }
    }
    
    results = {}
    ground_truth_mapping = create_ground_truth_mapping()
    
    print(f"\nğŸš€ å¼€å§‹å¾®æ”¹è¿›åŸºå‡†æµ‹è¯•")
    print("=" * 80)
    print(f"ğŸ“Š æµ‹è¯•æ–‡æœ¬æ•°é‡: {len(test_texts)}")
    print(f"ğŸ¯ æµ‹è¯•æ–¹æ³•æ•°é‡: {len(methods)}")
    print("=" * 80)
    
    for method_name, method_config in methods.items():
        print(f"\nğŸ“Š æµ‹è¯•æ–¹æ³•: {method_config['description']}")
        print("-" * 60)
        
        method_instance = method_config["instance"]
        
        print("âš¡ å¼€å§‹æ€§èƒ½æµ‹è¯•...")
        start_time = time.time()
        detailed_results = []
        processing_times = []
        
        for i, text in enumerate(test_texts):
            text_start = time.time()
            
            predicted_entities = method_instance.recognize_entities(text)
            
            text_time = time.time() - text_start
            processing_times.append(text_time)
            
            true_entities = extract_ground_truth_entities(text, ground_truth_mapping)
            
            metrics = calculate_performance_metrics(predicted_entities, true_entities)
            
            detailed_results.append({
                "text_id": i,
                "text": text,
                "entities": predicted_entities,
                "true_entities": true_entities,
                "processing_time": text_time,
                "metrics": metrics
            })
            
            if (i + 1) % 20 == 0:
                print(f"  å·²å¤„ç†: {i + 1}/{len(test_texts)} ä¸ªæ–‡æœ¬")
        
        total_time = time.time() - start_time
        
        # æ±‡æ€»æŒ‡æ ‡
        all_predicted = []
        all_true = []
        for result in detailed_results:
            all_predicted.extend(result['entities'])
            all_true.extend(result['true_entities'])
        
        overall_metrics = calculate_performance_metrics(all_predicted, all_true)
        
        results[method_name] = {
            "description": method_config["description"],
            "total_time": total_time,
            "avg_time_per_text": total_time / len(test_texts),
            "texts_per_second": len(test_texts) / total_time,
            "overall_metrics": overall_metrics,
            "detailed_results": detailed_results
        }
        
        overall = results[method_name]['overall_metrics']['overall']
        
        print(f"âœ… æµ‹è¯•å®Œæˆ!")
        print(f"âš¡ å¹³å‡å»¶è¿Ÿ: {results[method_name]['avg_time_per_text']*1000:.1f}æ¯«ç§’")
        print(f"ğŸ¯ æ•´ä½“F1åˆ†æ•°: {overall['f1']:.3f}")
        print(f"ğŸ“Š ç²¾ç¡®ç‡: {overall['precision']:.3f}")
        print(f"ğŸ“‹ å¬å›ç‡: {overall['recall']:.3f}")
    
    return results

# æ‰§è¡Œæµ‹è¯•
print("ğŸ”„ å¼€å§‹æ‰§è¡Œå¾®æ”¹è¿›æ€§èƒ½æµ‹è¯•...")
micro_improvement_results = run_micro_improvement_benchmark()

# %% [markdown]
# ## 6. ç»“æœåˆ†æ

def analyze_micro_improvement_results(results):
    """åˆ†æå¾®æ”¹è¿›ç»“æœ"""
    
    print(f"\nğŸ“ˆ å¾®æ”¹è¿›æ€§èƒ½åˆ†æ")
    print("=" * 80)
    
    # åˆ›å»ºå¯¹æ¯”è¡¨
    comparison_data = []
    
    for method_name, result in results.items():
        overall_metrics = result['overall_metrics']['overall']
        
        comparison_data.append({
            "æ–¹æ³•": method_name,
            "æè¿°": result["description"], 
            "å¹³å‡å»¶è¿Ÿ(æ¯«ç§’)": result['avg_time_per_text'] * 1000,
            "å¤„ç†é€Ÿåº¦(æ–‡æœ¬/ç§’)": result['texts_per_second'],
            "ç²¾ç¡®ç‡": overall_metrics['precision'],
            "å¬å›ç‡": overall_metrics['recall'],
            "F1åˆ†æ•°": overall_metrics['f1'],
            "çœŸæ­£ä¾‹": overall_metrics['tp'],
            "å‡æ­£ä¾‹": overall_metrics['fp'],
            "å‡è´Ÿä¾‹": overall_metrics['fn'],
            "é¢„æµ‹å®ä½“æ€»æ•°": overall_metrics['total_pred'],
            "çœŸå®å®ä½“æ€»æ•°": overall_metrics['total_true']
        })
    
    df = pd.DataFrame(comparison_data)
    
    print(f"ğŸ“Š å¾®æ”¹è¿›æ–¹æ³•æ€§èƒ½å¯¹æ¯”è¡¨")
    print("-" * 80)
    print(df.round(3).to_string(index=False)) 
    
    # è®¡ç®—æ”¹è¿›æ•ˆæœ
    original_metrics = results['original_method']['overall_metrics']['overall']
    improved_metrics = results['micro_improved']['overall_metrics']['overall']
    
    print(f"\nğŸ“ˆ å¾®æ”¹è¿›æ•ˆæœåˆ†æ")
    print("-" * 50)
    
    f1_change = improved_metrics['f1'] - original_metrics['f1']
    precision_change = improved_metrics['precision'] - original_metrics['precision']
    recall_change = improved_metrics['recall'] - original_metrics['recall']
    
    print(f"F1åˆ†æ•°å˜åŒ–: {original_metrics['f1']:.3f} â†’ {improved_metrics['f1']:.3f} ({f1_change:+.3f})")
    print(f"ç²¾ç¡®ç‡å˜åŒ–: {original_metrics['precision']:.3f} â†’ {improved_metrics['precision']:.3f} ({precision_change:+.3f})")
    print(f"å¬å›ç‡å˜åŒ–: {original_metrics['recall']:.3f} â†’ {improved_metrics['recall']:.3f} ({recall_change:+.3f})")
    
    # åˆ†æå…·ä½“æ”¹è¿›ç±»å‹
    print(f"\nğŸ” å…·ä½“æ”¹è¿›åˆ†æ:")
    
    # æŒ‰ç±»åˆ«åˆ†ææ”¹è¿›æ•ˆæœ
    category_improvements = {}
    
    for category, texts in categorized_tests.items():
        original_scores = []
        improved_scores = []
        
        for text in texts:
            # æ‰¾åˆ°å¯¹åº”çš„ç»“æœ
            original_result = None
            improved_result = None
            
            for result in results['original_method']['detailed_results']:
                if result['text'] == text:
                    original_result = result
                    break
            
            for result in results['micro_improved']['detailed_results']:
                if result['text'] == text:
                    improved_result = result
                    break
            
            if original_result and improved_result:
                original_scores.append(original_result['metrics']['overall']['f1'])
                improved_scores.append(improved_result['metrics']['overall']['f1'])
        
        if original_scores and improved_scores:
            avg_original = np.mean(original_scores)
            avg_improved = np.mean(improved_scores)
            improvement = avg_improved - avg_original
            
            category_improvements[category] = {
                'original_avg_f1': avg_original,
                'improved_avg_f1': avg_improved,
                'improvement': improvement
            }
    
    # æ˜¾ç¤ºå„ç±»åˆ«æ”¹è¿›æƒ…å†µ
    for category, stats in category_improvements.items():
        improvement = stats['improvement']
        status = "âœ…" if improvement > 0 else "âš ï¸" if improvement < -0.05 else "â–"
        print(f"{status} {category}: F1 {stats['original_avg_f1']:.3f} â†’ {stats['improved_avg_f1']:.3f} ({improvement:+.3f})")
    
    # æ˜¾ç¤ºæœ€ä½³æ”¹è¿›æ¡ˆä¾‹
    print(f"\nğŸ¯ æœ€æ˜¾è‘—æ”¹è¿›æ¡ˆä¾‹:")
    
    improvement_cases = []
    for i, text in enumerate(test_texts):
        original_result = results['original_method']['detailed_results'][i]
        improved_result = results['micro_improved']['detailed_results'][i]
        
        original_f1 = original_result['metrics']['overall']['f1']
        improved_f1 = improved_result['metrics']['overall']['f1']
        
        if improved_f1 > original_f1 + 0.1:  # æ˜¾è‘—æ”¹è¿›
            improvement_cases.append({
                'text': text,
                'original_f1': original_f1,
                'improved_f1': improved_f1,
                'improvement': improved_f1 - original_f1,
                'original_entities': original_result['entities'],
                'improved_entities': improved_result['entities']
            })
    
    # æŒ‰æ”¹è¿›å¹…åº¦æ’åº
    improvement_cases.sort(key=lambda x: x['improvement'], reverse=True)
    
    for i, case in enumerate(improvement_cases[:3]):  # æ˜¾ç¤ºå‰3ä¸ª
        print(f"\n{i+1}. æ–‡æœ¬: {case['text']}")
        print(f"   F1æ”¹è¿›: {case['original_f1']:.3f} â†’ {case['improved_f1']:.3f} (+{case['improvement']:.3f})")
        print(f"   åŸå§‹è¯†åˆ«: {len(case['original_entities'])} ä¸ªå®ä½“")
        print(f"   æ”¹è¿›è¯†åˆ«: {len(case['improved_entities'])} ä¸ªå®ä½“")
    
    return df, results

# åˆ†æç»“æœ
df_micro_results, micro_details = analyze_micro_improvement_results(micro_improvement_results)

# ä¿å­˜ç»“æœ
def save_micro_improvement_results(results, df):
    """ä¿å­˜å¾®æ”¹è¿›æµ‹è¯•ç»“æœ"""
    
    print(f"\nğŸ’¾ ä¿å­˜å¾®æ”¹è¿›æµ‹è¯•ç»“æœ")
    print("=" * 50)
    
    clean_results = {}
    for method_name, result in results.items():
        clean_result = {
            "description": result["description"],
            "total_time": result["total_time"],
            "avg_time_per_text": result["avg_time_per_text"],
            "texts_per_second": result["texts_per_second"],
            "overall_metrics": result["overall_metrics"]
        }
        clean_results[method_name] = clean_result
    
    with open("micro_improvement_results.json", "w", encoding="utf-8") as f:
        json.dump(clean_results, f, ensure_ascii=False, indent=2)
    
    if not df.empty:
        df.to_csv("micro_improvement_comparison.csv", index=False, encoding="utf-8")
    
    print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: micro_improvement_results.json")
    print(f"âœ… å¯¹æ¯”è¡¨å·²ä¿å­˜åˆ°: micro_improvement_comparison.csv")

# ä¿å­˜ç»“æœ
save_micro_improvement_results(micro_improvement_results, df_micro_results)

# æ€»ç»“
print(f"\nğŸ‰ **å¾®æ”¹è¿›æµ‹è¯•å®Œæˆ!**")
print("=" * 80)

if micro_details:
    original_metrics = micro_details['original_method']['overall_metrics']['overall']
    improved_metrics = micro_details['micro_improved']['overall_metrics']['overall']
    
    print(f"ğŸ† **æµ‹è¯•ç»“è®º**:")
    print(f"ğŸ“Š åœ¨{len(test_texts)}ä¸ªæµ‹è¯•æ–‡æœ¬ä¸Šå¯¹æ¯”äº†åŸå§‹æ–¹æ³•å’Œå¾®æ”¹è¿›æ–¹æ³•")
    
    print(f"\nğŸ“ˆ **æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡**:")
    print(f"ğŸ¯ F1åˆ†æ•°: {original_metrics['f1']:.3f} â†’ {improved_metrics['f1']:.3f}")
    print(f"ğŸ“Š ç²¾ç¡®ç‡: {original_metrics['precision']:.3f} â†’ {improved_metrics['precision']:.3f}")  
    print(f"ğŸ“‹ å¬å›ç‡: {original_metrics['recall']:.3f} â†’ {improved_metrics['recall']:.3f}")
    
    f1_change = improved_metrics['f1'] - original_metrics['f1']
    precision_change = improved_metrics['precision'] - original_metrics['precision']
    recall_change = improved_metrics['recall'] - original_metrics['recall']
    
    print(f"\nğŸ’¡ **æ”¹è¿›è¯„ä¼°**:")
    if f1_change > 0:
        print(f"âœ… æ•´ä½“æ€§èƒ½æå‡: F1åˆ†æ•°æå‡ {f1_change:.3f}")
    else:
        print(f"âš ï¸ æ•´ä½“æ€§èƒ½å˜åŒ–: F1åˆ†æ•°å˜åŒ– {f1_change:.3f}")
        
    if precision_change > 0:
        print(f"âœ… ç²¾ç¡®ç‡æå‡: +{precision_change:.3f} (å‡å°‘è¯¯è¯†åˆ«)")
    else:
        print(f"âš ï¸ ç²¾ç¡®ç‡å˜åŒ–: {precision_change:.3f}")
        
    if recall_change >= -0.05:  # å°å¹…ä¸‹é™å¯æ¥å—
        print(f"âœ… å¬å›ç‡ä¿æŒç¨³å®š: {recall_change:.3f}")
    else:
        print(f"âš ï¸ å¬å›ç‡ä¸‹é™: {recall_change:.3f}")