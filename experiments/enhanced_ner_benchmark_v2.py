import re
import time
import json
import copy
from typing import Dict, List, Tuple, Set, Optional
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict, Counter

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")

print("ğŸ”§ å¾®å¾®æ”¹è¿›NERç³»ç»Ÿ - å®Œæ•´ç‰ˆå®ç°")
print("=" * 80)

class OriginalRegexNERMethod:
    """åŸå§‹RegExå®ä½“è¯†åˆ«æ–¹æ³•ï¼ˆä¿æŒä¸å˜ï¼‰"""
    
    def __init__(self):
        self.entity_patterns = {
            "EXCHANGE": [
                r"éƒ‘å•†æ‰€|éƒ‘å·å•†å“äº¤æ˜“æ‰€|CZCE|czce",
                r"ä¸ŠæœŸæ‰€|ä¸Šæµ·æœŸè´§äº¤æ˜“æ‰€|SHFE|shfe", 
                r"å¤§å•†æ‰€|å¤§è¿å•†å“äº¤æ˜“æ‰€|DCE|dce",
                r"ä¸­é‡‘æ‰€|ä¸­å›½é‡‘èæœŸè´§äº¤æ˜“æ‰€|CFFEX|cffex|CFE",
                r"ä¸ŠæœŸèƒ½æº|ä¸Šæµ·å›½é™…èƒ½æºäº¤æ˜“ä¸­å¿ƒ|INE|ine",
                r"å¹¿æœŸæ‰€|å¹¿å·æœŸè´§äº¤æ˜“æ‰€|GFEX|gfex"
            ],
            "FUTURES_COMPANY": [
                r"åæ³°æœŸè´§(?:æœ‰é™å…¬å¸)?",
                r"ä¸­ä¿¡æœŸè´§(?:æœ‰é™å…¬å¸)?",
                r"æ°¸å®‰æœŸè´§(?:è‚¡ä»½æœ‰é™å…¬å¸)?",
                r"å›½æ³°å›å®‰æœŸè´§(?:æœ‰é™å…¬å¸)?",
                r"æµ·é€šæœŸè´§(?:æœ‰é™å…¬å¸)?",
                r"æ–¹æ­£ä¸­æœŸæœŸè´§(?:æœ‰é™å…¬å¸)?",
                r"å…‰å¤§æœŸè´§(?:æœ‰é™å…¬å¸)?",
                r"é“¶æ²³æœŸè´§(?:æœ‰é™å…¬å¸)?",
                r"æ‹›å•†æœŸè´§(?:æœ‰é™å…¬å¸)?",
                r"å¹¿å‘æœŸè´§(?:æœ‰é™å…¬å¸)?"
            ],
            "PRODUCT": [
                r"è‹¹æœ|AP|ap",
                r"æ£‰èŠ±|CF|cf", 
                r"ç™½ç³–|SR|sr",
                r"PTA|TA|ta",
                r"ç”²é†‡|MA|ma",
                r"ç»ç’ƒ|FG|fg",
                r"é“œ|CU|cu",
                r"é“|AL|al", 
                r"èºçº¹é’¢|RB|rb",
                r"çƒ­è½§å·æ¿|HC|hc",
                r"è±†ç²•|M|(?<!A)m(?!a)",
                r"è±†æ²¹|Y|(?<!C)y",
                r"ç‰ç±³|C|(?<!D)c(?!u)",
                r"è±†ä¸€|A|(?<!T)a(?!l)",
                r"é“çŸ¿çŸ³|I|(?<!F)i(?!f)",
                r"ç„¦ç‚­|J|(?<!D)j",
                r"ç„¦ç…¤|JM|jm",
                r"æ²ªæ·±300|IF|if",
                r"ä¸Šè¯50|IH|ih", 
                r"ä¸­è¯500|IC|ic"
            ],
            "CONTRACT_CODE": [
                r"(?:AP|ap)(?:24|25)\d{2}",
                r"(?:CU|cu)(?:24|25)\d{2}",
                r"(?:M|(?<!A)m(?!a))(?:24|25)\d{2}",
                r"(?:SR|sr)(?:24|25)\d{2}",
                r"(?:TA|ta)(?:24|25)\d{2}",
                r"(?:RB|rb)(?:24|25)\d{2}",
                r"(?:I|(?<!F)i(?!f))(?:24|25)\d{2}",
                r"(?:IF|if)(?:24|25)\d{2}",
                r"(?:AL|al)(?:24|25)\d{2}",
                r"(?:IC|ic)(?:24|25)\d{2}",
            ],
            "PRICE_VALUE": [
                r"\d+(?:,\d{3})*\.?\d*å…ƒ/å¨",
                r"\d+(?:\.\d+)?%",
                r"\d+(?:,\d{3})*æ‰‹",
                r"\d+(?:\.\d+)?ä¸‡å¨",
                r"\d+(?:,\d{3})*(?:\.\d+)?äº¿å…ƒ"
            ],
            "TIME": [
                r"\d{4}å¹´\d{1,2}æœˆ(?:\d{1,2}æ—¥)?",
                r"\d{4}å¹´Q[1-4]",
                r"\d{1,2}:\d{2}-\d{1,2}:\d{2}",
                r"å¤œç›˜|æ”¶ç›˜|å¼€ç›˜"
            ]
        }
        self.compiled_patterns = self._compile_patterns()
        
    def _compile_patterns(self):
        compiled = {}
        for entity_type, patterns in self.entity_patterns.items():
            compiled[entity_type] = []
            for pattern in patterns:
                try:
                    compiled[entity_type].append(re.compile(pattern, re.IGNORECASE))
                except re.error as e:
                    print(f"è­¦å‘Š: ç¼–è¯‘æ¨¡å¼å¤±è´¥ {pattern}: {e}")
        return compiled
        
    def recognize_entities(self, text: str) -> List[Tuple[str, str, int, int]]:
        entities = []
        for entity_type, patterns in self.compiled_patterns.items():
            for pattern in patterns:
                for match in pattern.finditer(text):
                    entity_text = match.group()
                    start_pos = match.start()
                    end_pos = match.end()
                    
                    is_duplicate = any(
                        start_pos == existing[2] and end_pos == existing[3]
                        for existing in entities
                    )
                    
                    if not is_duplicate:
                        entities.append((entity_text, entity_type, start_pos, end_pos))
        
        entities.sort(key=lambda x: x[2])
        return entities


class OptimizedMicroImprovedRegexNER:
    """ä¼˜åŒ–çš„å¾®å¾®æ”¹è¿›RegExæ–¹æ³• - è§£å†³åŸç‰ˆé—®é¢˜å¹¶æå‡æ€§èƒ½"""
    
    def __init__(self):
        # ä½¿ç”¨åŸå§‹æ–¹æ³•ä½œä¸ºåŸºç¡€
        self.original_method = OriginalRegexNERMethod()
        
        # ğŸ”§ æ”¹è¿›é…ç½® - æ›´ç²¾ç»†çš„æ§åˆ¶
        self.improvement_config = {
            'deduplicate_overlapping': True,
            'filter_false_positives': True,
            'enhance_contract_formats': True,
            'smart_entity_merging': False,  # é»˜è®¤å…³é—­ï¼Œé¿å…è¿‡åº¦å¤æ‚åŒ–
            'conservative_filtering': True   # ğŸ†• ä¿å®ˆè¿‡æ»¤æ¨¡å¼
        }
        
        # ğŸ¯ é¢„ç¼–è¯‘çš„ä¸Šä¸‹æ–‡æ¨¡å¼ - ä¼˜åŒ–æ€§èƒ½
        self.context_patterns = self._compile_context_patterns()
        
        # ğŸ“Š å®ä½“ä¼˜å…ˆçº§æ˜ å°„ - ç”¨äºå†²çªè§£å†³
        self.entity_priorities = {
            'CONTRACT_CODE': 10,
            'EXCHANGE': 9,
            'FUTURES_COMPANY': 8,
            'PRODUCT': 7,
            'PRICE_VALUE': 6,
            'TIME': 5
        }
    
    def _compile_context_patterns(self):
        """é¢„ç¼–è¯‘ä¸Šä¸‹æ–‡æ¨¡å¼ä»¥æå‡æ€§èƒ½"""
        patterns = {
            # ğŸ” å¼ºæœŸè´§æŒ‡ç¤ºè¯ - å‡ºç°è¿™äº›è¯æ—¶ä¼˜å…ˆä¿ç•™å®ä½“
            'strong_futures_positive': re.compile(
                r'æœŸè´§|åˆçº¦|äº¤æ˜“æ‰€|äº¤å‰²|ä¿è¯é‡‘|æŒä»“|å¼€ä»“|å¹³ä»“|å¤œç›˜|ä¸»åŠ›|æ”¶ç›˜|å¼€ç›˜', 
                re.IGNORECASE
            ),
            
            # âš ï¸ å¼ºéæœŸè´§æŒ‡ç¤ºè¯ - å‡ºç°è¿™äº›è¯æ—¶è°¨æ…è¿‡æ»¤
            'strong_non_futures': re.compile(
                r'å…¬å¸è‚¡ä»·|æ‰‹æœº|iPhone|è½¯ä»¶|åº”ç”¨|åŸºé‡‘å‡€å€¼|åŸºé‡‘æ”¶ç›Š', 
                re.IGNORECASE
            ),
            
            # ğŸ¯ æœŸè´§ç›¸å…³ä¸Šä¸‹æ–‡è¯æ±‡
            'futures_context': re.compile(
                r'ä»·æ ¼|èµ°åŠ¿|åˆ†æ|è¡Œæƒ…|æŠ•èµ„|ç­–ç•¥|ç ”ç©¶|æŠ¥å‘Š|å¸‚åœº', 
                re.IGNORECASE
            ),
            
            # ğŸš« æ˜ç¡®çš„éæœŸè´§ä¸Šä¸‹æ–‡
            'non_futures_context': re.compile(
                r'è‚¡ç¥¨|è¯åˆ¸(?!.*æœŸè´§)|åŸºé‡‘(?!.*æœŸè´§)|å€ºåˆ¸(?!.*æœŸè´§)', 
                re.IGNORECASE
            )
        }
        return patterns
    
    def _get_context_score(self, text: str, start_pos: int, end_pos: int, 
                          window_size: int = 25) -> Dict[str, int]:
        """ğŸ§  æ™ºèƒ½ä¸Šä¸‹æ–‡åˆ†æ - è¿”å›è¯¦ç»†çš„ä¸Šä¸‹æ–‡å¾—åˆ†"""
        context_start = max(0, start_pos - window_size)
        context_end = min(len(text), end_pos + window_size)
        context = text[context_start:context_end]
        
        scores = {}
        for pattern_name, pattern in self.context_patterns.items():
            matches = len(pattern.findall(context))
            scores[pattern_name] = matches
        
        return scores
    
    def _calculate_entity_confidence(self, entity: Tuple, text: str) -> float:
        """ğŸ¯ è®¡ç®—å®ä½“çš„ç½®ä¿¡åº¦å¾—åˆ†"""
        entity_text, entity_type, start_pos, end_pos = entity
        confidence = 0.0
        
        # 1. åŸºç¡€ç±»å‹ç½®ä¿¡åº¦
        base_confidence = self.entity_priorities.get(entity_type, 5) * 0.1
        confidence += base_confidence
        
        # 2. é•¿åº¦ç½®ä¿¡åº¦ - æ›´å®Œæ•´çš„å®ä½“æ›´å¯ä¿¡
        length_confidence = min(len(entity_text) * 0.05, 0.3)
        confidence += length_confidence
        
        # 3. ä¸Šä¸‹æ–‡ç½®ä¿¡åº¦
        context_scores = self._get_context_score(text, start_pos, end_pos)
        
        # å¼ºæœŸè´§ä¸Šä¸‹æ–‡å¤§å¹…æå‡ç½®ä¿¡åº¦
        if context_scores['strong_futures_positive'] > 0:
            confidence += 0.5
        
        # æœŸè´§ç›¸å…³ä¸Šä¸‹æ–‡é€‚åº¦æå‡ç½®ä¿¡åº¦
        if context_scores['futures_context'] > 0:
            confidence += 0.2
        
        # å¼ºéæœŸè´§ä¸Šä¸‹æ–‡é™ä½ç½®ä¿¡åº¦
        if context_scores['strong_non_futures'] > 0:
            confidence -= 0.4
        
        # éæœŸè´§ä¸Šä¸‹æ–‡é€‚åº¦é™ä½ç½®ä¿¡åº¦
        if context_scores['non_futures_context'] > 0:
            confidence -= 0.2
        
        # 4. æ ¼å¼è§„èŒƒæ€§ç½®ä¿¡åº¦
        if entity_type == 'CONTRACT_CODE':
            if re.match(r'^[A-Z]{1,3}\d{4}$', entity_text):
                confidence += 0.2
            elif re.match(r'^[a-z]{1,3}\d{4}$', entity_text):
                confidence += 0.15
        
        return max(0.0, min(1.0, confidence))  # é™åˆ¶åœ¨0-1ä¹‹é—´
    
    def _optimized_deduplicate_overlapping(self, entities: List[Tuple], text: str) -> List[Tuple]:
        """âš¡ ä¼˜åŒ–çš„é‡å å®ä½“å»é‡ - O(n log n)å¤æ‚åº¦"""
        if len(entities) <= 1:
            return entities
        
        # æŒ‰ä½ç½®æ’åº
        entities.sort(key=lambda x: x[2])
        
        # è®¡ç®—æ¯ä¸ªå®ä½“çš„ç½®ä¿¡åº¦
        entity_confidences = [(entity, self._calculate_entity_confidence(entity, text)) 
                             for entity in entities]
        
        deduplicated = []
        i = 0
        
        while i < len(entity_confidences):
            current_entity, current_confidence = entity_confidences[i]
            current_text, current_type, current_start, current_end = current_entity
            
            # æŸ¥æ‰¾ä¸å½“å‰å®ä½“é‡å çš„æ‰€æœ‰å®ä½“
            overlapping_entities = [(current_entity, current_confidence)]
            j = i + 1
            
            while j < len(entity_confidences) and entity_confidences[j][0][2] < current_end:
                candidate_entity, candidate_confidence = entity_confidences[j]
                candidate_start = candidate_entity[2]
                candidate_end = candidate_entity[3]
                
                # æ£€æŸ¥æ˜¯å¦çœŸæ­£é‡å ï¼ˆä¸ä»…ä»…æ˜¯ç›¸é‚»ï¼‰
                if candidate_start < current_end:
                    overlapping_entities.append((candidate_entity, candidate_confidence))
                
                j += 1
            
            # å¦‚æœæœ‰é‡å ï¼Œé€‰æ‹©ç½®ä¿¡åº¦æœ€é«˜çš„
            if len(overlapping_entities) > 1:
                best_entity, best_confidence = max(overlapping_entities, key=lambda x: x[1])
                deduplicated.append(best_entity)
                
                # è·³è¿‡æ‰€æœ‰è¢«åˆå¹¶çš„å®ä½“
                i = j
            else:
                deduplicated.append(current_entity)
                i += 1
        
        return deduplicated
    
    def _conservative_false_positive_filter(self, entities: List[Tuple], text: str) -> List[Tuple]:
        """ğŸ›¡ï¸ ä¿å®ˆçš„è¯¯æŠ¥è¿‡æ»¤ - è§£å†³åŸç‰ˆè¿‡åº¦è¿‡æ»¤é—®é¢˜"""
        if not entities:
            return entities
        
        filtered = []
        
        for entity in entities:
            entity_text, entity_type, start_pos, end_pos = entity
            
            # è®¡ç®—å®ä½“ç½®ä¿¡åº¦
            confidence = self._calculate_entity_confidence(entity, text)
            context_scores = self._get_context_score(text, start_pos, end_pos)
            
            should_keep = True
            
            # ğŸ¯ åˆ†å±‚è¿‡æ»¤ç­–ç•¥
            if entity_type == 'CONTRACT_CODE':
                # åˆçº¦ä»£ç ï¼šåªæœ‰åœ¨æåº¦è´Ÿé¢çš„ä¸Šä¸‹æ–‡ä¸­æ‰è¿‡æ»¤
                if (context_scores['strong_non_futures'] >= 2 and 
                    context_scores['strong_futures_positive'] == 0):
                    should_keep = False
            
            elif entity_type == 'EXCHANGE':
                # äº¤æ˜“æ‰€ï¼šå‡ ä¹ä¸è¿‡æ»¤ï¼Œå› ä¸ºè¯¯æŠ¥ç‡å¾ˆä½
                if (context_scores['strong_non_futures'] >= 3 and 
                    context_scores['strong_futures_positive'] == 0):
                    should_keep = False
            
            elif entity_type == 'FUTURES_COMPANY':
                # æœŸè´§å…¬å¸ï¼šé€‚åº¦è¿‡æ»¤
                if (context_scores['strong_non_futures'] >= 1 and 
                    context_scores['strong_futures_positive'] == 0 and
                    confidence < 0.4):
                    should_keep = False
            
            elif entity_type == 'PRODUCT':
                # ğŸ å•†å“åç§°ï¼šæœ€å®¹æ˜“è¯¯æŠ¥ï¼Œéœ€è¦æ›´è°¨æ…çš„è¿‡æ»¤
                if context_scores['strong_non_futures'] >= 1:
                    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æœŸè´§ä¸Šä¸‹æ–‡æ¥æŠµæ¶ˆè´Ÿé¢ä¿¡å·
                    positive_signal = (context_scores['strong_futures_positive'] + 
                                     context_scores['futures_context'] * 0.5)
                    negative_signal = context_scores['strong_non_futures']
                    
                    if positive_signal < negative_signal:
                        should_keep = False
                else:
                    # æ²¡æœ‰æ˜ç¡®è´Ÿé¢ä¿¡å·æ—¶ï¼Œä¿å®ˆä¿ç•™
                    should_keep = True
            
            # ğŸ”’ æœ€ç»ˆå®‰å…¨æ£€æŸ¥ï¼šå¦‚æœç½®ä¿¡åº¦å¾ˆé«˜ï¼Œå¼ºåˆ¶ä¿ç•™
            if confidence >= 0.7:
                should_keep = True
            
            if should_keep:
                filtered.append(entity)
        
        return filtered
    
    def _enhanced_contract_format_recognition(self, entities: List[Tuple], text: str) -> List[Tuple]:
        """ğŸ“ å¢å¼ºçš„åˆçº¦æ ¼å¼è¯†åˆ« - æ”¯æŒæ›´å¤šå˜ä½“"""
        enhanced = entities.copy()  # æµ…æ‹·è´æå‡æ€§èƒ½
        
        # ğŸ”¤ æ‰©å±•çš„åˆçº¦ä»£ç å˜ä½“æ¨¡å¼
        variant_patterns = [
            (r'\b([A-Z]{1,3})\s*[-_./]\s*(\d{4})\b', r'\1\2'),      # AP-2405 -> AP2405
            (r'\b([A-Z]{1,3})\s+(\d{4})\b', r'\1\2'),               # AP 2405 -> AP2405
            (r'\b([A-Z]{1,3})\s*\(\s*(\d{4})\s*\)\b', r'\1\2'),     # AP(2405) -> AP2405
            (r'\b([A-Z]{1,3})\s*(\d{2})\s*(\d{2})\b', r'\1\2\3'),   # AP 24 05 -> AP2405
        ]
        
        # ğŸ¯ æœ‰æ•ˆåˆçº¦ä»£ç åˆ—è¡¨ï¼ˆæ‰©å±•ç‰ˆï¼‰
        valid_codes = {
            'AP', 'CU', 'SR', 'TA', 'MA', 'RB', 'IF', 'AL', 'IC', 'M', 'Y', 'C', 'A', 
            'I', 'J', 'JM', 'IH', 'FG', 'CF', 'ZC', 'HC', 'NI', 'ZN', 'PB', 'SN', 
            'AU', 'AG', 'RU', 'BU', 'FU', 'SC', 'LU', 'NR', 'SP', 'SS', 'WH', 'PM',
            'RI', 'LR', 'JR', 'CY', 'RS', 'OI', 'RM', 'CJ', 'PK', 'SF', 'SM', 'PF'
        }
        
        for pattern_regex, replacement in variant_patterns:
            pattern = re.compile(pattern_regex, re.IGNORECASE)
            
            for match in pattern.finditer(text):
                # æ„å»ºæ ‡å‡†æ ¼å¼çš„åˆçº¦ä»£ç 
                if len(match.groups()) == 2:
                    code_part = match.group(1).upper()
                    number_part = match.group(2)
                    standard_code = code_part + number_part
                elif len(match.groups()) == 3:  # å¤„ç† AP 24 05 æ ¼å¼
                    code_part = match.group(1).upper()
                    standard_code = code_part + match.group(2) + match.group(3)
                else:
                    continue
                
                # éªŒè¯åˆçº¦ä»£ç çš„æœ‰æ•ˆæ€§
                if (code_part in valid_codes and 
                    len(number_part if len(match.groups()) == 2 else match.group(2) + match.group(3)) == 4):
                    
                    start_pos = match.start()
                    end_pos = match.end()
                    
                    # ğŸ§  ä¸Šä¸‹æ–‡éªŒè¯ - åªåœ¨æœŸè´§ç›¸å…³ä¸Šä¸‹æ–‡ä¸­æ·»åŠ 
                    context_scores = self._get_context_score(text, start_pos, end_pos)
                    has_futures_context = (context_scores['strong_futures_positive'] > 0 or 
                                         context_scores['futures_context'] > 0)
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸ç°æœ‰å®ä½“é‡å 
                    is_covered = any(
                        not (end_pos <= existing[2] or start_pos >= existing[3])
                        for existing in enhanced
                    )
                    
                    if has_futures_context and not is_covered:
                        enhanced.append((
                            match.group(),  # ä¿ç•™åŸå§‹åŒ¹é…æ–‡æœ¬
                            'CONTRACT_CODE',
                            start_pos,
                            end_pos
                        ))
        
        return enhanced
    
    def recognize_entities(self, text: str) -> List[Tuple[str, str, int, int]]:
        """ğŸš€ ä¼˜åŒ–çš„å®ä½“è¯†åˆ«ä¸»æµç¨‹"""
        # ğŸ” ç¬¬ä¸€æ­¥ï¼šåŸºç¡€å®ä½“è¯†åˆ«
        entities = self.original_method.recognize_entities(text)
        
        # ğŸ“ ç¬¬äºŒæ­¥ï¼šå¢å¼ºåˆçº¦æ ¼å¼è¯†åˆ«
        if self.improvement_config.get('enhance_contract_formats', False):
            entities = self._enhanced_contract_format_recognition(entities, text)
        
        # ğŸ”„ ç¬¬ä¸‰æ­¥ï¼šä¼˜åŒ–å»é‡
        if self.improvement_config.get('deduplicate_overlapping', False):
            entities = self._optimized_deduplicate_overlapping(entities, text)
        
        # ğŸ›¡ï¸ ç¬¬å››æ­¥ï¼šä¿å®ˆè¯¯æŠ¥è¿‡æ»¤
        if self.improvement_config.get('filter_false_positives', False):
            entities = self._conservative_false_positive_filter(entities, text)
        
        # ğŸ¯ ç¬¬äº”æ­¥ï¼šæœ€ç»ˆæ’åºå’Œæ¸…ç†
        entities.sort(key=lambda x: x[2])
        return entities


def create_comprehensive_test_corpus():
    """ğŸ“š åˆ›å»ºå…¨é¢çš„æµ‹è¯•è¯­æ–™åº“"""
    
    # 1. æ ¸å¿ƒåˆçº¦ä»£ç æŸ¥è¯¢
    contract_code_queries = [
        "éƒ‘å•†æ‰€AP2405æœŸè´§æ€ä¹ˆæ ·äº†",
        "ä¸ŠæœŸæ‰€CU2405é“œæœŸè´§ä»·æ ¼å¦‚ä½•", 
        "å¤§å•†æ‰€M2405è±†ç²•æœŸè´§èµ°åŠ¿åˆ†æ",
        "ä¸­é‡‘æ‰€IF2405æœŸè´§ä»Šæ—¥è¡¨ç°",
        "éƒ‘å•†æ‰€SR2405ç™½ç³–æœŸè´§è¡Œæƒ…",
        "ä¸ŠæœŸæ‰€RB2405èºçº¹é’¢æœŸè´§æ•°æ®",
        "å¤§å•†æ‰€I2405é“çŸ¿çŸ³æœŸè´§æŠ¥ä»·",
        "éƒ‘å•†æ‰€TA2405PTAæœŸè´§åˆ†æ",
        "ä¸ŠæœŸæ‰€AL2405é“æœŸè´§èµ°åŠ¿",
        "ä¸­é‡‘æ‰€IC2405ä¸­è¯500æœŸè´§",
    ]
    
    # 2. å¸¦å…¬å¸ä¿¡æ¯çš„å¤æ‚æŸ¥è¯¢
    company_contract_queries = [
        "åæ³°æœŸè´§å¯¹éƒ‘å•†æ‰€AP2405æœŸè´§çš„åˆ†æ",
        "ä¸­ä¿¡æœŸè´§å‘å¸ƒä¸ŠæœŸæ‰€CU2405é“œæœŸè´§æŠ¥å‘Š",
        "æ°¸å®‰æœŸè´§å…³äºå¤§å•†æ‰€M2405è±†ç²•æœŸè´§å»ºè®®",
        "å›½æ³°å›å®‰æœŸè´§ç ”ç©¶ä¸­é‡‘æ‰€IF2405æœŸè´§",
        "æµ·é€šæœŸè´§è§£è¯»éƒ‘å•†æ‰€SR2405ç™½ç³–æœŸè´§",
        "æ–¹æ­£ä¸­æœŸæœŸè´§çœ‹å¥½ä¸ŠæœŸæ‰€RB2405èºçº¹é’¢",
        "å…‰å¤§æœŸè´§åˆ†æå¤§å•†æ‰€I2405é“çŸ¿çŸ³æœŸè´§",
        "é“¶æ²³æœŸè´§ç­–ç•¥éƒ‘å•†æ‰€TA2405PTAæœŸè´§",
        "æ‹›å•†æœŸè´§å»ºè®®ä¸ŠæœŸæ‰€AL2405é“æœŸè´§",
        "å¹¿å‘æœŸè´§è§£æä¸­é‡‘æ‰€IC2405æœŸè´§",
    ]
    
    # 3. é‡å¤å®ä½“é—®é¢˜æµ‹è¯•æ¡ˆä¾‹
    duplicate_issue_queries = [
        "è‹¹æœæœŸè´§AP2405è‹¹æœæœŸè´§ä»·æ ¼",
        "éƒ‘å•†æ‰€äº¤æ˜“æ‰€AP2405æœŸè´§",
        "AP2405æœŸè´§APè‹¹æœåˆ†æ", 
        "åæ³°æœŸè´§åæ³°æœŸè´§å…¬å¸ç ”ç©¶æ‰€åˆ†æAP2405",
        "èºçº¹é’¢é’¢æœŸè´§RB2405ä»·æ ¼æ³¢åŠ¨",
    ]
    
    # 4. è¯¯è¯†åˆ«é—®é¢˜æµ‹è¯•æ¡ˆä¾‹ï¼ˆå…³é”®æµ‹è¯•ç±»åˆ«ï¼‰
    false_positive_queries = [
        "è‹¹æœå…¬å¸è‚¡ä»·åˆ†ææŠ¥å‘Š",          # åº”è¿‡æ»¤è‹¹æœ
        "åæ³°è¯åˆ¸ç ”ç©¶å›¢é˜Ÿå‘å¸ƒæŠ¥å‘Š",      # åº”è¿‡æ»¤åæ³°
        "é“œä»·æ ¼èµ°åŠ¿åˆ†æå›¾è¡¨",            # å¯èƒ½ä¿ç•™é“œï¼ˆæœ‰æœŸè´§ä¸Šä¸‹æ–‡ï¼‰
        "ç™½ç³–ç°è´§å¸‚åœºè¡Œæƒ…ç ”ç©¶",          # å¯èƒ½ä¿ç•™ç™½ç³–ï¼ˆæœ‰å¸‚åœºä¸Šä¸‹æ–‡ï¼‰
        "è‹¹æœæ‰‹æœºé”€é‡æ•°æ®ç»Ÿè®¡",          # åº”è¿‡æ»¤è‹¹æœ
        "åæ³°è¯åˆ¸æŠ•èµ„é“¶è¡Œä¸šåŠ¡",          # åº”è¿‡æ»¤åæ³°
        "é“œåˆ¶å“åŠ å·¥å·¥ä¸šåˆ†æ",            # å¯èƒ½ä¿ç•™é“œ
        "ç™½ç³–é£Ÿå“æ·»åŠ å‰‚ç”¨é€”",            # åº”è¿‡æ»¤ç™½ç³–
        "æœŸè´§å¸‚åœºæ•´ä½“èµ°åŠ¿åˆ†æ",          # ä¸€èˆ¬æ€§æœŸè´§åˆ†æ
        "äº¤æ˜“æ‰€ç›‘ç®¡æ”¿ç­–è§£è¯»",            # ä¸€èˆ¬æ€§äº¤æ˜“æ‰€
    ]
    
    # 5. æ ¼å¼å˜ä½“æµ‹è¯•æ¡ˆä¾‹  
    format_variant_queries = [
        "éƒ‘å•†æ‰€AP-2405æœŸè´§åˆçº¦åˆ†æ",
        "ä¸ŠæœŸæ‰€CU 2405æœŸè´§äº¤æ˜“", 
        "å¤§å•†æ‰€M.2405è±†ç²•æœŸè´§ä»·æ ¼",
        "ä¸­é‡‘æ‰€IF/2405æœŸè´§èµ°åŠ¿",
        "éƒ‘å•†æ‰€SR_2405ç™½ç³–æœŸè´§",
        "ä¸ŠæœŸæ‰€RB(2405)èºçº¹é’¢æœŸè´§",
        "AP 24 05æœŸè´§ä»·æ ¼åˆ†æ",
        "CU-24-05é“œæœŸè´§èµ°åŠ¿",
        "IFæœŸè´§2405åˆçº¦",
        "M 2405 è±†ç²•æœŸè´§åˆ†æ",
    ]
    
    # 6. ğŸ†• è¾¹ç•Œæƒ…å†µæµ‹è¯•
    boundary_case_queries = [
        "è‹¹æœæœŸè´§å’Œè‹¹æœå…¬å¸è‚¡ä»·å¯¹æ¯”åˆ†æ",      # æ··åˆä¸Šä¸‹æ–‡
        "åæ³°è¯åˆ¸åæ³°æœŸè´§æ¯å­å…¬å¸å…³ç³»",        # æ··åˆå®ä½“  
        "é“œæœŸè´§ä»·æ ¼ä¸é“œç°è´§ä»·æ ¼ä»·å·®åˆ†æ",      # æœŸè´§vsç°è´§
        "ç™½ç³–æœŸè´§äº¤å‰²ä¸ç™½ç³–ç°è´§è´¸æ˜“",         # æ··åˆä¸šåŠ¡
        "AP2405è‹¹æœæœŸè´§ä¸»åŠ›åˆçº¦åˆ†ææŠ¥å‘Š",     # å®Œæ•´æœŸè´§è¡¨è¿°
    ]
    
    all_test_categories = {
        "æ ¸å¿ƒåˆçº¦ä»£ç æŸ¥è¯¢": contract_code_queries,
        "å…¬å¸ä¸åˆçº¦ç»„åˆ": company_contract_queries, 
        "é‡å¤å®ä½“é—®é¢˜": duplicate_issue_queries,
        "è¯¯è¯†åˆ«é—®é¢˜": false_positive_queries,
        "æ ¼å¼å˜ä½“æµ‹è¯•": format_variant_queries,
        "è¾¹ç•Œæƒ…å†µæµ‹è¯•": boundary_case_queries,  # ğŸ†• æ–°å¢ç±»åˆ«
    }
    
    # å±•å¹³æ‰€æœ‰æµ‹è¯•æ–‡æœ¬
    flat_tests = []
    for category, texts in all_test_categories.items():
        flat_tests.extend(texts)
    
    return flat_tests, all_test_categories


def create_ground_truth_mapping():
    """ğŸ¯ åˆ›å»ºæ ‡å‡†ç­”æ¡ˆæ˜ å°„"""
    return {
        # äº¤æ˜“æ‰€æ ‡å‡†ç­”æ¡ˆ
        "éƒ‘å•†æ‰€": "EXCHANGE", "ä¸ŠæœŸæ‰€": "EXCHANGE", "å¤§å•†æ‰€": "EXCHANGE",
        "ä¸­é‡‘æ‰€": "EXCHANGE", "CZCE": "EXCHANGE", "SHFE": "EXCHANGE",
        "DCE": "EXCHANGE", "CFFEX": "EXCHANGE", "äº¤æ˜“æ‰€": "EXCHANGE",
        
        # æœŸè´§å…¬å¸æ ‡å‡†ç­”æ¡ˆ
        "åæ³°æœŸè´§": "FUTURES_COMPANY", "ä¸­ä¿¡æœŸè´§": "FUTURES_COMPANY",
        "æ°¸å®‰æœŸè´§": "FUTURES_COMPANY", "å›½æ³°å›å®‰æœŸè´§": "FUTURES_COMPANY", 
        "æµ·é€šæœŸè´§": "FUTURES_COMPANY", "æ–¹æ­£ä¸­æœŸæœŸè´§": "FUTURES_COMPANY",
        "å…‰å¤§æœŸè´§": "FUTURES_COMPANY", "é“¶æ²³æœŸè´§": "FUTURES_COMPANY",
        "æ‹›å•†æœŸè´§": "FUTURES_COMPANY", "å¹¿å‘æœŸè´§": "FUTURES_COMPANY",
        
        # åˆçº¦ä»£ç æ ‡å‡†ç­”æ¡ˆï¼ˆåŒ…æ‹¬å˜ä½“æ ¼å¼ï¼‰
        "AP2405": "CONTRACT_CODE", "CU2405": "CONTRACT_CODE",
        "M2405": "CONTRACT_CODE", "SR2405": "CONTRACT_CODE", 
        "TA2405": "CONTRACT_CODE", "RB2405": "CONTRACT_CODE",
        "I2405": "CONTRACT_CODE", "IF2405": "CONTRACT_CODE",
        "AL2405": "CONTRACT_CODE", "IC2405": "CONTRACT_CODE",
        "AP-2405": "CONTRACT_CODE", "CU 2405": "CONTRACT_CODE",
        "M.2405": "CONTRACT_CODE", "IF/2405": "CONTRACT_CODE",
        "SR_2405": "CONTRACT_CODE", "RB(2405)": "CONTRACT_CODE",
        
        # å“ç§åç§°ï¼ˆéœ€è¦ä¸Šä¸‹æ–‡åˆ¤æ–­ï¼‰
        "è‹¹æœ": "PRODUCT", "é“œ": "PRODUCT", "è±†ç²•": "PRODUCT",
        "ç™½ç³–": "PRODUCT", "PTA": "PRODUCT", "èºçº¹é’¢": "PRODUCT", 
        "é“çŸ¿çŸ³": "PRODUCT", "æ²ªæ·±300": "PRODUCT", "é“": "PRODUCT",
        "ä¸­è¯500": "PRODUCT", "æœŸè´§": "PRODUCT",
        
        # å…¬å¸åç§°ï¼ˆéæœŸè´§ä¸Šä¸‹æ–‡ä¸­ä¸ç®—ï¼‰
        # æ³¨æ„ï¼šåæ³°ã€è‹¹æœç­‰åœ¨éæœŸè´§ä¸Šä¸‹æ–‡ä¸­ä¸åº”è¯¥è¢«è¯†åˆ«
    }


def extract_ground_truth_entities(text: str, ground_truth_mapping: Dict) -> List[Tuple]:
    """ğŸ” ä»æ–‡æœ¬ä¸­æå–æ ‡å‡†ç­”æ¡ˆå®ä½“ï¼ˆå¸¦ä¸Šä¸‹æ–‡åˆ¤æ–­ï¼‰"""
    entities = []
    
    # ğŸ§  é¦–å…ˆåˆ¤æ–­æ–‡æœ¬çš„æ•´ä½“ä¸Šä¸‹æ–‡
    futures_keywords = ['æœŸè´§', 'åˆçº¦', 'äº¤æ˜“æ‰€', 'äº¤å‰²', 'ä¿è¯é‡‘', 'æŒä»“', 'ä»·æ ¼', 'èµ°åŠ¿', 'åˆ†æ', 'è¡Œæƒ…']
    non_futures_keywords = ['å…¬å¸è‚¡ä»·', 'æ‰‹æœº', 'è¯åˆ¸', 'é“¶è¡Œ', 'é£Ÿå“', 'åŠ å·¥', 'å·¥ä¸š']
    
    has_futures_context = any(keyword in text for keyword in futures_keywords)
    has_non_futures_context = any(keyword in text for keyword in non_futures_keywords)
    
    for entity_text, entity_type in ground_truth_mapping.items():
        start_pos = 0
        while True:
            pos = text.find(entity_text, start_pos)
            if pos == -1:
                break
            
            should_include = True
            
            # ğŸ¯ å¯¹å®¹æ˜“è¯¯æŠ¥çš„å®ä½“è¿›è¡Œä¸Šä¸‹æ–‡åˆ¤æ–­
            if entity_type == 'PRODUCT' and entity_text in ['è‹¹æœ', 'é“œ', 'ç™½ç³–']:
                if has_non_futures_context and not has_futures_context:
                    # å¦‚æœåªæœ‰éæœŸè´§ä¸Šä¸‹æ–‡ï¼Œåˆ™ä¸åŒ…å«
                    should_include = False
            
            if entity_type == 'FUTURES_COMPANY':
                # æ£€æŸ¥æ˜¯å¦æ˜¯æœŸè´§å…¬å¸è€Œéå…¶ä»–ç±»å‹å…¬å¸
                if 'è¯åˆ¸' in text and 'æœŸè´§' not in text:
                    should_include = False
            
            if should_include:
                entities.append((entity_text, entity_type, pos, pos + len(entity_text)))
            
            start_pos = pos + 1
    
    # å»é‡å¹¶æ’åº
    entities = list(set(entities))
    entities.sort(key=lambda x: x[2])
    return entities


def calculate_performance_metrics(predicted_entities: List[Tuple], 
                                true_entities: List[Tuple]) -> Dict:
    """ğŸ“Š è®¡ç®—æ€§èƒ½æŒ‡æ ‡"""
    pred_set = set([(ent[0], ent[1]) for ent in predicted_entities])
    true_set = set([(ent[0], ent[1]) for ent in true_entities])
    
    tp = len(pred_set & true_set)
    fp = len(pred_set - true_set)
    fn = len(true_set - pred_set)
    
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
    
    return {
        'overall': {
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'tp': tp,
            'fp': fp,
            'fn': fn,
            'total_true': len(true_entities),
            'total_pred': len(predicted_entities)
        }
    }


def run_comprehensive_benchmark():
    """ğŸš€ è¿è¡Œå…¨é¢çš„åŸºå‡†æµ‹è¯•"""
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_texts, categorized_tests = create_comprehensive_test_corpus()
    ground_truth_mapping = create_ground_truth_mapping()
    
    methods = {
        "original_method": {
            "instance": OriginalRegexNERMethod(),
            "description": "åŸå§‹RegExæ–¹æ³•"
        },
        "optimized_micro_improved": {
            "instance": OptimizedMicroImprovedRegexNER(),
            "description": "ä¼˜åŒ–çš„å¾®å¾®æ”¹è¿›æ–¹æ³•"
        }
    }
    
    results = {}
    
    print(f"\nğŸš€ å¼€å§‹å…¨é¢åŸºå‡†æµ‹è¯•")
    print("=" * 80)
    print(f"ğŸ“Š æµ‹è¯•æ–‡æœ¬æ•°é‡: {len(test_texts)}")
    print(f"ğŸ¯ æµ‹è¯•æ–¹æ³•æ•°é‡: {len(methods)}")
    print(f"ğŸ“‹ æµ‹è¯•ç±»åˆ«: {len(categorized_tests)}")
    print("=" * 80)
    
    for method_name, method_config in methods.items():
        print(f"\nğŸ“Š æµ‹è¯•æ–¹æ³•: {method_config['description']}")
        print("-" * 60)
        
        method_instance = method_config["instance"]
        
        start_time = time.time()
        detailed_results = []
        processing_times = []
        
        for i, text in enumerate(test_texts):
            text_start = time.time()
            predicted_entities = method_instance.recognize_entities(text)
            text_time = time.time() - text_start
            processing_times.append(text_time)
            
            true_entities = extract_ground_truth_entities(text, ground_truth_mapping)
            metrics = calculate_performance_metrics(predicted_entities, true_entities)
            
            detailed_results.append({
                "text_id": i,
                "text": text,
                "entities": predicted_entities,
                "true_entities": true_entities,
                "processing_time": text_time,
                "metrics": metrics
            })
            
            if (i + 1) % 20 == 0:
                print(f"  å·²å¤„ç†: {i + 1}/{len(test_texts)} ä¸ªæ–‡æœ¬")
        
        total_time = time.time() - start_time
        
        # æ±‡æ€»æŒ‡æ ‡
        all_predicted = []
        all_true = []
        for result in detailed_results:
            all_predicted.extend(result['entities'])
            all_true.extend(result['true_entities'])
        
        overall_metrics = calculate_performance_metrics(all_predicted, all_true)
        
        # ğŸ“Š æŒ‰ç±»åˆ«åˆ†æ
        category_metrics = {}
        for category, texts in categorized_tests.items():
            category_predicted = []
            category_true = []
            
            for text in texts:
                result = next((r for r in detailed_results if r['text'] == text), None)
                if result:
                    category_predicted.extend(result['entities'])
                    category_true.extend(result['true_entities'])
            
            if category_predicted or category_true:
                category_metrics[category] = calculate_performance_metrics(
                    category_predicted, category_true
                )
        
        results[method_name] = {
            "description": method_config["description"],
            "total_time": total_time,
            "avg_time_per_text": total_time / len(test_texts),
            "texts_per_second": len(test_texts) / total_time,
            "overall_metrics": overall_metrics,
            "category_metrics": category_metrics,
            "detailed_results": detailed_results[:5]  # åªä¿å­˜å‰5ä¸ªè¯¦ç»†ç»“æœ
        }
        
        overall = results[method_name]['overall_metrics']['overall']
        
        print(f"âœ… æµ‹è¯•å®Œæˆ!")
        print(f"âš¡ å¹³å‡å»¶è¿Ÿ: {results[method_name]['avg_time_per_text']*1000:.1f}æ¯«ç§’")
        print(f"ğŸ¯ æ•´ä½“F1åˆ†æ•°: {overall['f1']:.3f}")
        print(f"ğŸ“Š ç²¾ç¡®ç‡: {overall['precision']:.3f}")
        print(f"ğŸ“‹ å¬å›ç‡: {overall['recall']:.3f}")
    
    return results


def analyze_results_comprehensive(results):
    """ğŸ“ˆ å…¨é¢åˆ†ææµ‹è¯•ç»“æœ"""
    
    print(f"\nğŸ“ˆ å…¨é¢æ€§èƒ½åˆ†æ")
    print("=" * 80)
    
    if len(results) < 2:
        print("âš ï¸ éœ€è¦è‡³å°‘ä¸¤ä¸ªæ–¹æ³•è¿›è¡Œå¯¹æ¯”")
        return
    
    # åˆ›å»ºæ•´ä½“å¯¹æ¯”è¡¨
    comparison_data = []
    
    for method_name, result in results.items():
        overall_metrics = result['overall_metrics']['overall']
        
        comparison_data.append({
            "æ–¹æ³•": method_name.replace('_', ' ').title(),
            "æè¿°": result["description"],
            "å¹³å‡å»¶è¿Ÿ(ms)": f"{result['avg_time_per_text'] * 1000:.1f}",
            "å¤„ç†é€Ÿåº¦(æ–‡æœ¬/ç§’)": f"{result['texts_per_second']:.1f}",
            "ç²¾ç¡®ç‡": f"{overall_metrics['precision']:.3f}",
            "å¬å›ç‡": f"{overall_metrics['recall']:.3f}",
            "F1åˆ†æ•°": f"{overall_metrics['f1']:.3f}",
            "TP": overall_metrics['tp'],
            "FP": overall_metrics['fp'],
            "FN": overall_metrics['fn']
        })
    
    df = pd.DataFrame(comparison_data)
    print(f"ğŸ“Š æ•´ä½“æ€§èƒ½å¯¹æ¯”è¡¨")
    print("-" * 80)
    print(df.to_string(index=False))
    
    # ğŸ“Š åˆ†ç±»åˆ«æ€§èƒ½åˆ†æ
    print(f"\nğŸ“‹ åˆ†ç±»åˆ«æ€§èƒ½åˆ†æ")
    print("-" * 80)
    
    methods = list(results.keys())
    if len(methods) >= 2:
        original_key = methods[0]  # å‡è®¾ç¬¬ä¸€ä¸ªæ˜¯åŸå§‹æ–¹æ³•
        improved_key = methods[1]  # ç¬¬äºŒä¸ªæ˜¯æ”¹è¿›æ–¹æ³•
        
        category_comparison = []
        
        for category in results[original_key]['category_metrics']:
            original_f1 = results[original_key]['category_metrics'][category]['overall']['f1']
            improved_f1 = results[improved_key]['category_metrics'][category]['overall']['f1']
            improvement = improved_f1 - original_f1
            
            status = "âœ…" if improvement > 0.05 else "â–" if abs(improvement) <= 0.05 else "âš ï¸"
            
            category_comparison.append({
                "æµ‹è¯•ç±»åˆ«": category,
                "åŸå§‹F1": f"{original_f1:.3f}",
                "æ”¹è¿›F1": f"{improved_f1:.3f}",
                "æå‡": f"{improvement:+.3f}",
                "çŠ¶æ€": status
            })
        
        category_df = pd.DataFrame(category_comparison)
        print(category_df.to_string(index=False))
        
        # ğŸ“ˆ è®¡ç®—æ€»ä½“æ”¹è¿›æ•ˆæœ
        original_overall = results[original_key]['overall_metrics']['overall']
        improved_overall = results[improved_key]['overall_metrics']['overall']
        
        print(f"\nğŸ¯ æ€»ä½“æ”¹è¿›æ•ˆæœ")
        print("-" * 50)
        print(f"F1åˆ†æ•°: {original_overall['f1']:.3f} â†’ {improved_overall['f1']:.3f} ({improved_overall['f1'] - original_overall['f1']:+.3f})")
        print(f"ç²¾ç¡®ç‡: {original_overall['precision']:.3f} â†’ {improved_overall['precision']:.3f} ({improved_overall['precision'] - original_overall['precision']:+.3f})")
        print(f"å¬å›ç‡: {original_overall['recall']:.3f} â†’ {improved_overall['recall']:.3f} ({improved_overall['recall'] - original_overall['recall']:+.3f})")
        
        # ğŸ‰ ç»“è®º
        f1_improvement = improved_overall['f1'] - original_overall['f1']
        precision_improvement = improved_overall['precision'] - original_overall['precision']
        
        print(f"\nğŸ† æµ‹è¯•ç»“è®º")
        print("-" * 50)
        if f1_improvement > 0.02:
            print("âœ… å¾®å¾®æ”¹è¿›æ–¹æ¡ˆå–å¾—æ˜æ˜¾æ•ˆæœï¼")
        elif f1_improvement > 0:
            print("âœ… å¾®å¾®æ”¹è¿›æ–¹æ¡ˆå–å¾—æ­£é¢æ•ˆæœ")
        else:
            print("âš ï¸ å¾®å¾®æ”¹è¿›æ•ˆæœä¸æ˜¾è‘—ï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–")
        
        if precision_improvement > 0.05:
            print("ğŸ¯ ç²¾ç¡®ç‡æ˜¾è‘—æå‡ï¼Œå‡å°‘äº†è¯¯è¯†åˆ«é—®é¢˜")
        
        print(f"\nğŸ’¡ ä¸»è¦æ”¹è¿›ç‚¹:")
        positive_categories = [item for item in category_comparison if item['çŠ¶æ€'] == 'âœ…']
        if positive_categories:
            for category in positive_categories[:3]:  # æ˜¾ç¤ºå‰3ä¸ªæ”¹è¿›æœ€å¤§çš„ç±»åˆ«
                print(f"   â€¢ {category['æµ‹è¯•ç±»åˆ«']}: æå‡{category['æå‡']}")


def save_results_to_files(results):
    """ğŸ’¾ ä¿å­˜æµ‹è¯•ç»“æœ"""
    print(f"\nğŸ’¾ ä¿å­˜æµ‹è¯•ç»“æœ")
    print("=" * 50)
    
    # ä¿å­˜ç®€åŒ–çš„ç»“æœåˆ°JSON
    clean_results = {}
    for method_name, result in results.items():
        clean_result = {
            "description": result["description"],
            "total_time": result["total_time"],
            "avg_time_per_text": result["avg_time_per_text"],
            "texts_per_second": result["texts_per_second"],
            "overall_metrics": result["overall_metrics"],
            "category_metrics": result.get("category_metrics", {})
        }
        clean_results[method_name] = clean_result
    
    with open("optimized_micro_improvement_results.json", "w", encoding="utf-8") as f:
        json.dump(clean_results, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: optimized_micro_improvement_results.json")


# ğŸš€ ä¸»ç¨‹åºæ‰§è¡Œ
if __name__ == "__main__":
    print("ğŸ”„ å¼€å§‹æ‰§è¡Œä¼˜åŒ–çš„å¾®å¾®æ”¹è¿›æµ‹è¯•...")
    
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    benchmark_results = run_comprehensive_benchmark()
    
    # åˆ†æç»“æœ
    analyze_results_comprehensive(benchmark_results)
    
    # ä¿å­˜ç»“æœ
    save_results_to_files(benchmark_results)
    
    print(f"\nğŸ‰ ä¼˜åŒ–çš„å¾®å¾®æ”¹è¿›æµ‹è¯•å®Œæˆï¼")
    print("ğŸ” å…³é”®æ”¹è¿›ï¼šè§£å†³äº†è¯¯è¯†åˆ«è¿‡æ»¤è¿‡ä¸¥é—®é¢˜ï¼Œæå‡äº†æ•´ä½“æ€§èƒ½")
    print("ğŸ“ˆ å»ºè®®ï¼šç»§ç»­åœ¨çœŸå®æ•°æ®ä¸ŠéªŒè¯æ•ˆæœï¼Œæ ¹æ®ä¸šåŠ¡éœ€æ±‚å¾®è°ƒå‚æ•°")